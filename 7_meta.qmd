---
webr:
  packages: ['dplyr', 'meta']
---

# Meta-Analysis and Qualitative Synthesis {#sec-meta}

This section will cover methods of **combining measures of effect** together to calculate an overall average or pooled meta-analysis estimate. When conducting a random-effects meta-analysis, it is important to quantify and explore the **between-study heterogeneity**, or differences in the effect sizes across studies that are due to study differences compared to sampling error. We display the results of a meta-analysis using a **forest plot** figure to facilitate interpretation and visualization of effects, variation, and study weights. We will also briefly consider methods for synthesis of **qualitative research**.

![](images/meme_heterogeneity.jpg)

## Pooling Measures of Effect

In the previous chapter, we reviewed the basics of meta-analysis, including differences between fixed-effect and random-effects models. We will now cover procedures for **conducting meta-analysis** for different types of outcome data. All examples will use the [**meta package**](https://cran.r-project.org/web/packages/meta/index.html) in *R*.

The most common method of pooling measures of effect is called the **generic inverse-variance** method, the formula for which was provided in @sec-analysis. In this method, studies are weighted by the inverse of their precision. This method is used for **continuous data** (e.g., mean difference, SMD), and can also be used for **dichotomous data**. It is also the method used when conducting meta-analysis on **pre-calculated measures of effect** (e.g., odds ratios, risk ratios).

::: callout-warning
### Change-From-Baseline Data

Analyses of an intervention based on a change from baseline can be more efficient, increasing precision of estimates. They are ideally analyzed by including the baseline measurement as a covariate in a regression analysis or ANCOVA. In meta-analysis, change-from-baseline scores and post-intervention values can be combined in the same analysis if using a raw MD outcome. However, they should **not** be combined when using a SMD outcome, as [the standard deviations are not comparable](https://training.cochrane.org/handbook/current/chapter-10#section-10-5-2).
:::

When analyzing **dichotomous raw data** (e.g., number of events and sample size in each comparison group), there are alternative approaches available. The most common of these is the **Mantel-Haenszel method**. This method is *preferred* for dichotomous data, especially when the event is rare or when the sample size is small. The **Peto odds ratio method** is another approach, but it has more limitations and can only be used to calculate odds ratios.

One issue for dichotomous outcomes is that there may be **zero events** in one or both groups. In this case, the traditional, and often default approach in meta-analysis software, is to replace zero values with 0.5 (continuity correction). However, this correction is only required when using the generic inverse-variance approach; it should not be used when using the other methods.

In the **meta package**, the following functions can be used for meta-analysis of different types of data inputs:

-   `metagen` for pre-calculated measures of effect
-   `metacont` for continuous data
-   `metabin` for dichotomous data
-   `metaprop` for prevalence (proportion) data
-   `metainc` for incidence rate ratio or incidence rate difference data
-   `metarate` for incidence data in a single group
-   `metacor` for correlation coefficients

Note that we will not cover meta-analysis of all of these data types. If interested to see examples of others, you can visit the online book [Doing Meta-Analysis in R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pooling-es.html#pooling-es-r).

::: callout-note
### Continuous Data Meta-Analysis Example

To illustrate how to conduct a meta-analysis using **continuous data**, we will examine part of the dataset from a systematic review of music interventions to improve various health outcomes in people with cancer [@bradtMusicInterventionsImproving2021]. Specifically, we will examine 17 studies from that review that evaluated music interventions plus standard care compared to standard care alone in adults to improve anxiety. Anxiety was measured in all studies using the Spielberger State Anxiety Index (STAI) scale, with lower scores representing lower anxiety.

```{webr-r}
#| context: setup
# Load and import dataset
url <- "https://iany33.github.io/data-repository/Bradt_2021.csv"
download.file(url, "Bradt_2021.csv")
Bradt_2021 <- read.csv("Bradt_2021.csv")
```

```{r}
#| warning: false
library(DT)
library(rio)
library(here)

Bradt_2021 <- import(here("assets", "Bradt_2021.xlsx"))

Bradt_2021 |> datatable(
  rownames = FALSE,
  options = list(
    columnDefs = list(list(className = 'dt-center', 
                           targets = 0:4))))
```

We can see that each study reported the mean anxiety score, SD, and sample size in each comparison group. Because all studies measured the outcome on the same scale, we can calculate a raw mean difference (MD). For continuous data, we will use the `metacont` function.

```{webr-r}
library(meta)

Bradt_meta <- Bradt_2021 |> 
  metacont(n.e = int_n,             # number in intervention group
           mean.e = int_mean,       # intervention group mean
           sd.e = int_sd,           # intervention group SD
           n.c = con_n,             # number in control group
           mean.c = con_mean,       # control group mean
           sd.c = con_sd,           # control group SD
           studlab = study,         # study ID column
           sm = "MD",               # summary measure (MD or SMD)
           random = TRUE,           # conduct random-effects analysis
           common = FALSE,          # do not conduct a fixed-effect analysis
           method.tau = "REML",     # method of calculating tau
           method.random.ci = "HK", # apply the Hartung-Knapp adjustment
           title = "Music Intervention and Anxiety"
)

summary(Bradt_meta)
```

Based on this analysis, we can see that the anxiety scores were lower among participants that received music interventions in addition to standard care vs. standard care alone.

If we wanted to change any settings, we can use the `update` function to update our analysis. For example, we could run an updated analysis to compare how the results might change with a different method of calculating $\tau^2$.

```{webr-r}
Bradt_meta_update <- update(Bradt_meta,
                           method.tau = "PM") # Update to Paule-Mandel method

# Compare pooled estimates and tau^2 values
data.frame(Method = c("REML", "PM"),
       Pooled_MD = c(Bradt_meta$TE.random, Bradt_meta_update$TE.random),
       Pooled_SE = c(Bradt_meta$seTE.random, Bradt_meta_update$seTE.random),
       Tau2 = c(Bradt_meta$tau2, Bradt_meta_update$tau2))
```

We obtain slightly different results when using the alternative method of calculating $\tau^2$. How do the results change if we instead compare to the **DerSimonian-Laird (DL)** method?
:::

::: callout-note
### Dichotomous Data Meta-Analysis Example

To illustrate how to conduct a meta-analysis using **dichotomous data**, we will examine a dataset of 136 studies used in a meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation [@hartmann-boyceNicotineReplacementTherapy2018], as prepared by [White et al. in the metadat repository](https://wviechtb.github.io/metadat/index.html).

```{webr-r}
#| context: setup
# Load and import dataset
url <- "https://iany33.github.io/data-repository/Hartmann_2018.csv"
download.file(url, "Hartmann_2018.csv")
Hartmann_2018 <- read.csv("Hartmann_2018.csv")
```

```{r}
#| warning: false
Hartmann_2018 <- import(here("assets", "Hartmann_2018.xlsx"))

Hartmann_2018 |> datatable(
  rownames = FALSE,
  options = list(
    columnDefs = list(list(className = 'dt-center', 
                           targets = 0:4)))) 
```

As can been seen above, the dataset contains information from each study on the number of participants in each group that continued to abstain from smoking at 6+ months of follow-up. Also included is a column that specifies the type of treatment received. Since the data are dichotomous and all include raw data, we can calculate an measure of effect using the **Mantel-Haenszel method**. We will use the `metabin` function, but first we will subset our data to only conduct the meta-analysis on studies that investigated the nicotine *patch* as an intervention.

```{webr-r}
Hartmann_meta <- Hartmann_2018 |> 
  filter(treatment == "patch") |>   # subset data for meta-analysis 
  metabin(event.e = x.nrt,          # events in intervention group
          n.e = n.nrt,              # number in intervention group
          event.c = x.ctrl,         # events in control group
          n.c = n.ctrl,             # number in control group
          studlab = study,          # study ID column
          sm = "RR",                # summary measure (OR or RR)
          method = "MH",            # use the Mantel-Haenszel method
          random = TRUE,            # conduct random-effects analysis
          common = FALSE,           # do not conduct a fixed-effect analysis
          method.tau = "PM",        # method of calculating tau
          method.random.ci = "HK",  # apply the Hartung-Knapp adjustment
          MH.exact = TRUE,          # do not apply a continuity correction
          title = "Nicotine Patch and Smoking Cessation"
)

summary(Hartmann_meta)
```

In this analysis, we can see that participants in the intervention group were more likely to continue to abstain from smoking at follow-up compared to those in the control group.

How do the results change, if at all, if we instead do not apply the Hartung-Knapp adjustment?
:::

### Prevalence and Incidence Data

Meta-analysis of **prevalence and incidence data** uses a slightly different approach than continuous and dichotomous data. The recommended approach for synthesizing such data is to use a **generalized linear mixed-effects model (GLMM)** [@schwarzerSeriouslyMisleadingResults2019]. Prevalence data should be logit transformed prior to meta-analysis, while incidence data should be log transformed (this is done automatically in the **meta package**). For prevalence data, the GLMM approach fits an intercept-only logistic regression to the data, with a random-effect to account for the between-study variation. A Poisson GLMM model is used for incidence data.

Using the GLMM approach has some limitations. It is not possible to obtain individual study weights using this method. Additionally, there is only one method to calculate $\tau^2$, the maximum-likelihood (ML) estimator, and there will be no confidence internals for $\tau^2$. In case those details are needed, the inverse-variance approach can be used with the arcsine or logit transformation for prevalence data, or the log transformation for incidence data.

::: callout-note
#### Prevalence Meta-Analysis Example

An example is shown below of a meta-analysis of the prevalence of *selected outcomes* from a systematic review of the knowledge, behaviours, and training of restaurant and food service personnel toward food allergies and Celiac disease [@youngSystematicReviewMetaregression2018].

```{webr-r}
#| context: setup
# Load and import dataset
url <- "https://iany33.github.io/data-repository/Young_2018.csv"
download.file(url, "Young_2018.csv")
Young_2018 <- read.csv("Young_2018.csv")
```

```{r}
#| warning: false

Young_2018 <- import(here("assets", "Young_2018.xlsx"))

Young_2018 |> datatable(
  rownames = FALSE,
  options = list(
    columnDefs = list(list(className = 'dt-center', 
                           targets = 0:4)))) |> 
  formatStyle(columns = colnames(Young_2018), fontSize = '70%')
```

We can see that some article and study characteristics are also included in this dataset. For these data, we will subset the **training outcome** only for illustration purposes. This outcome shows the proportion of participants (i.e., restaurant and food service staff) in each study that reported receiving training about food allergies.

```{r}
#| code-fold: false
# Note that this model does not work in webr, and must be run in R/RStudio. 
Young_2018_meta <- 
  metaprop(event = n.positive, 
           n = n.total,
           studylab = "author.year", 
           subset = outcome.category == "Training status/policies",
           data = Young_2018,
           method = "GLMM",
           sm = "PLOGIT",
           random = TRUE,
           common = FALSE,
           method.random.ci = "HK", 
           title = "Food Allergy Training Prevalence"
)

summary(Young_2018_meta)
```

We can see the pooled prevalence value is \~36%, with a 95% CI of 26-47%.
:::

## Assessing Heterogeneity

**Heterogeneity** refers to differences between studies that is beyond what we would expect from chance (or random error) alone. It can be due to differences in how interventions (exposures) and outcomes were defined, implemented, and measured, differences in the characteristics of the populations assessed, or other factors (e.g., differences in study methods, context, and bias).

There is a $\chi2$ statistical test for heterogeneity, called **Cochran's Q**, that has been traditionally used and is included in the the **R meta** results output. This test evaluates whether this is more variation than would be expected by sampling error alone. However, this test has lower power when the number of studies is small. Additionally, its use is controversial, as some argue that since there are always differences expected between studies, some heterogeneity will always be present.

For this reason, the $I^2$ statistic was developed to quantify heterogeneity [@higginsMeasuringInconsistencyMetaanalyses2003]. The formula, based on Cochran's $Q$, is shown below, with $N$ referring to the number of studies in the analysis:

$$
I^2 = \frac{Q-(N-1)}{Q}
$$

$I^2$ refers to the percentage of variation in measures of effect across studies that is due to heterogeneity rather than sampling error. While thresholds are often used in practice, these are discouraged. Instead, the amount of heterogeneity that is important depends on the context (e.g., magnitude and direction of effects, strength of evidence for heterogeneity). In general, $I^2$ values of 0-40% might not be important, values of 75-100% usually indicate considerable heterogeneity, while values in 30-60% and 50-90% might indicate moderate to substantial heterogeneity [@higginsCochraneHandbookSystematic2022].

However, because $I^2$ is a relative measure, it should not be the only measure of heterogeneity reported. We can also examine $\tau$ and its 95% CI, which represents the estimated SD of the true effects across studies (it is on the same scale as the measure of effect used in the analysis).

**Prediction intervals (PIs)** are recommended to be included alongside other estimates of heterogeneity. A 95% PI estimates the range of values the measure of effect would be expected to fall within in 95% of similar studies that might be conducted in the future. In cases of heterogeneity, the PI covers a wider range of values than a CI. In **R meta**, we can add a PI to our output by adding the argument `prediction = TRUE` to the function input options.

We will explore in the next session @sec-meta2 how to investigate different causes of heterogeneity using subgroup analysis and meta-regression.

::: callout-note
### Heterogeneity Example

We will go back to our first meta-analysis example that examined the effect of music interventions plus standard care vs. standard care alone to reduce anxiety levels among people with cancer [@bradtMusicInterventionsImproving2021]. We will update the analysis to include a prediction interval (PI), then interpret the heterogeneity.

```{webr-r}
Bradt_meta <- update(Bradt_meta, prediction = TRUE)
Bradt_meta
```

We can see in the results that the $Q$ test for heterogeneity is significant, and the $I^2$ value is also very high at \~93% (95% CI: 90.9-95.3%). While the pooled measure of effect (MD) has a 95% CI that excludes the null, suggesting a consistent positive effect of the music intervention, the PI crosses above zero. This indicates that we cannot rule out that the intervention might have no effect or a negative effect in future studies.
:::

## Forest Plots

**Forest plots** are the most common way to visualize meta-analysis results. They show the measure of effect estimate, confidence interval, and weight of each study, and the pooled or average estimate at the bottom. They can also include a prediction interval.

Below we will create a forest plot from the meta-analysis of music interventions on reducing anxiety levels in people with cancer, from the earlier example [@bradtMusicInterventionsImproving2021]. Using the **meta package**, we can generate a forest plot for our saved meta-analysis results using the `forest` function.

```{webr-r}
Bradt_meta |> forest(digits.mean = 1,  # Limiting significant digits on left columns
                    digits.sd = 1,
                    fontsize = 7,           # Decrease fontsize so all text fits on image
                    spacing = 0.9           # Decrease spacing to ensure text fits
                    )
```

We can clean up the display of this plot by removing the data on the left side of the plot, and also adding in a prediction interval at the bottom. Other [customization options](https://cran.r-project.org/web/packages/meta/meta.pdf) can be made as needed.

```{webr-r}
Bradt_meta |> forest(sortvar = TE,        # sort studies by effect size
                    prediction = TRUE,         # add prediction interval
                    print.tau2 = FALSE,        # do not show tau2 at the bottom
                    leftcols = "study",        # show only the study ID on the left
                    leftlabs = "Study",        # relabel study column
                    col.diamond = "steelblue", # colour customization
                    col.predict = "darkblue",
                    spacing = 0.85)
```

::::: callout-tip
## Meta-Analysis Exercise

We will load data from the Young et al. [-@youngEffectivenessFoodHandler2019] systematic review and meta-analysis of the effectiveness of food handler training and education interventions. The dataset can be loaded and visualized as per below:

```{webr-r}
#| context: setup
# Load and import dataset
url <- "https://iany33.github.io/data-repository/Young_2019_ma.csv"
download.file(url, "Young_2019_ma.csv")
Young_2019 <- read.csv("Young_2019_ma.csv")
```

```{r}
#| warning: false
# First load necessary packages

Young_2019 <- import(here("assets", "Young_2019_ma.xlsx"))

Young_2019 |> datatable(
  rownames = FALSE,
  options = list(
    columnDefs = list(list(className = 'dt-center', 
                           targets = 0:4)))) |> 
  formatStyle(columns = colnames(Young_2019), fontSize = '70%')
```

We can see that there are 214 unique outcomes, with multiple outcomes reported in many studies. We will first conduct a meta-analysis of a subset of data that only examines **RCTs and behaviour outcomes**:

```{webr-r}
Young_2019_meta <- Young_2019 |> 
  filter(study_design == "RCT" & outcome == "Behaviour") |>
  metagen(TE = g,
          seTE = g_se,
          studlab = author_year,
          sm = "SMD", 
          random = TRUE,
          common = FALSE,
          method.tau = "REML",
          method.random.ci = "HK",
          title = "Food Handler Training and Behaviour - RCTs"
)

summary(Young_2019_meta)
```

Examine and interpret the results.

-   How many studies and unique outcomes were included?
-   What does the overall evidence say about the intervention?
-   How much heterogeneity is present? Is it significantly different than zero?
-   How does the result change if you use a different method of calculating $\tau^2$?

Now create a customized forest plot to visualize the results and include the prediction interval. How would you interpret the interval?

::: panel-tabset
### Code Editor

```{webr-r}
Young_2019_meta |> forest()
```

### Answer

```{webr-r}
#| read-only: true
Young_2019_meta |> forest(sortvar = TE,        # sort studies by effect size
                    prediction = TRUE,         # add prediction interval
                    print.tau2 = FALSE,        # do not show tau2 at the bottom
                    leftcols = "author_year",        # show only the study ID on the left
                    leftlabs = "Author (Year)",        # relabel study column
                    col.diamond = "steelblue", # colour customization
                    col.predict = "darkblue",
                    spacing = 0.85)
```
:::

Now conduct a meta-analysis for the subset of **non-randomized studies and the inspection scores outcome**. Answer the same questions above for this analysis.

::: panel-tabset
### Code Editor

```{webr-r}
Young_2019_meta <- Young_2019 |> 
  filter(study_design == "RCT" & outcome == "Behaviour") |>
  metagen(TE = g,
          seTE = g_se,
          studlab = author_year,
          sm = "SMD", 
          random = TRUE,
          common = FALSE,
          method.tau = "REML",
          method.random.ci = "HK",
          title = "Food Handler Training and Behaviour - RCTs"
)

summary(Young_2019_meta)
```

### Answer

```{webr-r}
#| read-only: true
Young_2019_meta2 <- Young_2019 |> 
  filter(study_design == "Non-randomized study" & outcome == "Inspection") |>
  metagen(TE = g,
          seTE = g_se,
          studlab = author_year,
          sm = "SMD", 
          random = TRUE,
          common = FALSE,
          method.tau = "REML",
          method.random.ci = "HK",
          title = "Food Handler Training and Inspection Scores - NRTs"
)

summary(Young_2019_meta2)
```
:::
:::::

## Qualitative Syntheses

Many of the systematic review methods discussed in the course can similarly be applied to **qualitative syntheses**, with some adaptations. The analysis, or synthesis, stage in particular is very different [@barnett-pageMethodsSynthesisQualitative2009]. The data that is synthesized in such reviews can vary depending on the review objectives and types of evidence included, but is typically themes supported by participant quotes from qualitative research studies (e.g., focus groups, grounded theory studies, etc.) or mixed-method studies. Sometimes, such reviews might also include data from review articles or policy documents to be synthesized into a **narrative synthesis**, which is common for reviews that incorporate multiple types and sources of evidence.

One of the more common syntheses approaches for qualitative reviews is called **thematic synthesis** [@thomasMethodsThematicSynthesis2008]. Thomas and Harden [-@thomasMethodsThematicSynthesis2008] describe a three-step process of conducting this analysis, which consists of:

-   Line-by-line coding of the findings (e.g., participant quotes and themes) of primary studies
-   Organization and grouping of the codes into related areas, which is used to develop *descriptive themes*
-   Development of *analytical themes*, which aim to go beyond the findings of the original studies and produce overarching themes.

The latter step requires some judgement of reviewers and involves making inferences about the mechanisms behind the descriptive themes.

There are other approaches to qualitative synthesis as well, including:

-   **Meta-aggregation**: uses an approach similar to thematic synthesis that aims to generate recommendation statements for policy-makers and practitioners.
-   **Framework synthesis**: uses a theory or model to guide analysis and interpretation of findings
-   **Meta-ethnography**: a theory-building approach to synthesis

Meta-aggregation is described by JBI [@lockwoodQualitativeResearchSynthesis2015], while the latter two methods are covered by separate chapters in the [Cochrane-Campbell Handbook for Qualitative Evidence Synthesis](https://training.cochrane.org/cochrane-campbell-handbook-qualitative-evidence-synthesis).

::: callout-tip
### Qualitative Synthesis Exercise

Examine the following qualitative systematic review by Rawlings et al. [-@rawlingsExploringAdultsExperiences2019] that investigated the [perceptions and experiences of sedentary behaviour in adults](https://link.springer.com/article/10.1186/s12889-019-7365-1).

-   To what extent did their search differ from that of a standard systematic review?
-   What was their process for assessing methodological limitations of the studies?
-   How did they conduct their synthesis?
-   How did they report their synthesis results?
:::
