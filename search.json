[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OH8004 Knowledge Synthesis in Occupational and Public Health",
    "section": "",
    "text": "Overview\nThis is the course website for OH8004 Knowledge Synthesis in Occupational and Public Health in the School of Occupational and Public Health, Toronto Metropolitan University.\nThe course calendar description for OH8004 is as follows:\n\nThis course will cover structured methods of comprehensively synthesizing and reviewing research evidence. Detailed guidance will be provided on the design and conduct of systematic reviews and meta-analysis in occupational and public health. Students will be required to identify an individual occupational or public health review topic of key relevance to their thesis, and will conduct a structured literature review on this topic.\n\nFor the most recent course outline and all announcements, please visit the D2L website for this course. This website will be used to review core content, examples, and exercises during in-class sessions.\nThis e-book was prepared using Quarto in RStudio (Posit). This book is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means you are free to share and adapt this material in any format or medium, as long as appropriate credit is given, a link to the license is provided, the material is used for non-commercial purposes, and any shared or adapted work is distributed under the same license.\n\n\n\nCC BY-NC-SA 4.0"
  },
  {
    "objectID": "1_intro.html#systematic-review-team",
    "href": "1_intro.html#systematic-review-team",
    "title": "1  Introduction",
    "section": "1.1 Systematic Review Team",
    "text": "1.1 Systematic Review Team\nBefore embarking on a systematic review or other knowledge synthesis, consider the expertise you might require on your team:\n\nKnowledge of systematic review or knowledge synthesis methods\nA research librarian or information scientist to assist with the search strategy\nKnowledge of the topic area being investigated\nAny additional methodological expertise relevant to the topic or methods used (e.g., advanced meta-analysis, qualitative analysis)"
  },
  {
    "objectID": "1_intro.html#systematic-review-protocol",
    "href": "1_intro.html#systematic-review-protocol",
    "title": "1  Introduction",
    "section": "1.2 Systematic Review Protocol",
    "text": "1.2 Systematic Review Protocol\nSystematic review protocols are critically important to ensure all review steps, assumptions, and procedures are justified and outlined in advance of conducting the review. This helps to reduce author biases (e.g., selective reporting) and arbitrary decision-making in the review process. Additionally, protocols are important to:\n\nHelp reviewers organize their review and anticipate potential problems\nHelp reduce duplication of efforts and (potentially) enhance collaboration\nImprove transparency and allow replication of the methods\n\nThere is a reporting standard for systematic review protocols, called PRIMSA-P. The reporting checklist is available form the PRISMA-P website as a PDF or Word file. See the published paper and the elaboration document for more information (Moher et al. 2015; Shamseer et al. 2015).\n\n\n\n\n\n\nImportant\n\n\n\nYour systematic review protocol final assignment must include all relevant PRISMA-P items. The checklist will be used as evaluation criteria.\n\n\nIt is recommended that protocols of reviews you intend to publish be registered at PROSPERO, an international database of protocols of systematic reviews. This is important reduce the duplication of synthesis efforts. PROSPERO is:\n\n“An international database of prospectively registered systematic reviews in health and social care, welfare, public health, education, crime, justice, and international development, where there is a health related outcome”.\n\nProtocols can also be published as journal articles in publications as such Systematic Reviews, BMJ Open, and PLoS ONE.\n\n\n\n\n\n\nPROSPERO Database\n\n\n\nThe PROSPERO database does not accept protocols for scoping reviews, only systematic reviews and rapid reviews.\n\n\n\n\n\n\n\n\nSystematic Review Protocol Example\n\n\n\nExamine the following systematic review protocol by Burns et et. (2022), as published in Systematic Reviews. They included the PRISMA-P checklist as a supplementary file.\nReview the checklist and compare to the relevant sections of the article. Was each criterion appropriately addressed?"
  },
  {
    "objectID": "1_intro.html#key-knowledge-synthesis-resources",
    "href": "1_intro.html#key-knowledge-synthesis-resources",
    "title": "1  Introduction",
    "section": "1.3 Key Knowledge Synthesis Resources",
    "text": "1.3 Key Knowledge Synthesis Resources\nThe following organizations are important resources for knowledge synthesis research:\n\nThe Cochrane Collaboration publishes the Handbook of Systematic Reviews of Interventions, as well as systematic reviews of various healthcare and related topics in the Cochrane Library.\n\nThe Joanna Briggs Collaboration has published a Manual for Evidence Synthesis of different types of reviews and questions. Of particular note are critical appraisal tools for various types of study designs.\nThe McGill Library has compiled numerous resources and suggested articles related to the process of conducting a knowledge synthesis."
  },
  {
    "objectID": "1_intro.html#homework",
    "href": "1_intro.html#homework",
    "title": "1  Introduction",
    "section": "1.4 Homework",
    "text": "1.4 Homework\nFor next week, identify and obtain the PDF of one systematic review article published on an occupational or public health topic related to your thesis. Place the article in the following shared folder. We will discuss each article next week in class, and in the following weeks.\n\n\n\n\nMoher, David, Larissa Shamseer, Mike Clarke, Davina Ghersi, Alessandro Liberati, Mark Petticrew, Paul Shekelle, and Lesley A Stewart. 2015. “Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols (PRISMA-P) 2015 Statement.” Systematic Reviews 4 (1): 1. https://doi.org/10.1186/2046-4053-4-1.\n\n\nShamseer, Larissa, David Moher, Mike Clarke, Davina Ghersi, Alessandro Liberati, Mark Petticrew, Paul Shekelle, et al. 2015. “Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols (Prisma-p) 2015: Elaboration and Explanation.” BMJ (Online) 349 (January): 1–25. https://doi.org/10.1136/bmj.g7647.\n\n\nTricco, A C, J Tetzlaff, and D Moher. 2011. “The Art and Science of Knowledge Synthesis.” Journal of Clinical Epidemiology 64 (1): 11–20. http://www.scopus.com/inward/record.url?eid=2-s2.0-78149388685&partnerID=40&md5=cff0dc293630269631887e1b25835cd7."
  },
  {
    "objectID": "2_question.html#review-question",
    "href": "2_question.html#review-question",
    "title": "2  Review Question and Eligibility Criteria",
    "section": "2.1 Review Question",
    "text": "2.1 Review Question\nIn defining a question, different approaches are used depending on the type of question being considered (e.g., intervention, exposure-outcome relationship, prevalence or incidence of an outcome). Types of questions can be categorized as follows (Munn et al. 2018).\n\nIntervention (effectiveness) reviews\nEtiology or risk factor reviews\nPrevalence or incidence reviews\nDiagnostic test accuracy reviews\nQualitative (experiential) reviews\n“Umbrella” reviews (reviews of reviews)\nOthers (e.g., economic evaluations, policy reviews)\n\nFor the purposes of this course, we will mainly focus on intervention reviews, which are the most common, but will address aspects of etiology/risk factor and incidence/prevalence reviews, which are common in occupational and public health research. These scenarios are further defined below. For more details on the other types of reviews, see Munn et al. (2018) and the JBI Manual for Evidence Synthesis.\n\n2.1.1 Intervention Reviews\nSystematic reviews of interventions should have questions that follow the PICO framework. PICO refers to the Population, Intervention, Comparison, and Outcome. Sometimes an S is added for Study Design.\nPopulation includes the study participants and settings of interests. This could include populations with specific socio-demographic characteristics (e.g., age groups, sex, ethnicity), different occupational or other groups, or specific settings (e.g., community-based, institutions).\nIntervention includes the specific process, technique, policy, or other change being evaluated in the populations of interest. It is important to define exactly what is being delivered (e.g., materials, procedures), by whom and to who, how, where, when and how much. The TIDieR Checklist is a useful framework to consider these factors and which types of interventions might be relevant. Interventions can then be grouped or sub-grouped into specific labels based on their characteristics, if appropriate, with separate analyses conducted on each group. E.g., a review of falls prevention interventions might have groups such as exercise, medication, supportive environments.\nComparisons includes the control groups being considered. This could include placebo groups, those receiving no intervention or a current “standard” or “typical” procedure, or groups receiving an alternative or competing intervention.\nOutcome includes the set of measures that will determine intervention effectiveness. You should select a limited number of the most important outcomes relevant to your topic, and avoid extracting data on all possible outcomes. The Cochrane Collaboration recommends no more than seven critical outcomes for their review summary documents. Similar to interventions, outcomes should be grouped into similar categories or domains (e.g., knowledge, behaviours, biological); this can include hierarchical groupings if appropriate. Consider what time points of measurement will be used, and measurement methods (e.g., lab-based, self-reported, objective). It can be useful to consider logic models to map out the possible outcomes and their relationships when deciding what is relevant to the review.\nStudy designs usually focus on RCTs for intervention reviews, though non-randomized trials, and sometimes observational studies, can also be included. For occupational and public health questions (compared to clinical questions), RCTs may be limited or not feasible, so it is often beneficial to include non-randomized studies.\n\n\n\n\n\n\nPICO Question Exercise\n\n\n\nConsider the following intervention review questions. Identify the PICO elements of each one, and discuss how the questions could be made broader (more inclusive) or more targeted (restrictive).\n\nWhat is the efficacy of different training and education interventions to improve the food safety knowledge, attitudes, and behaviours of food handlers working at retail and food service? (Young et al. 2019)\n\n\nWhat is the effect of workplace-delivered mindfulness-based interventions for non-clinical working populations? (Bartlett et al. 2019) https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=36650\n\n\n\n\n\n2.1.2 Etiology/Risk Factor Reviews\nThese reviews assess the association between an exposure of interest and outcomes of interest. Review questions of this type can be defined using the PECO framework, which is similar to PICO but replaces the intervention with the exposure of interest. These review rely heavily on observational studies (e.g., cohort, case-control), so there are additional concerns related to possible biases that need to be considered at the risk of bias stage.\nExposures could include a risk factor or other determinant of a health outcome. The time period or length of time of the exposure should also be specified if relevant.\n\n\n2.1.3 Prevalence/Incidence Reviews\nThese reviews assess the health or disease burden in a specific population, and are useful to describe the distribution of a health outcome in different locations (e.g., countries) and population groups. This type of review is used in global burden of disease studies. Questions of this type can be expressed using the CoCoPop framework: Condition, Context, and Population (Munn et al. 2015). The population consideration is similar to what is mentioned above for interventions.\nCondition refers to the variable of interest, and could be a disease, other health outcome, symptom, or other factor (e.g., knowledge, behaviour).\nContext refers to the specific setting (e.g., seasons, communities) or geographical areas of interest to the review.\n\n\n\n\n\n\nOther Questions Exercise\n\n\n\nConsider the following review questions. Identify the elements of each one, and discuss how the questions could be made broader (more inclusive) or more targeted (restrictive).\n\nWhat is the evidence on the association between long-term exposure to residential green spaces and mortality in adults? (Rojas-Rueda et al. 2019)\n\n\nWhat is the global prevalence and incidence of tinnitus? (Jarach et al. 2022)\n\n\nWhat is the global prevalence of occupational morbidity in migrant workers, and what are the occupational health risks and outcomes associated with specific industries? (Hargreaves et al. 2019)\n\n\nWhat is the evidence for the association between (1) silicosis and pulmonary tuberculosis and (2) silica exposure and pulmonary tuberculosis, excluding or controlling for radiological silicosis (Ehrlich et al. 2021)"
  },
  {
    "objectID": "2_question.html#additional-eligibility-criteria",
    "href": "2_question.html#additional-eligibility-criteria",
    "title": "2  Review Question and Eligibility Criteria",
    "section": "2.2 Additional Eligibility Criteria",
    "text": "2.2 Additional Eligibility Criteria\n\n2.2.1 Document Type\nOne of the first considerations in deciding which types of studies will be eligible for inclusion in the review is the document source. Most of your relevant articles will be peer-reviewed journal articles. Other documents to consider include:\n\nConference papers or abstracts: care must be taken to avoid double-counting studies that are subsequently published as journal articles. Also, information in an abstract is often very limited.\nDissertations and theses: may be useful if the research was not published. Research has shown that disserations are often not published. But the research may not have been peer-reviewed (especially master’s theses) beyond the advisory committee, and the documents can be very long and onerous to review.\nResearch reports: unpublished reports, such as those published by government and non-profit agencies, could be relevant depending on the review topic.\n\n\n\n\n\n\n\nUnit of analysis\n\n\n\nIn systematic reviews, the unit of analysis is individual studies, which may be reported in multiple references or reports. These should be identified and linked together, usually easiest at the full-text assessment stages. They can be identified by noting similar author names, locations/settings, study details, and results.\n\n\n\n\n2.2.2 Language\nLanguage of articles is another important consideration. Limiting inclusion to only articles in English can lead to a language bias. While practical considerations may limit ability to review in additional languages, it is important to at least capture such articles in the search strategy (if possible) and report on the number of articles excluded due to language. The role of language and the impact of excluding non-English articles likely depends on the specific topic being investigated.\nTools such as Google Translate have been found to be quite reliable for data extraction in systematic reviews. These translation capabilities and other tools (e.g., large language AI models) are likely to continue to improve in the future.\n\n\n\n\n\n\nReview Question and Eligibility Criteria Exercise\n\n\n\nUsing the article you identified for your homework exercise from last week, determine:\n\nThe type of review conducted (using the categorization system described above)\nThe review question\nThe specific elements of the question (e.g., population, intervention/exposure)\nOther eligibility criteria (e.g., document type, language)\n\n\n\n\n\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees. 2021. “The Association Between Silica Exposure, Silicosis and Tuberculosis: A Systematic Review and Meta-Analysis.” BMC Public Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nHargreaves, Sally, Kieran Rustage, Laura B Nellums, Alys McAlpine, Nicola Pocock, Delan Devakumar, Robert W Aldridge, et al. 2019. “Occupational Health Outcomes Among International Migrant Workers: A Systematic Review and Meta-Analysis.” The Lancet Global Health 7 (7): e872–82. https://doi.org/10.1016/S2214-109X(19)30204-9.\n\n\nJarach, Carlotta M., Alessandra Lugo, Marco Scala, Piet A. van den Brandt, Christopher R. Cederroth, Anna Odone, Werner Garavello, Winfried Schlee, Berthold Langguth, and Silvano Gallus. 2022. “Global Prevalence and Incidence of Tinnitus: A Systematic Review and Meta-analysis.” JAMA Neurology 79 (9): 888–900. https://doi.org/10.1001/jamaneurol.2022.2189.\n\n\nMunn, Zachary, Sandeep Moola, Karolina Lisy, Dagmara Riitano, and Catalin Tufanaru. 2015. “Methodological Guidance for Systematic Reviews of Observational Epidemiological Studies Reporting Prevalence and Cumulative Incidence Data.” International Journal of Evidence-Based Healthcare 13 (3): 147–53. https://doi.org/10.1097/XEB.0000000000000054.\n\n\nMunn, Zachary, Cindy Stern, Edoardo Aromataris, Craig Lockwood, and Zoe Jordan. 2018. “What Kind of Systematic Review Should I Conduct? A Proposed Typology and Guidance for Systematic Reviewers in the Medical and Health Sciences.” BMC Medical Research Methodology 18 (1): 5. https://doi.org/10.1186/s12874-017-0468-4.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "3_searching.html#peer-reviewed-literature",
    "href": "3_searching.html#peer-reviewed-literature",
    "title": "3  Search Strategy",
    "section": "3.1 Peer-Reviewed Literature",
    "text": "3.1 Peer-Reviewed Literature\nThe most important place to search for potentially relevant studies for a systematic review is bibliographic databases. The list of databases accessible through the TMU library is avialable here. The selection of which and how many databases to search should be conducted in consultation with a librarian. Some common options include:\n\nMEDLINE: is publicly accessible through the PubMed interface, or the OVID interface via the TMU library. It contains &gt;35 million references for biomedical (including occupational and public health) literature.\nEmbase: is a biomedical database with &gt;30 million references, hosted by Elsevier.\nScopus: is a multi-disciplinary database that covers multiple research fields, including all journals indexed in MEDLINE. It covers &gt;14,000 scholarly sources.\nWeb of Science: is another multi-disciplinary database containing references from &gt;8500 international research journals.\n\nIn addition to the above, subject-specific and specialized databases may be useful to search depending on the topic. Examples of these include: PsycINFO for psychological literature, CINAHL for nursing and allied health literature, OSH References Collection for occupational health and safety literature, and Food Science and Technology Abstracts for food safety and nutrition literature. There are no specific rules for the number of databases to include, as this will depend on the review question, topic, and other factors, but typically reviews might search 3-7 different databases."
  },
  {
    "objectID": "3_searching.html#search-algorithm",
    "href": "3_searching.html#search-algorithm",
    "title": "3  Search Strategy",
    "section": "3.2 Search Algorithm",
    "text": "3.2 Search Algorithm\nA search algorithm needs to be developed based on the review question, and implemented in each bibliographic database. The algorithm should be developed and tested in one of the databases before being adapted to the other databases. Developing a search algorithm for a systematic review is very complex and librarian assistance is strongly recommended.\nPrior to developing a search algorithm, it is useful to identify some seed articles (i.e., examples of relevant articles). These are articles that you would expect to identify and include as relevant in your review based on the review question and eligibility criteria. Specifically, the title, abstract, and keywords of these articles should be reviewed to identify keywords that can be used to build the search algorithm. The keywords can also be used to help develop controlled vocabulary terms (see below for more details). Once the algorithm is fully developed, it should be tested in a database to ensure that it captures each of the seed articles.\nThe search algorithm should aim to balance sensitivity (i.e, the proportion of relevant studies identified from all relevant studies) with precision (i.e, the proportion of relevant studies identified from all studies/references identified). Increasing the sensitivity of the search will reduce its precision, and lead to a need to review more references.\nThe search algorithm will typically include combinations of search terms for each category of the question. For example, an intervention review would have search terms related to the population, the intervention, and the outcome, with additional potential terms for the topic or setting or other criteria. Within each category, terms are combined in parentheses (or through multiple individual searches) with the OR Boolean operator, and across categories with the AND operator. A NOT operator can also be used to exclude specific terms, but should be used with caution. To search for an extra phrase, it should be placed within quotation marks (e.g., “food safety”, “occupational health”).\n\n\n\n\n\n\nExample Search Algorithms\n\n\n\nAn example search algorithm from Young et al. (2019) is shown below:\n\nAnother example is shown below for an occupational health risk factor review (Ehrlich et al. 2021), as implemented in PubMed:\n\n\n\nIn addition to the Boolean operators above, you can also consider proximity operators, which will search for a word within a certain number of places of another word (e.g., food adj3 safety in the OVID platform will search for those terms within three places of other).\nMany databases have controlled vocabulary (i.e., standardized subject terms) that should also be searched as part of the search strategy in addition to text words (i.e., keywords). In MEDLINE, these are called MeSH terms. Finally, specific limits can be placed on certain search fields, such as publication dates or languages, if desired. The approach to implement such limits varies by database. An example of implementing a complex search in MEDLINE accessed through OVID is available here."
  },
  {
    "objectID": "3_searching.html#mehs-headings-examples",
    "href": "3_searching.html#mehs-headings-examples",
    "title": "3  Search Strategy",
    "section": "3.3 MeHS Headings Examples",
    "text": "3.3 MeHS Headings Examples\nYou can see all MeSH terms related to public health in the following tree branch.\nIf using the PubMed interface, you can include a MeSH term in your search by adding ‘MeSH’ in square brackets beside the term (e.g., “Public Health”[Mesh])."
  },
  {
    "objectID": "3_searching.html#saving-search-results-and-de-duplication",
    "href": "3_searching.html#saving-search-results-and-de-duplication",
    "title": "3  Search Strategy",
    "section": "3.4 Saving Search Results and De-duplication",
    "text": "3.4 Saving Search Results and De-duplication\nOnce the searches are implemented, the records can be saved as RIS (or other) files and imported into a reference management program, systematic review management program, or de-duplication tool for de-duplication. Some databases have restrictions on how many references can be exported at once, in which case the results will need to be exported in batches. All results should be documented, including saving the exact algorithm used in each database, search dates, and number of hits in each database.\nSome systematic review management programs (see below) will automatically de-duplicate references (e.g., Covidence). Reference management programs (e.g., Zotero, Mendeley) can also be used for de-duplication, with the de-duplicated file exported and imported into the systematic review management program of choice. Finally, there is an online Systematic Review De-duplicator Tool that can be used for this purpose.\n\n\n\n\n\n\nSearch Exercise\n\n\n\nUsing Scopus via the TMU Library, conduct an “Advanced document search” by reproducing the following algorithm from Young et al. (2019).\nTITLE-ABS-KEY((\"food safety\" OR \"food-borne\" OR foodborne OR \"food poisoning\" OR “food hygiene” OR “safe food” OR HACCP) AND (handler* OR worker* OR employee* OR manager* OR chef* OR cater* OR vendor* OR staff OR service OR business* OR restaurant* OR premise* OR retail) AND (behaviour* OR behavior* OR practice* OR attitude* OR knowledge OR belief* OR perceive* OR perception OR understanding OR violation*) AND (intervention OR train* OR educat* OR course))\n\nHow many hits did you get?\nTry setting a publication date restriction to exclude publications from 2018 to present to approximately reproduce the original search (which was conducted in January, 2018). Now how many hits to do you get?\nNow try saving the results as an RIS file. When exporting, make sure you select the abstract and keywords fields to include in your exported file. Try importing it into a reference management program such as Zotero Mendeley.\nFinally, how might you translate this search algorithm into MEDLINE (OVID)? You can access MEDLINE through the TMU OVID account? What MeSH terms might you use for this algorithm?"
  },
  {
    "objectID": "3_searching.html#grey-literature",
    "href": "3_searching.html#grey-literature",
    "title": "3  Search Strategy",
    "section": "3.5 Grey Literature",
    "text": "3.5 Grey Literature\nIt may be useful to search for grey literature as part of your search strategy. This may help to identify studies that are not published in peer-reviewed journal articles, or those not indexed in major bibliographic databases. Some grey literature search options are noted below:\n\nGoogle search: a series of simple Google searches may help to identify unpublished studies. The search algorithm will need to be simplified for these searches. Limits will need to be placed on the number of hits reviewed (e.g., top 50-100) to be this manageable and balance disminishing returns.\nGoogle Scholar: this will specifically search for scholarly publications, most of which should be captured by your bibliographic database search, but could uncover additional studies.\nOpenGrey: this is a grey literature database in Europe that contains reports, dissertations, and other types of research documents.\nTargeted websites of organizations: you can search through the websites of specific organizations (e.g., governments, non-profit groups) that are known to publish research reports or other documents that might be relevant. This can also include targeted searching of the conference proceedings from prior relevant conferences (if not already indexed elsewhere)."
  },
  {
    "objectID": "3_searching.html#search-verification-and-peer-review",
    "href": "3_searching.html#search-verification-and-peer-review",
    "title": "3  Search Strategy",
    "section": "3.6 Search Verification and Peer Review",
    "text": "3.6 Search Verification and Peer Review\nIt is important to verify that your search did not miss any important studies. One common method used for verification is to search the reference lists of all of your relevant articles (this can be done after you have reached the data extraction stage). You can also search the reference lists of other literature review articles conducted on the same topic as your review.\nIt is increasingly recommended to consider having your search strategy peer reviewed for accuracy before implementation. The Peer Review of Electronic Search Strategies (PRESS) Statement can be used as a checklist for this purpose."
  },
  {
    "objectID": "3_searching.html#review-management",
    "href": "3_searching.html#review-management",
    "title": "3  Search Strategy",
    "section": "3.7 Review Management",
    "text": "3.7 Review Management\nTo facilitate the management of the relevance screening and data extraction steps, it is highly recommended to use a systematic review management program. Some commonly used programs include:\n\nCovidence\nDistillerSR\nEPPI-Reviewer\nRayyan\n\nMost of these programs require a subscription to use, though some may offer free trials or discounts for students (e.g., Covidence). Other non-subscription based options could include spreadsheets or online forms (e.g., Google Forms), but these would be challenging to use for larger projects and could be prone to errors.\nAn example of a Covidence project and interface is shown below. TMU currently does not offer an institutional subscription to Covidence, though this may be provided in future years. Covidence does provide a free trial for students, which allows screening of up to 500 references and up to two reviewers per project. As such, it can be used for your course assignment 2."
  },
  {
    "objectID": "3_searching.html#homework",
    "href": "3_searching.html#homework",
    "title": "3  Search Strategy",
    "section": "3.8 Homework",
    "text": "3.8 Homework\nFor next week, and to help with preparing your second assignment, identify a few different seed articles related to your proposed systematic review topic. Use these seed articles, along with your review question, to identify some keywords and keyword categories required to start building your search algorithm. We will discuss your findings at the start of next week’s class.\n\n\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees. 2021. “The Association Between Silica Exposure, Silicosis and Tuberculosis: A Systematic Review and Meta-Analysis.” BMC Public Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "4_selecting.html#relevance-screening",
    "href": "4_selecting.html#relevance-screening",
    "title": "4  Selecting Studies",
    "section": "4.1 Relevance Screening",
    "text": "4.1 Relevance Screening\nA relevance screening form should be developed, pilot tested, and included in the review protocol. It is easiest and most straightforward for this to contain only one key question if possible to streamline the process. The form should explicitly identify the eligibility criteria and any key definitions. An example from Young et al. (2019) is shown below:\n\n\n\nRelevance Screening Form Example\n\n\n\n4.1.1 Pilot Testing\nThe relevance screening form should be pilot tested before use to ensure the eligibility and inclusion decisions are valid and reliable. The recommended process is as follows:\n\nPurposively select a small number of abstracts for pre-testing (e.g, 30-50) to ensure the sample includes those known to be relevant, not relevant, and potentially relevant\nEach reviewer assesses each abstract using the form and records their results\nAssess agreement in the pilot test using the kappa statistic\nA kappa of &gt;0.8 is often recommended for “almost perfect agreement” (e.g, Sim and Wright 2005)\n\n\n\n\n\n\n\nKappa Agreement Example\n\n\n\nWe will simulate reviewer inclusion/exclusion (i.e., yes/no) ratings for a sample of 30 abstracts. Assume that Reviewer 1 included 50% pf the abstracts, including two that Reviewer 2 did not include. Assume Reviewer 2 included 16 abstracts, including three of the abstracts that Reviewer 1 did not include.\n\n\nCode\n# Create simulated rating data\nreviewer1 &lt;- rep(0:1, each = 15)\nreviewer2 &lt;- c(rep(0, times = 12), rep(1, times = 16), rep(0, times = 2))\n\n# Bind the two dataframes together\nrs_test &lt;- cbind(reviewer1, reviewer2)\nrs_test\n\n\n      reviewer1 reviewer2\n [1,]         0         0\n [2,]         0         0\n [3,]         0         0\n [4,]         0         0\n [5,]         0         0\n [6,]         0         0\n [7,]         0         0\n [8,]         0         0\n [9,]         0         0\n[10,]         0         0\n[11,]         0         0\n[12,]         0         0\n[13,]         0         1\n[14,]         0         1\n[15,]         0         1\n[16,]         1         1\n[17,]         1         1\n[18,]         1         1\n[19,]         1         1\n[20,]         1         1\n[21,]         1         1\n[22,]         1         1\n[23,]         1         1\n[24,]         1         1\n[25,]         1         1\n[26,]         1         1\n[27,]         1         1\n[28,]         1         1\n[29,]         1         0\n[30,]         1         0\n\n\nNow we can evaluate the kappa agreement between the two reviewer’s ratings.\n\n\nCode\npacman::p_load(irr)\n\nkappa2(rs_test)\n\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 30 \n   Raters = 2 \n    Kappa = 0.667 \n\n        z = 3.66 \n  p-value = 0.000253 \n\n\nThe agreement is moderate, and below an ideal level of 0.8 or higher.\nWhat happens if we re-run the simulation but this time each reviewer only differs by one abstract each (i.e., there are only two conflicts)?\n\n\nCode\n# Create the new simulated rating data\nreviewer1 &lt;- c(rep(0, times = 15), rep(1, times = 15))\nreviewer2 &lt;- c(rep(0, times = 14), rep(1, times = 15), rep(0, times = 1))\nrs_test &lt;- cbind(reviewer1, reviewer2)\n\nkappa2(rs_test)\n\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 30 \n   Raters = 2 \n    Kappa = 0.867 \n\n        z = 4.75 \n  p-value = 2.07e-06 \n\n\nThis is now an acceptable level of reviewer disagreement.\n\n\n\n\n4.1.2 Systematic Review Management Software\nSystematic management programs, such as Covidence, automatically calculate a kappa agreement statistic, along with other inter-rater reliability measures. The relevance screening form in Covidence defaults to having three options: yes, no, or maybe. Answers of maybe are considered the same as yes for screening and inter-rated reliability calculation purposes. For Covidence, a CSV file can be downloaded to show the screening reliability statistics for each pair of reviewers. An example of this output is shown in the figure below.\n\n\n\n\n\n\n\nRelevance Screening Exercise\n\n\n\nUsing the example relevance screening form above, from Young et al. (2019), assess the relevance of the following five abstracts captured in the search from that review and record your answers for discussion."
  },
  {
    "objectID": "4_selecting.html#article-procurement",
    "href": "4_selecting.html#article-procurement",
    "title": "4  Selecting Studies",
    "section": "4.2 Article Procurement",
    "text": "4.2 Article Procurement\nFollowing relevance screening, full-text PDFs should be obtained for all references considered relevant. These can be saved to a folder and uploaded/linked to the applicable reference IDs in the systematic review management program used.\n\n\n\n\n\n\nFinding Articles\n\n\n\nYou should be able to identify most PDFs using a combination of simple Google Searching (for open-access papers) and the TMU scholarly paper search feature.\nOccasionally, you may need to search for TMU access to the specific journal of interest, then search for the paper in specific databases if the global search does not find the paper(s) you need.\nIn cases where you cannot find the paper via these methods, you can ask the librarian for assistance, or make a special request for the paper through the Interlibrary Loan Service: RACER."
  },
  {
    "objectID": "4_selecting.html#article-characterization",
    "href": "4_selecting.html#article-characterization",
    "title": "4  Selecting Studies",
    "section": "4.3 Article Characterization",
    "text": "4.3 Article Characterization\nIt is often useful to include a separate screening confirmation (sometimes referred to as second-level screening) step and an article characterization step (i.e., data charting). The relevance confirmation step can be used to exclude irrelevant papers, which can be combined with article characterization to subsequently extract key characteristics of the relevant articles. This step is sometimes combined with data extraction, where study outcomes are also extracted. The confirmation of relevance should include a checkbox to indicate the primary reason for exclusion for articles that are not relevant.\nThe data captured in this form should be summarized in a characteristics of included studies table in the review results. Additionally, some of the variables captured in this form may also be used in the analysis (e.g., subgroup analysis or meta-regression). Some frequently important variables to capture include:\n\nDocument type, year of publication, language\nStudy design\nStudy dates and location(s)\nRecruitment and sampling methods\nCharacteristics of participants (e.g., age, gender)\nCharacteristics of the intervention or exposure (e.g., dose, length, how they were measured)\nCharacteristics of the control or comparison group (if applicable)\nTypes of outcomes measured and how they were measured\n\nIt is often useful to develop outlines of the anticipated figures and tables that you plan to include in the results section of your systematic review. This will help you to decide on which important characteristics to extract.\n\n\n\n\n\n\nMultiple reports of the same study\n\n\n\nIt is sometimes the case that the same study is reported in multiple publications. In the case of authors splitting results of a study across multiple journal papers, it might be easiest to treat these as separate studies at this stage and note that they are from the same larger study. In cases where preliminary data are published in a thesis or abstract, usually you will want to keep only the published journal article as the main source for that study.\n\n\n\n4.3.1 Pilot Testing\nIt is also important to pilot test the article characterization form. Usually this can be done on a small selection of articles (e.g., 5-10). Instead of checking kappa agreement, you should compare reviewer answers, look for discrepancies in interpretation of questions, discuss, and modify the form to enhance its clarify and ensure consistent interpretation.\n\n\n\n\n\n\nArticle Characterization Exercise\n\n\n\nUse the example article characterization form from Young et al. (2019) to extract data from the three full-text articles captured and screened as potentially relevant in that review]. Then record your answers for discussion.\nThe three articles are available at the following shared folder: https://drive.google.com/drive/folders/1mvBj074odovMpWIQ-UQ9BJprNzRiB9cv?usp=sharing.\n\n\n\n\n\n\nSim, Julius, and Chris C Wright. 2005. “The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements.” Physical Therapy 85 (3): 257–68. https://doi.org/10.1093/ptj/85.3.257.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "5_bias.html#risk-of-bias",
    "href": "5_bias.html#risk-of-bias",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.1 Risk of bias",
    "text": "5.1 Risk of bias\nThere is strong evidence that certain study design features (e.g., lack of blinding in clinical studies, lack of allocation concealment) can lead to a higher risk of bias, which can affect the magnitude and direction of the results. Biases can be introduced into a study through different mechanisms. These can be classified into different domains, which form the basis for risk-of-bias assessment tools. Risk of bias should be assessment differently for each unique outcome in a review, because some domains (e.g., blinding) may be important for some outcomes (e.g., subjective measures) but less important for others (e.g., death).\nRisk-of-bias assessments should use recommended, and ideally validated tools, and should be conducted by two independent reviewers, especially as judgement are frequently required in these assessments. Pilot testing of risk-of-bias assessment tools is important for all reviewers involved in a review (e.g., on 3-5 papers) to ensure criteria are applied consistently."
  },
  {
    "objectID": "5_bias.html#cochrane-tool-for-rcts",
    "href": "5_bias.html#cochrane-tool-for-rcts",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.2 Cochrane Tool for RCTs",
    "text": "5.2 Cochrane Tool for RCTs\nThe Cochrane Collaboration has developed a structured, domain-based tool to assess risk of bias in RCTs (Sterne et al. 2019). The tool, called RoB 2, uses a series of signalling questions to determine the risk of bias within each domain. Questions are answered with options of “yes”, “probably yes”, “probably no”, “no”, and “no information”. There are also variations of the tool available for cluster-randomized trials and crossover trials. As noted above, the tool should be applied at the outcome level, so multiple assessments may be needed for a study if it reports multiple relevant outcomes.\nBased on the results of the signalling questions, each domain will then receive a risk-of-bias judgement (or rating) of low, some concerns, or high. The tool includes flow charts to guide which judgement is most appropriate given the responses to the signalling questions. Each signalling question also contains a free-text box to record supporting information about the answer. Reviewers can copy and paste direct quotes from the article in this section to support their answers and add transparency. Optionally, reviewers can make a judgement about the likely direction of bias for each domain and overall, if this is possible to assess.\nOnce the risk of bias of each domain is assessed, an overall risk-of-bias judgement is made for the study-specific outcome. The overall risk judgement cannot be lower than the judgement assigned to any individual domain (e.g., if one domain is judged as “some concerns” and all others are “low risk”, then the overall risk must be at least “some concerns”).\nOne of the first decisions in applying this tool is determine whether you are interested in the effect of assignment to the interventions at baseline (i.e., intention-to-treat effect), or the effect of adhering to the interventions as specified in the protocol (i.e., per-protocol effect). The former is usually more important for occupational and public health reviews, as it reflects population-level realities of intervention administration.\n\n5.2.1 RCT Bias Domains\nBias arising from the randomization process This domain assesses whether intervention allocation process was random, and whether it was adequately concealed. Proper randomization is critical to mitigate the possibility of confounding bias. Randomization can be conducted in multiple different ways (e.g., simple, blocked, stratified, minimization). It is also important to prevent participants or trial personnel from knowing the next assignment (e.g., intervention or control), as this can enable selective enrollment. Baseline differences between intervention groups can be examined, and if identified (beyond what is expected by chance), suggest a problem with the randomization process.\nBias due to deviations from intended interventions This domain assesses whether there were any important deviations from the intended interventions that could bias results. For example, if participants or trial personnel were not “blinded” to the intervention group status, this could affect certain outcomes (e.g., behaviours). If blinding was not conducted or not possible, the possibility that deviations arose because of the trial context should be considered. If so, whether these deviations could have affected the outcome should also be considered, including whether any deviations were balanced across groups. This domain also considers whether the analysis correctly followed intention-to-treat principles (assuming the interest is in the effect of assignment to interventions).\nBias due to missing outcome data This domain assesses whether outcome data were available for all, or nearly all, of the trial participants. For continuous outcomes, data from at least 95% of participants will often be sufficient to mitigate missing data biases. For dichotomous outcomes, the proportion of acceptable missing data depends on the risk of the event. If missing data are a possible concern, we should look for evidence in the study that the result was not biased due to the missing data (e.g., from bias-corrected analysis or sensitivity analysis). We can use additional information in the study to determine whether missing data could or likely depended on the true value of the outcome, including any differences between groups in missing data, reported reasons for missing data, and circumstances of the trial.\nBias in measurement of the outcome This domain assesses the appropriateness of the method of measuring the outcome, whether the measurement method differed across intervention groups, and whether outcome assessors were blinded to the intervention group status of participants. It aims to determine if there were any differential measurement or misclassification errors. If outcome assessors were not blinded to group status, then it should be considered whether this knowledge could have influenced their assessment.\nBias in selection of the reported result This domain assesses whether the result reported in the study is likely to have been selected from multiple outcome measurements or multiple analyses (e.g., due to being statistically significant). Assessors should consider whether the trial was analyzed in accordance with a pre-specific analysis plan (e.g., published protocol). Evidence of selecting reporting should be examined, including reporting only one or a subset of multiple instruments, scales, time points, or analyses.\n\n\n\n\n\n\nRCT Risk-of-Bias Example\n\n\n\nAn example of the outcome-specific risk-of-bias assessment results for two studies from a systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). In this review, separate assessments were made for objective (e.g., blood pressure) and subjective (e.g., self-reported anxiety) outcomes.\n\n\n\nExample 1: Liao 2013\n\n\n\n\n\nExample 2: Chen 2013\n\n\n\n\n\n\n\n\n\n\nRCT Risk-of-Bias Exercise\n\n\n\nConsider the Young et al. (2019) systematic review about the effectiveness of food handler training interventions. Using two example RCT articles from that review, conduct a risk-of-bias assessment using the Cochrane RoB tool. For the Mancini et al. article, the outcome to assess is food inspection scores, while for Richard et al., the relevant outcome is food safety knowledge.\n\nWhat is your risk-of-bias judgement for each domain?\nWhat is your overall risk-of-bias judgement for each study?"
  },
  {
    "objectID": "5_bias.html#non-randomized-and-observational-study-designs",
    "href": "5_bias.html#non-randomized-and-observational-study-designs",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.3 Non-Randomized and Observational Study Designs",
    "text": "5.3 Non-Randomized and Observational Study Designs\nNon-randomized studies may be important to include in reviews of the effects of interventions. For example, for population-level interventions (e.g., legislation changes) and investigations of long-term or rare outcomes are often more feasible to investigate in non-randomized studies. Non-randomized intervention studies are often at a higher risk of bias than RCTs due to the lack of randomization and other study features. The Cochrane Collaboration has developed a special risk-of-bias tool called ROBINS-I to evaluate risks of bias in non-randomized studies.\nObservational studies are critical to answer questions about exposures and health outcomes. For these types of questions, RCTs are often not feasible or ethical to conduct, and they suffer other limitations (e.g., shorter follow-up times). An international collaboration has developed a new risk-of-bias assessment tool for observations studies in exposure/risk factor reviews, called ROBINS-E. ROBINS-E is based on the same framework and approach as the Cochrane RoB tool and ROBINS-I. We will consider how to apply this tool below. However, it is designed for cohort studies, so for reviews that focus on other designs (e.g.., case-control studies), other tools may be needed.\n\n5.3.1 ROBINS-E Tool\nOne of the first steps in using the ROBINS-E tool is to pre-specify important confounding factors that are likely to influence the association between the exposure and outcome of interest. These will be specified for each outcome in each study and details recorded in Part E. The outcome of interest is first identified (Part A), then studies are screened to assess if they are at very high risk of bias (Part B), which would avoid the need for a detailed assessment. This involves answering key questions from the tool about whether confounding was controlled for and may be a concern, and whether the method of measuring exposure and outcome was inappropriate.\nFor each study, details of the participants, exposure, and outcome are extracted (Part C). Then the causal effect estimated by the study must be specified (Part D). This represents the target effect assuming no potential for confounding (e.g., target experiment). Bias risks are then assessed across seven domains using signalling questions (Part E), and an overall risk judgement is also determined. Risks of bias in each domain is assessed as low, some concerns, high, or very high. The predicted direction of bias and whether it may threaten conclusions is also assessed for each outcome.\nThere is a spreadsheet version of the tool online you can download and use to help complete the risk-of-bias assessments.\n\n5.3.1.1 ROBINS-E Domains\nRisk of bias due to confounding Confounding occurs when there are common causes of the exposure and outcome of interest. This domain assesses whether the study appropriately measured and controlled for all of the important confounding factors. Common control methods include stratification, matching, regression, standardization, or inverse probability weighting. This domain also assesses whether any post-exposure variables were controlled for in the analysis, which can introduce bias. If time-varying confounding is also a concern, additional questions are used to assess the impact of this bias. Time-varying confounding occurs when the exposure changes over time and prognostic factors affect exposure status during the exposure time period.\n\nEven if all important confounders were identified and controlled for in analysis, there is always a possibility for uncontrolled confounding due to unmeasured factors, imprecise measurement of confounding factors, or because measurement confounding factors were not adjusted for in the analysis. Therefore, there will always be some concerns about uncontrolled confounding even with a low risk judgement.\n\nRisk of bias arising from measurement of the exposure This domain addresses the risk of possible measurement error (continuous variables) or misclassification (categorical variables). Different variants of this domain are used depending on whether there are multiple measures of the exposure over time and how those measurements are used in the analysis. This domain considers how well the exposure variable represents the target exposure of interest, whether the exposure was likely to be measured with error (or was misclassified), and whether any measurement errors or misclassification could have been differential (i.e., related to the outcome or risk of outcome). An example of differential error is recall bias in case-control studies.\nRisk of bias in selection of participants into the study This domain assesses possible selection bias, where some participants, follow-up time of of some participants, or some outcome events are excluded in a way that leads to an association between the exposure and outcome. Questions assess whether selection was based on characteristics observed after the start of the exposure window, and whether these characteristics were related to both the exposure and outcome. Questions also assess whether follow-up began at or close to the start of the exposure window, and whether the exposure is likely to be constant over time. Methods of analysis to correct for potential selection biases are also considered.\nRisk of bias due to post-exposure interventions This domain considers whether the exposure leads to administration of interventions that can alter the exposure-outcome relationship. For example, in a study of individuals exposed to asbestos, the most highly exposed were more likely to receive a CT scan, this could affect their risk of lung cancer mortality. If no interventions are administered to alleviate the effects of the exposure, there should be no issues with this domain.\nRisk of bias due to missing data This domain assesses risks of missing data for exposures, outcomes, or confounding variables, and whether this led to bias. The approach to analysis is considered in making this assessment (complete case analysis or multiple imputation).\nRisk of bias arising from measurement of the outcome This domain assess risks of non-differential or differential measurement errors in the outcome. It considers if measurements could have differed by group, whether outcome assessors were award of participants’ exposure history, and whether this could have affected the outcome measurement.\nRisk of bias in selection of the reported result This domain assess selective reporting of the exposure (from multiple measurement methods), the outcome (from multiple measurement methods), the results (from multiple analysis methods), and the participants (from a larger cohort). Unlike RCTs, it is unlikely that pre-specified analysis plans are available to compare with a study’s results, and clues must be gained from comparing the methods and results sections.\n\n\n\n\n\n\nROBINS-E Example\n\n\n\nA systematic review and meta-analysis of the association between ambient air pollution and clinical dementia used the ROBINS-E tool to assess risk of bias in relevant studies (Wilker, Osman, and Weisskopf 2023). A total of 51 relevant studies were included. Results of the ROBINS-E assessment were summarized in a table. The first several studies of this summary table are shown below:\n\nIn this table, an asterisk (*) Indicates that the likely direction of bias would be towards the null. The table legend is as follows: A = Confounding; B = Post-exposure intervention; C = Missing data; D = Measurement of the outcome. All studies were rated as some concerns in the domains of “Measurement of the exposure” and “Selection of reported results”, all studies were rated as “Low risk” in the “Selection of participants” domain.\n\n\n\n\n\n5.3.2 Other Risk-of-Bias Tools\nThere are various other risk-of-bias tools that have been developed and applied to different study designs. However, not all of these tools have been validated, and some use outdated methods (e.g., scoring systems). For reviews of interventions, the Cochrane Collaboration tools are highly recommended. For reviews of exposures, the ROBINS-E tool is recommended if the predominant design is cohort studies. For other types of reviews, the tools from Joanna Briggs Institute (JBI) are recommended. For example, JBI has developed critical appraisal tools for:\n\nAnalytical cross-sectional studies\nCase-control studies\nCase reports and case series\nPrevalence studies\nQualitative research\nText and opinion articles\nEconomic evaluations, and many others.\n\n\n\n\n\n\n\nObservational Study Risk-of-Bias Exercise\n\n\n\nWe will practice using the ROBINS-E tool on two studies that were included in the systematic review of the association between green spaces and mortality (Rojas-Rueda et al. 2019). The two articles are available in the following shared folder.\nFor the purposes of this exercise, consider confounders of interest to be any of the following socio-economic factors: age, sex, ethnicity, marital status, education, income, and employment.\n\nWhat is your risk-of-bias judgement for each domain?\nWhat is your overall risk-of-bias judgement for each study?"
  },
  {
    "objectID": "5_bias.html#risk-of-bias-for-missing-data",
    "href": "5_bias.html#risk-of-bias-for-missing-data",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.4 Risk-of-Bias for Missing Data",
    "text": "5.4 Risk-of-Bias for Missing Data\nThe risk-of-bias tools covered above include an item to assess possible risks from selective reporting of results when multiple measurements or analyses were made. In cases where the result for an outcome is omitted entirely, the risk of bias should be assessed at the overall synthesis level. The Cochrane Collaboration describes a process that can be used to assess possible biases due to missing data:\n\nSelect syntheses to assess for risk of missing data\nDetermine which studies are eligible for the synthesis\nDetermine whether any of the studies have missing data that cannot contribute to the synthesis.\nConsider whether the synthesis is likely to be biased due to missing data\nConsider whether results from additional studies are likely to be missing.\nReach an overall judgement about risk of bias from missing data in the synthesis.\n\nThis process aims to first determine if any studies might have measured relevant results for a synthesis, but did not report them at all or in a format usable for the synthesis (e.g., meta-analysis). This is especially important if the results were not reported because they were not statistically significant or did not support the authors’ hypotheses. This process may involve checking a study’s protocol (if published), methods, analysis plan, or other supporting documents. A risk-of-bias tool for this purpose is currently under development that would assess these factors at the synthesis level."
  },
  {
    "objectID": "5_bias.html#reporting-risk-of-bias-results",
    "href": "5_bias.html#reporting-risk-of-bias-results",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.5 Reporting Risk-of-Bias Results",
    "text": "5.5 Reporting Risk-of-Bias Results\nRisk-of-bias summary tables or figures should be provided in the systematic review report. A table or plot should be included that contains the risk-of-bias judgement for each domain for each study outcome, or this data can be made available in the supplementary materials if there are many included studies. In that case, it can be useful to provide a summary table in the manuscript text. An example from Young et al. (2019) is shown below. Additionally, the domain-specific judgements or overall judgement can also be included alongside each study in meta-analysis forest plots.\n\nThere is an online R Shiny App called robvis that can be used to create high-quality risk-of-bias figures for the Cochrane Collaboration risk-of-bias tools. The app contains template spreadsheets that can be altered with your review risk-of-bias results, which are then uploaded to create a Cochrane-style risk-of-bias plot as per below.\n\nThe risk-of-bias results should be incorporated into the summary assessment for each outcome in a review. For example, if conducting meta-analysis, a subgroup analysis (Chapter 9) could be conducted to examine differences in measures of effect for studies at different risks of bias. Alternatively, the analysis could be restricted to only studies at low risk of bias, but if this is done, a sensitivity analysis should be conducted to compare the results with those including all of the studies. If all studies are included, the summary risk of bias should be incorporated into a certainty of evidence assessment (Chapter 10) (e.g., certainty of evidence would be lowered if many studies at high risk of bias).\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSterne, Jonathan A. C., Jelena Savović, Matthew J. Page, Roy G. Elbers, Natalie S. Blencowe, Isabelle Boutron, Christopher J. Cates, et al. 2019. “RoB 2: A Revised Tool for Assessing Risk of Bias in Randomised Trials.” The BMJ 366. https://doi.org/10.1136/bmj.l4898.\n\n\nWilker, Elissa H., Marwa Osman, and Marc G. Weisskopf. 2023. “Ambient Air Pollution and Clinical Dementia: Systematic Review and Meta-Analysis.” BMJ 381 (April): e071620. https://doi.org/10.1136/bmj-2022-071620.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "6_extraction.html#types-of-outcomes-and-measures-of-effect",
    "href": "6_extraction.html#types-of-outcomes-and-measures-of-effect",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.1 Types of Outcomes and Measures of Effect",
    "text": "6.1 Types of Outcomes and Measures of Effect\nA key first step in designing the outcome data extraction form and planning for analysis is to determine the relevant types of outcome data measures for the review. The most common types are listed below:\n\nDichotomous (AKA binary)\nContinuous\nOrdinal\nCounts and rates\nTime-to-event (survival data)\n\nEach of these are discussed further below. For each study, we will want to either directly extract a measure of effect (i.e., effect measure, effect size) from the study, or extract the raw data and use that data to calculate a measure of effect. Typically, for reviews of RCTs, the raw data is sufficient for extraction (if available), as the randomization process should mitigate the impact of confounding bias. When extracting data from observational studies and non-randomization trials, typically the adjusted measure of effect should be extracted as it reduces to the impact of confounding variables. Additionally, it is preferable to extract measures of effect directly from RCTs that have adjusted for clustering or baseline measurements.\nMeasures of effect can be expressed as ratios (e.g., odds ratio, risk ratio, hazard ratio) or differences (e.g., mean difference, risk difference). For reviews of prevalence and incidence questions, the measure of effect is simply the prevalence or incidence point estimate(s) of interest.\n\n\n\n\n\n\nRatio Measures of Effect\n\n\n\nRatio measures of effect, such as odds ratios, are analyzed on the natural log scale, so must be log-transformed for all studies prior to analysis."
  },
  {
    "objectID": "6_extraction.html#unit-of-analysis",
    "href": "6_extraction.html#unit-of-analysis",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.2 Unit of Analysis",
    "text": "6.2 Unit of Analysis\nWhen extracting outcomes from studies you will likely encounter unit-of-analysis issues. These are situations where outcomes are nested within larger units, or there are repeated measurements on the same participants, and this extra level of variation must be accounted for when conducting analysis. The following are common situations of this:\n\nThe study has clusters (e.g., schools, communities) with outcomes measured and reported on participants within the cluster\nThere are repeated measurements on participants (e.g., outcomes are reported at multiple follow-up periods)\nEach participant receives multiple treatments or interventions\nThere are multiple variations of the same outcome reported for participants in the same study\nThere are multiple intervention groups compared in the same study\n\nStatistical approaches to address some of these issues will be covered in a later section."
  },
  {
    "objectID": "6_extraction.html#data-extraction-form",
    "href": "6_extraction.html#data-extraction-form",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.3 Data Extraction Form",
    "text": "6.3 Data Extraction Form\nThe data extraction form should be designed so that multiple outcomes can be extracted for each study. The suggested approach is to pre-determine outcome categories for analysis based on your review question. These should represent specific groups of similar outcomes that will combined together in the same analysis. You can also include additional questions on the form that might have difference responses for different outcome categories (e.g., how the outcome was measured). You may also have different categories for intervention or exposure groups of interest.\nAn example of a data extraction form from Young et al. (2019) is available for download here."
  },
  {
    "objectID": "6_extraction.html#dichotomous-measures",
    "href": "6_extraction.html#dichotomous-measures",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.4 Dichotomous Measures",
    "text": "6.4 Dichotomous Measures\nDichotomous outcomes are those with only two possibilities (e.g., disease positive or disease negative). Two common measures of effect for these outcomes are odds ratios (OR) and risk ratios (RR). An RR is a ratio of the risk of the event in each group, while the OR is ratio of the odds of event (compared to a non-event) in each group. The table below shows how these two measures are calculated:\n\n2 X 2 table for an intervention study\n\n\n\nEvent\nNon-Event\nTotal\n\n\n\n\nIntervention\nA\nB\n\\({N_1}\\)\n\n\nControl\nC\nD\n\\({N_2}\\)\n\n\n\n\\[\n\\text{RR} = \\frac{\\text{A}/{N_1}}{\\text{C}/{N_2}}\n\\]\n\\[\n\\text{OR} = \\frac{\\text{A}/{B}}{\\text{C}/{D}}\n\\]\nThe RR and the OR are only similar when the event is rare.\nAnother possible measure for dichotomous outcomes is the risk difference (RD). This measure reflects the difference between the two observed groups risks. The practical significance of the RD depends greatly on the baseline risk of the event in the population.\n\\[\n\\text{RD} = \\left(\\frac{\\text{A}}{N_1}\\right)-\\left(\\frac{\\text{C}}{N_2}\\right)\n\\]\nTo conduct a meta-analysis of dichotomous data, we typically need one of the following sets of values for the intervention (or association) effect:\n\nNumerator and denominator in each group\nProportion + EITHER numerator or denominator in each group, or\nMeasure of effect (e.g., OR, RR) + a measure of variability"
  },
  {
    "objectID": "6_extraction.html#continuous-measures",
    "href": "6_extraction.html#continuous-measures",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.5 Continuous Measures",
    "text": "6.5 Continuous Measures\nContinuous outcomes can theoretically take any value within a specified range. The two most common measures of effect for continuous data are the mean difference (MD) and the standardization mean difference (SMD).\nThe MD can be used when the outcome is reported on a meaningful scale (e.g., weight in kg, blood pressure in mmHg), and the same scale is used in all studies. This measure is easy and intuitive to interpret if possible to calculate.\nThe SMD can be used when the studies in a SR measure the same outcome, but in different ways (e.g., different scales). The SMD is calculated by dividing a study’s mean different by its standard deviation (SD) to create an index. There are different SMD metrics, the most common of which is called Hedges’ g, which uses a pooled SD in the denominator based on outcome data in both groups being compared, and assuming that the SDs of the two groups are similar.\nFor continuous data outcomes, we typically require the mean, sample size, and SD in each group to calculate either the MD or SMD. Alternatively, a pre-calculated measure of effect (e.g., MD) and its measure of variability (e.g., standard error) can also be used directly in a meta-analysis.\n\n\n\n\n\n\nChange from baseline data\n\n\n\nSome studies will report baseline and post-intervention values (often with different follow-up time points). It can be useful to extract both values if they are available and compare any differences. However, often the standard deviation (SD) of the change score is missing and needs to be imputed or estimated to include the score in a meta-analysis. This process involves many assumptions that should be evaluated through sensitivity analysis."
  },
  {
    "objectID": "6_extraction.html#other-outcomes",
    "href": "6_extraction.html#other-outcomes",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.6 Other Outcomes",
    "text": "6.6 Other Outcomes\nOrdinal outcomes occur when there are multiple, ordered categories of an outcome (e.g., low, moderate, and high knowledge levels). As the number of ordinal categories increases, they start to resemble continuous outcomes, and could potentially be analyzed using those methods. If there is an obvious cut-point, the categories could be combined and used in a dichotomous analysis. Otherwise, if the same categories are used across all studies, this outcome can be analyzed using a proportional odds model.\nCount outcomes occur when the outcome can happen multiple times for each participant. These are usually expressed as rates (e.g., number of events per person-year), with the comparison of two rates being expressed as a rate ratio. The most common situation involving rate data is that studies directly report rate ratios and a measure of variability (e.g., standard error) from regression models, which can be extracted and combined in a meta-analysis.\nTime-to-event outcomes refer to the measurement of the time elapsed until some event (e.g., death) is experienced (e.g., survival data). These data are usually analyzed in studies using survival analysis and expressed as a hazard ratio (HR). Hazard measures instantaneous risk and can change continuously over time. As above, studies usually report the HR and a measure of variability which can be extracted directly and included in a meta-analysis.\n\n\n\n\n\n\nData Extraction Exercise\n\n\n\nConsider again the Young et al. (2019) systematic review about the effectiveness of food handler training interventions. Using three example articles, identify the outcomes of interest in each study and which data should be extracted.\n\nHow many relevant outcomes are reported in each study?\nAre any important outcomes or values (e.g., measures of variability) missing?"
  },
  {
    "objectID": "6_extraction.html#data-required-for-meta-analysis",
    "href": "6_extraction.html#data-required-for-meta-analysis",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.7 Data Required for Meta-Analysis",
    "text": "6.7 Data Required for Meta-Analysis\nIf you are interested or planning to conduct a meta-analysis, data must be sufficiently reported in the article to allow calculation of a summary measure of effect and to allow inclusion in a meta-analysis. Often, many studies will be missing information on one of the common inputs required. However, we can thankfully use various formulas to estimate the missing information in many cases. Some common scenarios are highlighted below.\n\n6.7.1 Convert Confidence Interval to a Standard Error (SE)\nIf only a 95% confidence interval (CI) is reported for an absolute measure of effect (e.g., standardized mean difference, risk difference), the SE can be calculated from the following formula:\n\\[\n\\text{SE} = \\frac{(\\text{Upper limit}-\\text{Lower limit})}{3.92}\n\\]\nFor ratio measures (e.g., odds ratio, risk ratio), the upper and lower CI limits, and intervention effect estimate, should be on the natural log scale.\nThe same formula can also be used to convert a CI of a mean value within each group (e.g., intervention, control) to its SE. However, in this case, if the sample size in each group is small (e.g., &lt;60), then the CIs should have been calculated using a t distribution and the divisor (3.92) should be replaced by a different number from a t distribution.\n\n\n6.7.2 Convert SE for Each Group to a SD for Each Goup\nIf only a SE, and not a SD, is reported within each group being compared, the SD can be obtained from this formula:\n\\[\n\\text{SD} = \\text{SE}\\sqrt{N}\n\\]\n\n\n\n\n\n\nExample SE to SD conversion\n\n\n\nWe can illustrate this conversion with some simulated data in R.\n\n\nCode\npacman::p_load(tidyverse)\n\n# Simulate some study data for meta-analysis\ndata &lt;- tibble(study_ID = as.factor(rep(1:4, times = 1)),\n               treat_mean = c(1.5, 2.1, 2.2, 2.7),\n               treat_SE = c(0.03, 0.06, 0.07, 0.08),\n               treat_n = c(100, 50, 70, 60),\n               control_mean = c(1.3, 1.8, 2.0, 2.2),\n               control_SE = c(0.03, 0.07, 0.07, 0.09),\n               control_n = c(100, 50, 70, 50),\n                )\ndata\n\n\n# A tibble: 4 × 7\n  study_ID treat_mean treat_SE treat_n control_mean control_SE control_n\n  &lt;fct&gt;         &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 1               1.5     0.03     100          1.3       0.03       100\n2 2               2.1     0.06      50          1.8       0.07        50\n3 3               2.2     0.07      70          2         0.07        70\n4 4               2.7     0.08      60          2.2       0.09        50\n\n\nNow calculate a SD variable for each group using the formula above.\n\n\nCode\ndata &lt;- data |&gt; mutate(\n  treat_SD = treat_SE*sqrt(treat_n),\n  control_SD = control_SE*sqrt(control_n)\n)\ndata |&gt; select(-treat_SE, -control_SE)\n\n\n# A tibble: 4 × 7\n  study_ID treat_mean treat_n control_mean control_n treat_SD control_SD\n  &lt;fct&gt;         &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 1               1.5     100          1.3       100    0.3        0.3  \n2 2               2.1      50          1.8        50    0.424      0.495\n3 3               2.2      70          2          70    0.586      0.586\n4 4               2.7      60          2.2        50    0.620      0.636\n\n\n\n\n\n\n6.7.3 Convert from SE, CI, t value, or P value to a SD for Mean Differences\nWhen using the raw MD as the outcome, missing SD values can be estimated from a SE, CI, t value, or P value. These conversions assume that the SD values are the same in both groups being compared.\nTo calculate the t value from a P value (from a t-test), we can use the qt function, inputting the P value first (divided by 2), and the degrees of freedom next (equal to the sample size in each group minus 2). For example, for a study with a P value of 0.03 and a sample size of 20 in each group, we could use this formula:\n\n\nCode\nqt(p=0.03/2, df=40-2, lower.tail=FALSE)\n\n\n[1] 2.254648\n\n\nTo convert from a t value to a SE, we can use the following formula:\n\\[\n\\text{SE} = |\\frac{\\text{MD}}{\\text{t value}}|\n\\]\nThe SE can then be converted to a within-group SD using the following formula, where \\(N_1\\) and \\(N_2\\) represent the sample size in each group:\n\\[\n\\text{SD} = \\frac{\\text{SE}}{\\sqrt{\\frac{1}{N_1}+{\\frac{1}{N_2}}}}\n\\]\nThe SD is the average of the SDs of each group, and should be entered for both groups.\n\n\n6.7.4 Conversions to a Standardized Mean Difference\nThere are various formulas available to convert from SEs, ANOVA F values, t-test values, regression beta coefficients to SMD measures such as Hedges’ g, a log odds ratio, or a correlation coefficient r.\nThese use formulas from the practical meta-analysis effect size calculator and are implemented in the esc package in R. Some examples are given in the Doing Meta-Analysis in R online textbook.\nTwo examples are shown below for a one-way ANOVA F test and for a \\(chi^2\\) test.\n\n\n\n\n\n\nExample F Test and \\(chi^2\\) Test Converison\n\n\n\nSuppose we have a study that only reports the F value from a one-way ANOVA test, instead of means and SDs within each group or a MD value. If the F value is 6.23, and the sample size is 190 in the intervention group and 80 in the control group, we can calculate the SMD (as Hedges’ g) as follows:\n\n\nCode\npacman::p_load(esc)\n\nesc_f(f = 6.23, grp1n = 190, grp2n = 80, es.type = \"g\")\n\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: F-value (one-way-Anova) to effect size Hedges' g\n    Effect Size:   0.3317\n Standard Error:   0.1340\n       Variance:   0.0180\n       Lower CI:   0.0690\n       Upper CI:   0.5945\n         Weight:  55.6542\n\n\nSimilarly, if we had study that only reported a \\(chi^2\\) test value for a dichotomous association, we could convert that to a log odds ratio and its SE. Note that this conversion assumes a degrees of freedom of 1. For example, suppose the \\(chi^2\\) value was 2.5 and the sample size was 20:\n\n\nCode\nesc &lt;- esc_chisq(chisq = 2.5, totaln = 20, es.type = \"logit\")\nesc\n\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: chi-squared-value to effect size logits\n    Effect Size:   1.3711\n Standard Error:   0.8672\n       Variance:   0.7520\n       Lower CI:  -0.3285\n       Upper CI:   3.0707\n         Weight:   1.3298\n\n\nWe can then load the data from the saved object into the applicable row in the dataset. For example, suppose the previous conversion was conducted for Study 3 as per the mock dataset below:\n\n\nCode\ndata2 &lt;- tibble(study_ID = as.factor(rep(1:3, times = 1)),\n               log_or = c(1.1, 0.8, NA),\n               log_se = c(0.05, 0.04, NA),\n               n = c(100, 150, 150)\n                )\ndata2\n\n\n# A tibble: 3 × 4\n  study_ID log_or log_se     n\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1           1.1   0.05   100\n2 2           0.8   0.04   150\n3 3          NA    NA      150\n\n\nWe can then use the following code to insert the saved conversion values into the applicable columns for Study 3:\n\n\nCode\ndata2$log_or[\"Study ID\" = 3] &lt;- esc$es\ndata2$log_se[\"Study ID\" = 3] &lt;- esc$se\ndata2\n\n\n# A tibble: 3 × 4\n  study_ID log_or log_se     n\n  &lt;fct&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 1          1.1   0.05    100\n2 2          0.8   0.04    150\n3 3          1.37  0.867   150\n\n\n\n\n\n\n\n\n\n\nData Conversions Exercise\n\n\n\nUsing the systematic review article you identified earlier in the course, review the article to examine whether meta-analysis was conducted. If it wasn’t, you can answer the following questions based on this systematic review and meta-analysis of the effectiveness of workplace mindfulness training interventions (Bartlett et al. 2019). Consider the following questions for the article analyzed:\n\nWhat was the main outcome measure extracted and analyzed?\nDid the authors report whether any data conversions were necessary? If so, which ones, how many, and how were they conducted?\nDid the authors provide any references or rationale for any conversions conducted?\nWere any sensitivity analyses conducted related to the data conversions or assumptions made?\n\n\n\n\n\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "7_analysis.html#review-flow-chart",
    "href": "7_analysis.html#review-flow-chart",
    "title": "7  Data Analysis",
    "section": "7.1 Review Flow Chart",
    "text": "7.1 Review Flow Chart\nUsually the first part of data analysis and reporting results includes summarizing details of the number of studies captured in the review and included/excluded at each step. These details should be shown through a flow chart diagram or figure. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) reporting guideline has a template for flow charts. A copy of the flow chart template is shown below:\n\nThe flow chart should capture exact numbers of references identified from each source (e.g., bibliographic databases, grey literature, etc.) It should clearly indicate the number of references excluded and included at each step of the review, along with reasons for excluding references or articles. There is also an online Shiny app for creating flow charts using this template. An example flow chart from Young et al. (2019) is show below."
  },
  {
    "objectID": "7_analysis.html#descriptive-analysis-of-article-characterization-results",
    "href": "7_analysis.html#descriptive-analysis-of-article-characterization-results",
    "title": "7  Data Analysis",
    "section": "7.2 Descriptive Analysis of Article Characterization Results",
    "text": "7.2 Descriptive Analysis of Article Characterization Results\nThe next step in analysis is usually to descriptively summarize the article characteristics (e.g., PICO/PECO elements, and other relevant article and study details). It can be helpful at this stage to start categorizing the PICO/PECO elements into groups that can facilitate summary displays (e.g., tables) and further analysis. For example, specific types of interventions and outcomes might be grouped together. A table can then be presented that has each relevant study as a row and specific columns for each of the relevant PICO/PECO elements or groups. An example of such a table from Bartlett et al. (2019) is shown below (note this shows only the first several studies included in the table):\n\nIn addition to showing a table with a the summary characteristics and PICO/PECO details for each study, in can also be useful, especially in cases where the number of relevant studies included in the review is very large, to create a tabulation table to show overall numbers and percentages of studies that investigated specific PICO/PECO elements and reported specific details. An example of such a table from Young et al. (2019) is shown below (note that only the first several rows of the table are shown here). In the example below, we will examine how to reproduce such a table in R.\n\n\n\n\n\n\n\nSummary Article Characterization Table Example\n\n\n\nAs an illustrative example of calculating a summary tabulation table, we will load some selected article characterization data from the relevant articles in Young et al. (2019).\n\n\nCode\n# Load necessary packages\npacman::p_load(\n  rio,  # package required for importing Excel files\n  here, # package to allow us to import using the location of our R project\n  DT    # package for displaying the dataset as a table\n  )\n\n# Load dataset\nYoung_2019 &lt;- import(here(\"assets\", \"Young_2019_charting.xlsx\"))\n\n# Display dataset as an interactive table\nYoung_2019 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) |&gt; \n  formatStyle(columns = colnames(Young_2019), fontSize = '70%')\n\n\n\n\n\n\n\nNow we can produce some summary statistics and tabulations of the variables and organize them into a table to present in our results.\n\n\nCode\npacman::p_load(\n  tidyverse,  # package for data management and visualization\n  gtsummary   # package to calculate summary tables\n  )\n\n# Change display of large numbers (no commas)\ntheme_gtsummary_language(\"en\", big.mark = \"\")\n\n\n\n\nCode\n# First convert Year variable to numeric\nYoung_2019 &lt;- Young_2019 |&gt; \n  mutate(Year = as.numeric(Year))\n\n# Create summary table\nYoung_2019_summary &lt;- Young_2019 |&gt; \n  select(-Refid, -\"Author (year)\") |&gt;  # Remove Refid and author (year) columns from summary\n  tbl_summary(digits = list(all_categorical() ~ c(0, 1)),\n              type = all_categorical() ~ \"categorical\", \n              sort = list(everything() ~ \"frequency\"))\nYoung_2019_summary\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 471\n\n\n\n\nYear\n2010 (2000, 2014)\n\n\nDocument type\n\n\n\n    Journal article\n35 (74.5%)\n\n\n    Thesis\n12 (25.5%)\n\n\nLanguage\n\n\n\n    English\n45 (95.7%)\n\n\n    Italian\n1 (2.1%)\n\n\n    Korean\n1 (2.1%)\n\n\nCountry\n\n\n\n    US\n22 (46.8%)\n\n\n    Canada\n7 (14.9%)\n\n\n    India\n3 (6.4%)\n\n\n    South Korea\n3 (6.4%)\n\n\n    Brazil\n2 (4.3%)\n\n\n    UK\n2 (4.3%)\n\n\n    Bahrain\n1 (2.1%)\n\n\n    Iran\n1 (2.1%)\n\n\n    Italy\n1 (2.1%)\n\n\n    Kenya\n1 (2.1%)\n\n\n    Malaysia\n1 (2.1%)\n\n\n    Myanmar\n1 (2.1%)\n\n\n    Nigeria\n1 (2.1%)\n\n\n    Saudi Arabia\n1 (2.1%)\n\n\nStudy design\n\n\n\n    Controlled before-and-after study\n19 (40.4%)\n\n\n    Randomized controlled trial (RCT)\n18 (38.3%)\n\n\n    Non-randomized controlled trial\n10 (21.3%)\n\n\nSetting includes restaurants\n\n\n\n    No\n31 (66.0%)\n\n\n    Yes\n16 (34.0%)\n\n\nIntervention type\n\n\n\n    Training course or session\n31 (66.0%)\n\n\n    Multi-faceted\n11 (23.4%)\n\n\n    Messaging materials\n4 (8.5%)\n\n\n    Consulting services\n1 (2.1%)\n\n\nIntervention content described\n\n\n\n    Yes\n36 (76.6%)\n\n\n    No\n11 (23.4%)\n\n\nIntervention informed by theory\n\n\n\n    No / not reported\n40 (85.1%)\n\n\n    Yes\n7 (14.9%)\n\n\nIntervention informed by research\n\n\n\n    Yes\n35 (74.5%)\n\n\n    No / not specified\n12 (25.5%)\n\n\nStakeholder engagement used\n\n\n\n    No / not reported\n30 (63.8%)\n\n\n    Yes\n17 (36.2%)\n\n\nIntervention duration\n\n\n\n    Not reported\n15 (31.9%)\n\n\n    Less than 1 day\n14 (29.8%)\n\n\n    1 day or longer\n12 (25.5%)\n\n\n    N/a\n6 (12.8%)\n\n\nComparison group\n\n\n\n    No intervention\n39 (83.0%)\n\n\n    Standard/traditional intervention\n8 (17.0%)\n\n\nOutcome - knowledge\n\n\n\n    Yes\n32 (68.1%)\n\n\n    No\n15 (31.9%)\n\n\nOutcome - attitudes\n\n\n\n    No\n35 (74.5%)\n\n\n    Yes\n12 (25.5%)\n\n\nOutcome - behaviour\n\n\n\n    No\n28 (59.6%)\n\n\n    Yes\n19 (40.4%)\n\n\nOutcome - inspection scores\n\n\n\n    No\n30 (63.8%)\n\n\n    Yes\n17 (36.2%)\n\n\nCollection tool pre-tested\n\n\n\n    Yes\n29 (61.7%)\n\n\n    No / not specified\n18 (38.3%)\n\n\n\n1 Median (IQR); n (%)\n\n\n\n\n\n\n\n\nWe can further adjust the settings and presentation of the table as needed, see the Epi R Handbook section on the gtsummary package for more information. We can also copy and paste our table from the HTML viewer in RStudio into a Word document for easy editing and inclusion in our report.\n\n\n\n\n\n\n\n\nFlow Chart and Article Summary Exercise\n\n\n\nUsing the systematic reviews you identified in previous weeks related to your thesis topic, review the article to examine how they displayed their flow chart and article summary data. Consider the following questions:\n\nHow does the flow chart compare to the PRIMSA template shown above?\nWhat format is used for the article summary information?\nAre any important article characteristics missing?"
  },
  {
    "objectID": "7_analysis.html#methods-of-analyzing-measures-of-effect",
    "href": "7_analysis.html#methods-of-analyzing-measures-of-effect",
    "title": "7  Data Analysis",
    "section": "7.3 Methods of Analyzing Measures of Effect",
    "text": "7.3 Methods of Analyzing Measures of Effect\nThe recommended approach to analyze our outcome data (i.e., measures of effect), if possible and reasonable to do so, is meta-analysis. Meta-analysis is defined as statistical method for combining the results of two or more studies.\nThere are some situations where it may not be possible to use meta-analysis, for example:\n\nThere is only one or no studies containing the outcome of interest\nThe outcome data are not sufficiently reported and cannot be estimated from available statistics\nThe study designs, methods, or outcome measures are too diverse and heterogeneous that it doesn’t make biological or reasonable sense to combine them\nThere are major concerns about bias in the studies\n\nIn these cases, one can consider conducting a descriptive and narrative summary of the evidence. This includes presenting summary tables of the results of each study and some discussion of the results of each study. Tables can be grouped by relevant criteria from the question (e.g., intervention groups, outcome domains), or by other factors such as strength of evidence or risk of bias.\nAnother option when summary measures of effect are available from studies, but not measures of variability needed to conduct meta-analysis, is to summarize the effect estimates directly using box-and-whisker (and/or violin) plots. This type of summary provides information about the size and range of effects across studies, but does not account for the differences in sample size across studies. Combining P values is another method that can be used when only P values are available, and aims to determine is there is evidence of an effect in at least one study. For more information on these and other alternative analysis methods, see the relevant chapter in the Cochrane Collaboration Handbook.\n\n\n\n\n\n\nExample of when Meta-Analysis was Not Feasible\n\n\n\nIn a systematic review about the prevalence of knowledge, practices, and training outcomes among restaurant and food service personnel toward food allergies and Celiac disease (Young and Thaivalappil 2018), the authors did not conduct meta-analysis on the outcome due to substantive differences in the populations assessed, their characteristics, and how outcomes were measured. Instead, they summarized the distribution of prevalence outcomes across studies within each outcome domain using box-and-whisker plots. The figure below shows the prevalence of food allergy and Celiac disease behaviour outcomes reported across studies:"
  },
  {
    "objectID": "7_analysis.html#introduction-to-meta-analysis",
    "href": "7_analysis.html#introduction-to-meta-analysis",
    "title": "7  Data Analysis",
    "section": "7.4 Introduction to Meta-Analysis",
    "text": "7.4 Introduction to Meta-Analysis\nMeta-analysis is a statistical technique that involves combining the measure of effect for each study along with its weight to calculate a weighted average of the measure of effect. Each study receives a weight based on its standard error (SE), so studies with greater precision (i.e., less error) receive more weight. There are two main approaches to meta-analysis, which determine how weights are calculated: fixed-effect and random-effects.\nA fixed-effect meta-analysis assumes there is one true effect size among all of the included studies, and that any observed differences in the effects are due only to sampling error. In contrast, random-effects meta-analysis models assume that the study effects represent a random sample of possible effect sizes (that usually follows a normal distribution). The null hypothesis under a fixed-effect model is that there is zero effect in every study, while for a random-effects model the null hypothesis is that average effect is zero. Both models will give identical results when there is no statistical heterogeneity among the studies.\nAssuming an overall true effect of \\(\\theta\\), under a fixed-effect model, the observed effect of study j, \\(\\theta_j\\), differs from the overall true effect only because of its sampling error (\\(\\epsilon_j\\)):\n\\[\n\\hat{\\theta_j} = \\theta + \\epsilon_j\n\\] Studies are then weighted by the inverse of the variance (which is equal to the SE squared). For study j, the weight would be calculated as follows:\n\\[\n{W_j} = \\frac{1}{SE_j^2}\n\\]\nThe weighted average, or pooled effect, can then be calculated by multiplying each study’s effect size by its weight, adding those together, and then dividing by the sum of all study weights. This is referred to as the generic inverse-variance weighting approach:\n\\[\n\\hat{\\theta} = \\frac{\\sum_{j=1}^J \\hat{\\theta_j}{W_j}} {\\sum_{j=1}^J {W_j}}\n\\]\nIn contrast, random-effects models incorporate an additional source of error, denoted \\(\\zeta_j\\), to indicate the true effect size of study j is part of a distribution of true effect sizes with a mean value of \\(\\mu\\):\n\\[\n\\hat{\\theta_j} = \\mu + \\zeta_j + \\epsilon_j\n\\]\nIn a random-effects model, study weights incorporate a measure of the variance of the distribution of true effect sizes, denoted \\(\\tau^2\\):\n\\[\n{W_j} = \\frac{1}{{SE_j^2}+{\\tau^2}}\n\\]\nThe adjusted weight is then used to calculate the weighted average measure of effect. There are several methods to estimate \\(\\tau^2\\). The most famous is called the DerSimonian-Laird (DL) method. This method has straightforward calculations, and is often the default method in various software packages. However, this method can be biased and results in confidence intervals that are too narrow when the number of studies is small and heterogeneity is high.\nBased on simulation studies, the restricted maximum likelihood estimator (REML) approach seems to be good choice for continuous outcome data (Langan et al. 2019; Veroniki et al. 2016). For dichotomous (binary) outcome data, Paule-Mandel estimator, or the Empirical Bayes method, seems to be good choice. You can always conduct a sensitivity analysis to compare with other approaches or the standard DL method.\nAdditionally, it is generally recommended to apply Knapp-Hartung adjustments if the option is available, as this method widens the confidence interval of the weighted measure of effect to account for uncertainty in the estimation of \\(\\tau^2\\)."
  },
  {
    "objectID": "7_analysis.html#fixed-vs.-random-effects-models",
    "href": "7_analysis.html#fixed-vs.-random-effects-models",
    "title": "7  Data Analysis",
    "section": "7.5 Fixed vs. Random-Effects Models",
    "text": "7.5 Fixed vs. Random-Effects Models\nFixed-effect models are usually not appropriate for reviews in occupational and public health, as they assume that all studies are essentially identical, which is usually only the case under highly controlled and replicable conditions. The results from such an analysis are only applicable to the participants in each study, and cannot be generalized for other populations.\nIn contrast, random-effects models are more appropriate when there are differences across studies beyond what we would expect from sampling error alone (e.g., differences in interventions, populations, how outcomes are measured). However, this approach gives more weight to smaller studies, which can exacerbate potential biases that may be present in smaller studies. Additionally, if the number of included studies is very small, the estimate of between-study variance \\(\\tau^2\\) will have poor precision and is less reliable. Unfortunately, there is no clear guidance which approach should be used in this situation; Bayesian methods are one approach but require more advanced expertise.\n\n\n\n\n\n\nMeta-Analysis Exercise\n\n\n\nUsing the systematic review article you identified earlier in the course, review the article to examine whether meta-analysis was conducted. If not, search for another paper on the topic that conducted meta-analysis or examine one of the other example articles given in Chapter 11. Consider the following questions for the article analyzed:\n\nDid they conduct a fixed-effect or random-effects model, and what rationale (if any) was given for the choice?\nIf a random-effects model was used, which method was used to estimate \\(\\tau^2\\)?\n\n\n\n\n\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nLangan, Dean, Julian P. T. Higgins, Dan Jackson, Jack Bowden, Areti Angeliki Veroniki, Evangelos Kontopantelis, Wolfgang Viechtbauer, and Mark Simmonds. 2019. “A Comparison of Heterogeneity Variance Estimators in Simulated Random-Effects Meta-Analyses.” Research Synthesis Methods 10 (1): 83–98. https://doi.org/10.1002/jrsm.1316.\n\n\nVeroniki, Areti Angeliki, Dan Jackson, Wolfgang Viechtbauer, Ralf Bender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian P. T. Higgins, Dean Langan, and Georgia Salanti. 2016. “Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis.” Research Synthesis Methods 7 (1): 55–79. https://doi.org/10.1002/jrsm.1164.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review and Meta-Regression of the Knowledge, Practices, and Training of Restaurant and Food Service Personnel Toward Food Allergies and Celiac Disease.” Edited by Louise Emilsson. PLOS ONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496."
  },
  {
    "objectID": "8_meta.html#pooling-measures-of-effect",
    "href": "8_meta.html#pooling-measures-of-effect",
    "title": "8  Meta-Analysis",
    "section": "8.1 Pooling Measures of Effect",
    "text": "8.1 Pooling Measures of Effect\nIn the previous chapter, we reviewed the basics of meta-analysis, including differences between fixed-effect and random-effects models. We will now cover procedures for conducting meta-analysis for different types of outcome data. All examples will use the meta package in R.\nThe most common method of pooling measures of effect is called the generic inverse-variance method, the formula for which was provided in Chapter 7. In this method, studies are weighted by the inverse of their precision. This method is used for continuous data (e.g., mean difference, SMD), and can also be used for dichotomous data. It is also the method used when conducting meta-analysis on pre-calculated measures of effect (e.g., odds ratios, risk ratios).\n\n\n\n\n\n\nChange-From-Baseline Data\n\n\n\nAnalyses of an intervention based on a change from baseline can be more efficient, increasing precision of estimates. They are ideally analyzed by including the baseline measurement as a covariate in a regression analysis or ANCOVA. In meta-analysis, change-from-baseline scores and post-intervention values can be combined in the same analysis if using a raw MD outcome. However, they should not be combined when using a SMD outcome, as the standard deviations are not comparable.\n\n\nWhen analyzing dichotomous raw data (e.g., number of events and sample size in each comparison group), there are alternative approaches available. The most common of these is the Mantel-Haenszel method. This method is preferred for dichotomous data, especially when the event is rare or when the sample size is small. The Peto odds ratio method is another approach, but it has more limitations and can only be used to calculate odds ratios.\nOne issue for dichotomous outcomes is that there may be zero events in one or both groups. In this case, the traditional, and often default approach in meta-analysis software, is to replace zero values with 0.5 (continuity correction). However, this correction is only required when using the generic inverse-variance approach; it should not be used when using the other methods.\nIn the meta package, the following functions can be used for meta-analysis of different types of data inputs:\n\nmetagen for pre-calculated measures of effect\nmetacont for continuous data\nmetabin for dichotomous data\nmetaprop for prevalence (proportion) data\nmetainc for incidence rate ratio or incidence rate difference data\nmetarate for incidence data in a single group\nmetacor for correlation coefficients\n\nNote that we will not cover meta-analysis of all of these data types. If interested to see examples of others, you can visit the online book Doing Meta-Analysis in R.\n\n\n\n\n\n\nContinuous Data Meta-Analysis Example\n\n\n\nTo illustrate how to conduct a meta-analysis using continuous data, we will examine part of the dataset from a systematic review of music interventions to improve various health outcomes in people with cancer (Bradt et al. 2021). Specifically, we will examine 17 studies from that review that evaluated music interventions plus standard care compared to standard care alone in adults to improve anxiety. Anxiety was measured in all studies using the Spielberger State Anxiety Index (STAI) scale, with lower scores representing lower anxiety.\n\n\nCode\npacman::p_load(\n  rio,        # load in an Excel file\n  here,       # loading data using relative path\n  tidyverse,  # data management\n  meta,       # meta-analysis\n  DT          # to visualize the dataset online\n)\n\nBradt_2021 &lt;- import(here(\"assets\", \"Bradt_2021.xlsx\"))\n\nBradt_2021 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4))))\n\n\n\n\n\n\n\nWe can see that each study reported the mean anxiety score, SD, and sample size in each comparison group. Because all studies measured the outcome on the same scale, we can calculate a raw mean difference (MD). For continuous data, we will use the metacont function.\n\n\nCode\nBradt_meta &lt;- Bradt_2021 |&gt; \n  metacont(n.e = int_n,         # number in intervention group\n           mean.e = int_mean,   # intervention group mean\n           sd.e = int_sd,       # intervention group SD\n           n.c = con_n,         # number in control group\n           mean.c = con_mean,   # control group mean\n           sd.c = con_sd,       # control group SD\n           studlab = study,     # study ID column\n           sm = \"MD\",           # summary measure (MD or SMD)\n           random = TRUE,       # conduct random-effects analysis\n           fixed = FALSE,       # do not conduct a fixed-effect analysis\n           method.tau = \"REML\", # method of calculating tau\n           hakn = TRUE,         # apply the Hartung-Knapp adjustment\n           title = \"Music Intervention and Anxiety\"\n)\n\nsummary(Bradt_meta)\n\n\nReview:     Music Intervention and Anxiety\n\n                        MD               95%-CI %W(random)\nBinns-Turner 2008 -19.0000 [-30.4116;  -7.5884]        3.5\nBro 2019           -1.9000 [ -6.6561;   2.8561]        6.0\nBulfone 2009       -8.3000 [-13.1441;  -3.4559]        6.0\nChen 2013          -6.1500 [ -6.3510;  -5.9490]        7.1\nDanhauer 2010      -1.1000 [ -5.9697;   3.7697]        6.0\nFirmeza 2017       -7.5000 [ -9.7117;  -5.2883]        6.8\nHarper 2001       -20.1000 [-30.3764;  -9.8236]        3.9\nJin 2011           -8.1400 [-11.0220;  -5.2580]        6.6\nLi 2012            -9.4800 [-10.8968;  -8.0632]        7.0\nLin 2011           -5.3900 [-10.1061;  -0.6739]        6.0\nO'Callaghan 2012    2.0000 [ -2.2163;   6.2163]        6.2\nRossetti 2017      -9.4000 [-14.6008;  -4.1992]        5.8\nSmith 2001         -1.6000 [ -8.8116;   5.6116]        5.0\nVachiramon 2013    -6.5000 [ -9.8650;  -3.1350]        6.5\nWan 2009          -22.1000 [-24.4885; -19.7115]        6.8\nWren 2019          -2.1300 [-12.5696;   8.3096]        3.8\nZhou 2015          -9.3400 [-10.9550;  -7.7250]        6.9\n\nNumber of studies combined: k = 17\nNumber of observations: o = 1381\n\n                          MD              95%-CI     t p-value\nRandom effects model -7.7482 [-11.0659; -4.4306] -4.95  0.0001\n\nQuantifying heterogeneity:\n tau^2 = 33.0199 [16.0833; 94.6051]; tau = 5.7463 [4.0104; 9.7265]\n I^2 = 93.4% [90.9%; 95.3%]; H = 3.91 [3.32; 4.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 244.16   16 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 16)\n\n\nBased on this analysis, we can see that the anxiety scores were lower among participants that received music interventions in addition to standard care vs. standard care alone.\nIf we wanted to change any settings, we can use the update.meta function to update our analysis. For example, we could run an updated analysis to compare how the results might change with a different method of calculating \\(\\tau^2\\).\n\n\nCode\nBradt_meta_update &lt;- update.meta(Bradt_meta,\n                           method.tau = \"PM\") # Update to Paule-Mandel method\n\n# Compare pooled estimates and tau^2 values\ntibble(Method = c(\"REML\", \"PM\"),\n       Pooled_MD = c(Bradt_meta$TE.random, Bradt_meta_update$TE.random),\n       Pooled_SE = c(Bradt_meta$seTE.random, Bradt_meta_update$seTE.random),\n       Tau2 = c(Bradt_meta$tau2, Bradt_meta_update$tau2))\n\n\n# A tibble: 2 × 4\n  Method Pooled_MD Pooled_SE  Tau2\n  &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 REML       -7.75      1.57  33.0\n2 PM         -7.75      1.57  35.0\n\n\nWe obtain slightly different results when using the alternative method of calculating \\(\\tau^2\\).\n\n\n\n\n\n\n\n\nDichotomous Data Meta-Analysis Example\n\n\n\nTo illustrate how to conduct a meta-analysis using continuous data, we will examine a dataset of 136 studies used in a meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018), as prepared by White et al. in the metadat repository.\n\n\nCode\nHartmann_2018 &lt;- import(here(\"assets\", \"Hartmann_2018.xlsx\"))\n\nHartmann_2018 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) \n\n\n\n\n\n\n\nAs can been seen above, the dataset contains information from each study on the number of participants in each group that continued to abstain from smoking at 6+ months of follow-up. Also included is a column that specifies the type of treatment received. Since the data are dichotomous and all include raw data, we can calculate an measure of effect using the Mantel-Haenszel method. We will use the metabin function, but first we will subset our data to only conduct the meta-analysis on studies that investigated the patch as an intervention.\n\n\nCode\nHartmann_meta &lt;- Hartmann_2018 |&gt; \n  filter(treatment == \"patch\") |&gt;   # subset data for meta-analysis \n  metabin(event.e = x.nrt,          # events in intervention group\n          n.e = n.nrt,              # number in intervention group\n          event.c = x.ctrl,         # events in control group\n          n.c = n.ctrl,             # number in control group\n          studlab = study,          # study ID column\n          sm = \"RR\",                # summary measure (OR or RR)\n          method = \"MH\",            # use the Mantel-Haenszel method\n          random = TRUE,            # conduct random-effects analysis\n          fixed = FALSE,            # do not conduct a fixed-effect analysis\n          method.tau = \"PM\",        # method of calculating tau\n          hakn = TRUE,              # apply the Hartung-Knapp adjustment\n          MH.exact = TRUE,          # do not apply a continuity correction\n          title = \"Nicotine Patch and Smoking Cessation\"\n)\n\nsummary(Hartmann_meta)\n\n\nReview:     Nicotine Patch and Smoking Cessation\n\n                     RR            95%-CI %W(random)\nAbelin 1989      1.5300 [0.7555;  3.0986]        1.5\nAhluwalia 1998   1.4583 [0.9006;  2.3615]        2.6\nAnthenelli 2016  1.6729 [1.4137;  1.9797]        5.9\nBuchkremer 1988  1.4568 [0.7423;  2.8593]        1.7\nCampbell 1996    1.4609 [0.8295;  2.5729]        2.1\nCinciripini 1996 1.7143 [0.7757;  3.7885]        1.3\nColeman 2012     1.2438 [0.8340;  1.8551]        3.3\nCummins 2016     1.1506 [0.7561;  1.7510]        3.1\nCunningham 2016  2.7944 [1.0142;  7.6997]        0.8\nDaughton 1991    3.4340 [1.2713;  9.2759]        0.9\nDaughton 1998    1.5710 [0.8679;  2.8437]        2.0\nDavidson 1998    2.0625 [1.1539;  3.6867]        2.1\nEhrsam 1991      3.5000 [0.7600; 16.1183]        0.4\nFiore 1994a      1.6667 [0.8170;  3.4000]        1.5\nFiore 1994b      2.4123 [0.8040;  7.2379]        0.7\nGallagher 2007   0.2500 [0.0288;  2.1719]        0.2\nGlavas 2003a     1.4444 [0.6722;  3.1039]        1.4\nGlavas 2003b     2.4167 [1.3301;  4.3908]        2.0\nHays 1999        2.2421 [1.2753;  3.9420]        2.1\nHeydari 2012     3.7917 [1.6199;  8.8753]        1.1\nHughes 1999      1.6786 [1.1945;  2.3589]        3.9\nHughes 2003      1.4385 [0.6457;  3.2046]        1.3\nHurt 1990        1.3333 [0.5239;  3.3933]        1.0\nHurt 1994        1.9412 [1.1453;  3.2900]        2.4\nICRF 1994        1.4374 [1.0256;  2.0144]        3.9\nJorenby 1999     1.7486 [0.8346;  3.6639]        1.4\nJoseph 1996      0.8173 [0.5135;  1.3007]        2.8\nKillen 1997      1.0952 [0.6256;  1.9176]        2.2\nKornitzer 1995   0.9500 [0.4653;  1.9396]        1.5\nLerman 2015      1.3470 [0.9612;  1.8877]        3.9\nLewis 1998       1.7005 [0.5970;  4.8439]        0.8\nMoolchan 2005    5.2941 [1.2266; 22.8504]        0.4\nNCT00534404      1.7755 [1.4335;  2.1991]        5.3\nOncken 2007      1.1310 [0.6990;  1.8299]        2.7\nOtero 2006       1.5952 [1.3108;  1.9414]        5.6\nPaoletti 1996    3.7500 [1.3211; 10.6443]        0.8\nPerng 1998       3.2000 [0.9562; 10.7095]        0.6\nPiper 2009       1.5887 [0.8412;  3.0007]        1.8\nPrapavessis 2007 1.9516 [0.8367;  4.5518]        1.1\nRichmond 1994    2.0579 [1.1327;  3.7387]        2.0\nSachs 1993       2.6513 [1.3541;  5.1912]        1.7\nScherphof 2014   0.7114 [0.2538;  1.9941]        0.8\nStapleton 1995   2.0263 [1.2445;  3.2994]        2.6\nSonderskov 1997  1.5196 [0.7847;  2.9428]        1.7\nTNSG 1991        1.8070 [1.2477;  2.6171]        3.6\nTuisku 2016      1.3372 [0.7037;  2.5409]        1.8\nTonnesen 1991    3.9724 [1.6736;  9.4286]        1.1\nTonnesen 2000    4.7163 [1.0435; 21.3163]        0.4\nWard 2013        1.0704 [0.5647;  2.0290]        1.8\nWestman 1993     8.2051 [1.9508; 34.5106]        0.4\nWisborg 2000     1.0726 [0.5915;  1.9448]        2.0\n\nNumber of studies combined: k = 51\nNumber of observations: o = 25754\nNumber of events: e = 3291\n\n                         RR           95%-CI    t  p-value\nRandom effects model 1.6243 [1.4675; 1.7978] 9.60 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0359 [0.0000; 0.1494]; tau = 0.1895 [0.0000; 0.3866]\n I^2 = 23.3% [0.0%; 46.2%]; H = 1.14 [1.00; 1.36]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 65.20   50  0.0730\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 50)\n\n\nIn this analysis, we can see that participants in the intervention group were more likely to continue to abstain from smoking at follow-up compared to those in the control group.\n\n\n\n8.1.1 Prevalence and Incidence Data\nMeta-analysis of prevalence and incidence data uses a slightly different approach than continuous and dichotomous data. The recommended approach for synthesizing such data is to use a generalized linear mixed-effects model (GLMM) (Schwarzer et al. 2019). Prevalence data should be logit transformed prior to meta-analysis, while incidence data should be log transformed (this is done automatically in the meta package). For prevalence data, the GLMM approach fits an intercept-only logistic regression to the data, with a random-effect to account for the between-study variation. A Poisson GLMM model is used for incidence data.\nUsing the GLMM approach has some limitations. It is not possible to obtain individual study weights using this method. Additionally, there is only one method to calculate \\(\\tau^2\\), the maximum-likelihood (ML) estimator, and there will be no confidence internals for \\(\\tau^2\\). In case those details are needed, the inverse-variance approach can be used with the arcsine or logit transformation for prevalence data, or the log transformation for incidence data.\n\n\n\n\n\n\nPrevalence Meta-Analysis Example\n\n\n\nAn example is shown below of a meta-analysis of the prevalence of selected outcomes from a systematic review of the knowledge, behaviours, and training of restaurant and food service personnel toward food allergies and Celiac disease (Young and Thaivalappil 2018).\n\n\nCode\nYoung_2018 &lt;- import(here(\"assets\", \"Young_2018.xlsx\"))\n\nYoung_2018 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) |&gt; \n  formatStyle(columns = colnames(Young_2018), fontSize = '70%')\n\n\n\n\n\n\n\nWe can see that some article and study characteristics are also included in this dataset. For these data, we will subset the training outcome only for illustration purposes. This outcome shows the proportion of participants (i.e., restaurant and food service staff) in each study that reported receiving training about food allergies.\n\n\nCode\nYoung_2018_meta &lt;- \n  metaprop(event = n.positive, \n           n = n.total,\n           subset = outcome.category == \"Training status/policies\",\n           data = Young_2018,\n           studylab = \"author.year\", \n           method = \"GLMM\",\n           sm = \"PLOGIT\",\n           random = TRUE,\n           fixed = FALSE,\n           hakn = TRUE,\n           title = \"Food Allergy Training Prevalence\"\n)\n\nsummary(Young_2018_meta)\n\n\nReview:     Food Allergy Training Prevalence\n\n   proportion           95%-CI\n79     0.4200 [0.3220; 0.5229]\n80     0.3333 [0.2374; 0.4405]\n81     0.3057 [0.2416; 0.3759]\n82     0.1500 [0.0571; 0.2984]\n83     0.4933 [0.3758; 0.6114]\n84     0.7909 [0.7030; 0.8626]\n85     0.3668 [0.3043; 0.4328]\n86     0.4085 [0.3701; 0.4477]\n87     0.1709 [0.1331; 0.2145]\n88     0.4557 [0.3998; 0.5124]\n89     0.2500 [0.1766; 0.3357]\n90     0.2609 [0.1834; 0.3510]\n\nNumber of studies combined: k = 12\nNumber of observations: o = 2382\nNumber of events: e = 871\n\n                     proportion           95%-CI\nRandom effects model     0.3587 [0.2578; 0.4739]\n\nQuantifying heterogeneity:\n tau^2 = 0.5203; tau = 0.7213; I^2 = 93.2% [89.9%; 95.4%]; H = 3.84 [3.15; 4.67]\n\nTest of heterogeneity:\n      Q d.f.  p-value             Test\n 161.87   11 &lt; 0.0001        Wald-type\n 195.78   11 &lt; 0.0001 Likelihood-Ratio\n\nDetails on meta-analytical method:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Random effects confidence interval based on t-distribution (df = 11)\n- Logit transformation\n- Clopper-Pearson confidence interval for individual studies\n\n\nWe can see the pooled prevalence value is ~36%, with a 95% CI of 26-47%."
  },
  {
    "objectID": "8_meta.html#assessing-heterogeneity",
    "href": "8_meta.html#assessing-heterogeneity",
    "title": "8  Meta-Analysis",
    "section": "8.2 Assessing Heterogeneity",
    "text": "8.2 Assessing Heterogeneity\nHeterogeneity refers to differences between studies that is beyond what we would expect from chance (or random error) alone. It can be due to differences in how interventions (exposures) and outcomes were defined, implemented, and measured, differences in the characteristics of the populations assessed, or other factors (e.g., differences in study methods, context, and bias).\nThere is a \\(\\chi2\\) statistical test for heterogeneity, called Cochran’s Q, that has been traditionally used and is included in the the R meta results output. This test evaluates whether this is more variation than would be expected by sampling error alone. However, this test has lower power when the number of studies is small. Additionally, its use is controversial, as some argue that since there are always differences expected between studies, some heterogeneity will always be present.\nFor this reason, the \\(I^2\\) statistic was developed to quantify heterogeneity (J. P. T. Higgins et al. 2003). The formula, based on Cochran’s \\(Q\\), is shown below, with \\(N\\) referring to the number of studies in the analysis:\n\\[\nI^2 = \\frac{Q-(N-1)}{Q}\n\\]\n\\(I^2\\) refers to the percentage of variation in measures of effect across studies that is due to heterogeneity rather than sampling error. While thresholds are often used in practice, these are discouraged. Instead, the amount of heterogeneity that is important depends on the content (e.g., magnitude and direction of effects, strength of evidence for heterogeneity). In general, \\(I^2\\) values of 0-40% might not be important, values of 75-100% usually indicate considerable heterogeneity, while values in 30-60% and 50-90% might indicate moderate to substantial heterogeneity (J. Higgins et al. 2022).\nHowever, because \\(I^2\\) is a relative measure, it should not be the only measure of heterogeneity reported. We can also examine \\(\\tau\\) and its 95% CI, which represents the estimated SD of the true effects across studies (it is on the same scale as the measure of effect used in the analysis).\nPrediction intervals (PIs) are recommended to be included alongside other estimates of heterogeneity. A 95% PI estimates the range of values the measure of effect would be expected to fall within in 95% of similar studies that might be conducted in the future. In cases of heterogeneity, the PI covers a wider range of values than a CI. In R meta, we can add a PI to our output by adding the argument prediction = TRUE to the function input options.\nWe will explore in the next session Chapter 9 how to investigate different causes of heterogeneity using subgroup analysis and meta-regression.\n\n\n\n\n\n\nHeterogeneity Example\n\n\n\nWe will go back to our first meta-analysis example that examined the effect of music interventions plus standard care vs. standard care alone to reduce anxiety levels among people with cancer (Bradt et al. 2021). We will update the analysis to include a prediction interval (PI), then interpret the heterogeneity.\n\n\nCode\nBradt_meta &lt;- update.meta(Bradt_meta, prediction = TRUE)\nBradt_meta\n\n\nReview:     Music Intervention and Anxiety\n\nNumber of studies combined: k = 17\nNumber of observations: o = 1381\n\n                          MD              95%-CI     t p-value\nRandom effects model -7.7482 [-11.0659; -4.4306] -4.95  0.0001\nPrediction interval          [-20.4226;  4.9262]              \n\nQuantifying heterogeneity:\n tau^2 = 33.0199 [16.0833; 94.6051]; tau = 5.7463 [4.0104; 9.7265]\n I^2 = 93.4% [90.9%; 95.3%]; H = 3.91 [3.32; 4.59]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 244.16   16 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 16)\n- Prediction interval based on t-distribution (df = 15)\n\n\nWe can see in the results that the \\(Q\\) test for heterogeneity is significant, and the \\(I^2\\) value is also very high at ~93% (95% CI: 90.9-95.3%). While the pooled measure of effect (MD) has a 95% CI that excludes the null, suggesting a consistent positive effect of the music intervention, the PI crosses above zero. This indicates that we cannot rule out that the intervention might have no effect or a negative effect in future studies."
  },
  {
    "objectID": "8_meta.html#forest-plots",
    "href": "8_meta.html#forest-plots",
    "title": "8  Meta-Analysis",
    "section": "8.3 Forest Plots",
    "text": "8.3 Forest Plots\nForest plots are the most common way to visualize meta-analysis results. They show the measure of effect estimate, confidence interval, and weight of each study, and the pooled or average estimate at the bottom. They can also include a prediction interval.\nBelow we will create a forest plot from the meta-analysis of music interventions on reducing anxiety levels in people with cancer, from the earlier example (Bradt et al. 2021). Using the meta package, we can generate a forest plot for our saved meta-analysis results using the forest.meta function.\n\n\nCode\nBradt_meta |&gt; forest.meta(digits.mean = 1,  # Limiting significant digits on left columns\n                    digits.sd = 1,\n                    fontsize = 7,           # Decrease fontsize so all text fits on image\n                    spacing = 0.9           # Decrease spacing to ensure text fits\n                    )\n\n\n\n\n\nWe can clean up the display of this plot by removing the data on the left side of the plot, and also adding in a prediction interval at the bottom. Other customization options can be made as needed.\n\n\nCode\nBradt_meta |&gt; forest.meta(sortvar = TE,        # sort studies by effect size\n                    prediction = TRUE,         # add prediction interval\n                    print.tau2 = FALSE,        # do not show tau2 at the bottom\n                    leftcols = \"study\",        # show only the study ID on the left\n                    leftlabs = \"Study\",        # relabel study column\n                    col.diamond = \"steelblue\", # colour customization\n                    col.predict = \"darkblue\",\n                    spacing = 0.85)\n\n\n\n\n\n\n\n\n\n\n\nMeta-Analysis Exercise\n\n\n\nWe will load data from the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions. The dataset can be loaded and visualized as per below:\n\n\nCode\nYoung_2019 &lt;- import(here(\"assets\", \"Young_2019_ma.xlsx\"))\n\nYoung_2019 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) |&gt; \n  formatStyle(columns = colnames(Young_2019), fontSize = '70%')\n\n\n\n\n\n\n\nWe can see that there are 214 unique outcomes, with multiple outcomes reported in many studies. We will first conduct a meta-analysis of a subset of data that only examines RCTs and behaviour outcomes:\n\n\nCode\nYoung_2019_meta &lt;- Young_2019 |&gt; \n  filter(study_design == \"RCT\" & outcome == \"Behaviour\") |&gt;\n  metagen(TE = g,\n          seTE = g_se,\n          studlab = author_year,\n          sm = \"SMD\", \n          random = TRUE,\n          fixed = FALSE,\n          method.tau = \"REML\",\n          hakn = TRUE,\n          title = \"Food Handler Training and Behaviour - RCTs\"\n)\n\nsummary(Young_2019_meta)\n\n\nReview:     Food Handler Training and Behaviour - RCTs\n\n                            SMD            95%-CI %W(random)\nCraggs-Dino (2002)       0.0787 [-0.8962; 1.0537]        8.1\nMaung et al. (2017)      0.1008 [-0.8240; 1.0257]        8.9\nNik Husain et al. (2016) 0.3560 [-0.1716; 0.8836]       27.5\nNik Husain et al. (2016) 0.0030 [-0.5195; 0.5255]       28.0\nNik Husain et al. (2016) 0.3670 [-0.1606; 0.8946]       27.5\n\nNumber of studies combined: k = 5\n\n                             SMD            95%-CI    t p-value\nRandom effects model (HK) 0.2149 [-0.0135; 0.4434] 2.61  0.0593\n\nQuantifying heterogeneity:\n tau^2 = 0 [0.0000; 0.1448]; tau = 0 [0.0000; 0.3805]\n I^2 = 0.0% [0.0%; 79.2%]; H = 1.00 [1.00; 2.19]\n\nTest of heterogeneity:\n    Q d.f. p-value\n 1.36    4  0.8513\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 4)\n\n\nExamine and interpret the results.\n\nHow many studies and unique outcomes were included?\nWhat does the overall evidence say about the intervention?\nHow much heterogeneity is present? Is it significantly different than zero?\nHow does the result change if you use a different method of calculating \\(\\tau^2\\)?\n\nCreate a customized forest plot to visualize the results and include the prediction interval. How would you interpret the interval?\nNow conduct a meta-analysis for the subset of non-randomized studies and the inspection scores outcome. Answer the same questions above for this analysis.\n\n\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nHiggins, J P T, S G Thompson, J J Deeks, and D G Altman. 2003. “Measuring Inconsistency in Meta-Analyses.” BMJ (Clinical Research Ed.) 327 (7414): 557–60. https://doi.org/10.1136/bmj.327.7414.557.\n\n\nHiggins, JPT, J Thomas, J Chandler, M Cumpston, T Li, MJ Page, and VA Welch, eds. 2022. Cochrane Handbook for Systematic Reviews of Interventions. Cochrane. www.training.cochrane.org/handbook.\n\n\nSchwarzer, Guido, Hiam Chemaitelly, Laith J. Abu-Raddad, and Gerta Rücker. 2019. “Seriously Misleading Results Using Inverse of Freeman-Tukey Double Arcsine Transformation in Meta-Analysis of Single Proportions.” Research Synthesis Methods 10 (3): 476–83. https://doi.org/10.1002/jrsm.1348.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review and Meta-Regression of the Knowledge, Practices, and Training of Restaurant and Food Service Personnel Toward Food Allergies and Celiac Disease.” Edited by Louise Emilsson. PLOS ONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496."
  },
  {
    "objectID": "9_meta2.html#subgroup-analysis",
    "href": "9_meta2.html#subgroup-analysis",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.1 Subgroup Analysis",
    "text": "9.1 Subgroup Analysis\nSubgroup analysis, also known as moderator analysis, can be conducted to determine if specific study-related factors might explain some of the heterogeneity in our analysis. We essentially conduct one or more meta-analyses that are stratified by categorical variables of interest. Any such analyses should be pre-specified and identified in the review protocol. We should keep these analyses to a minimum, and focus on only those that are of most importance and have a strong rationale. For example, we may want to conduct a subgroup analysis to examine differences in pooled estimates across two or more variations of the intervention (or exposure), different control groups, study risk-of-bias ratings, study design, population characteristics (e.g., age group), or setting.\nThe process of subgroup analysis involves calculating a separate pooled measure of effect within each subgroup, then comparing the pooled estimates across groups with a statistical test. when comparing estimates across subgroups, an omnibus \\(Q\\) test is used, which compares whether all subgroup pooled estimates are equal. We have the option of calculating \\(\\tau^2\\) separately within each group, or assuming a common estimate of \\(\\tau^2\\) that is applied for each subgroup (the latter is recommended if there are only 5 or fewer studies in a subgroup).\nHowever, subgroup analyses have several limitations. They have low power to detect differences with small numbers of studies in each group, pooled estimates in each group will have lower precision, an absence of a detectable difference does not mean that the groups are equivalent, and any observed differences are only observational (and could be confounded by biases or other factors). At least 10 studies are recommended if conducting a subgroup analysis.\nTo conduct subgroup analysis in R, we can specify the subgroup option with any of the meta-analysis functions in the meta package.\n\n\n\n\n\n\nSubgroup Analysis Example\n\n\n\nWe will re-load the same dataset from the last session that contained 136 studies used in a meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018), as prepared by White et al. in the metadat repository.\n\n\nCode\npacman::p_load(\n  rio,        # load in an Excel file\n  here,       # loading data using relative path\n  tidyverse,  # data management\n  meta,       # meta-analysis\n  DT          # to visualize the dataset online\n)\n\nHartmann_2018 &lt;- import(here(\"assets\", \"Hartmann_2018.xlsx\"))\n\nHartmann_2018 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) \n\n\n\n\n\n\n\nWe can see that there is a variable called treatment reflecting the type of nicotine replacement therapy assessed. We will conduct a subgroup analysis to compare differences between the types of treatments.\n\n\nCode\nHartmann_meta &lt;- Hartmann_2018 |&gt; \n  metabin(event.e = x.nrt,\n          n.e = n.nrt, \n          event.c = x.ctrl, \n          n.c = n.ctrl, \n          studlab = study,\n          sm = \"RR\",\n          method = \"MH\", \n          random = TRUE,\n          fixed = FALSE, \n          method.tau = \"PM\",\n          hakn = TRUE,\n          MH.exact = TRUE,\n          subgroup = treatment,  # specify our subgroup variable\n          tau.common = FALSE,    # calculate tau separately for each subgroup\n          title = \"Nicotine Replacement Therapy and Smoking Cessation\"\n)\n\nHartmann_meta\n\n\nReview:     Nicotine Replacement Therapy and Smoking Cessation\n\nNumber of studies combined: k = 136\nNumber of observations: o = 64640\nNumber of events: e = 8889\n\n                         RR           95%-CI     t  p-value\nRandom effects model 1.5735 [1.4798; 1.6731] 14.61 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0411 [0.0155; 0.0861]; tau = 0.2028 [0.1245; 0.2934]\n I^2 = 38.2% [23.8%; 49.9%]; H = 1.27 [1.15; 1.41]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 218.60  135 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                      k      RR              95%-CI  tau^2\ntreatment = gum                      56  1.4907 [1.3529;    1.6424] 0.0423\ntreatment = patch                    51  1.6243 [1.4675;    1.7978] 0.0359\ntreatment = inhalator                 4  1.8521 [1.1959;    2.8684]      0\ntreatment = intranasal spray          4  1.9881 [1.3830;    2.8580]      0\ntreatment = tablets/lozenges          8  1.7175 [1.2230;    2.4118] 0.0965\ntreatment = oral spray                1  2.4752 [1.2396;    4.9423]     --\ntreatment = choice of product         7  1.4198 [1.1907;    1.6929] 0.0075\ntreatment = patch and inhalator       1  1.0686 [0.5747;    1.9870]     --\ntreatment = patch and lozenge         1  1.8256 [1.0064;    3.3117]     --\ntreatment = patch and gum             2  1.2975 [0.0004; 3977.9193] 0.4485\ntreatment = patch, gum, and lozenge   1 15.0000 [1.9993;  112.5414]     --\n                                       tau     Q   I^2\ntreatment = gum                     0.2057 90.42 39.2%\ntreatment = patch                   0.1895 65.20 23.3%\ntreatment = inhalator                    0  1.90  0.0%\ntreatment = intranasal spray             0  1.62  0.0%\ntreatment = tablets/lozenges        0.3107 23.81 70.6%\ntreatment = oral spray                  --  0.00    --\ntreatment = choice of product       0.0867 10.40 42.3%\ntreatment = patch and inhalator         --  0.00    --\ntreatment = patch and lozenge           --  0.00    --\ntreatment = patch and gum           0.6697  1.78 43.7%\ntreatment = patch, gum, and lozenge     --  0.00    --\n\nTest for subgroup differences (random effects model):\n                   Q d.f. p-value\nBetween groups 17.95   10  0.0558\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 135)\n\n\nWe can see that there are some differences in the pooled RR estimates across subgroups, but most groups contain very few studies. We could re-run our analysis and include only the four sub-groups which have more than 5 studies for a more reliable comparison.\n\n\nCode\nHartmann_meta &lt;- Hartmann_2018 |&gt; \n  filter(treatment == \"gum\" | treatment == \"patch\" | \n           treatment == \"tablets/lozenges\" | treatment == \"choice of product\") |&gt; \n  metabin(event.e = x.nrt,\n          n.e = n.nrt, \n          event.c = x.ctrl, \n          n.c = n.ctrl, \n          studlab = study,\n          sm = \"RR\",\n          method = \"MH\", \n          random = TRUE,\n          fixed = FALSE, \n          method.tau = \"PM\",\n          hakn = TRUE,\n          MH.exact = TRUE,\n          subgroup = treatment,  # specify our subgroup variable\n          tau.common = FALSE,    # calculate tau separately for each subgroup\n          title = \"Nicotine Replacement Therapy and Smoking Cessation\"\n)\n\nHartmann_meta\n\n\nReview:     Nicotine Replacement Therapy and Smoking Cessation\n\nNumber of studies combined: k = 122\nNumber of observations: o = 61062\nNumber of events: e = 8342\n\n                         RR           95%-CI     t  p-value\nRandom effects model 1.5543 [1.4587; 1.6562] 13.75 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0395 [0.0143; 0.0851]; tau = 0.1988 [0.1195; 0.2917]\n I^2 = 38.9% [23.8%; 51.0%]; H = 1.28 [1.15; 1.43]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 198.00  121 &lt; 0.0001\n\nResults for subgroups (random effects model):\n                                k     RR           95%-CI  tau^2    tau     Q\ntreatment = gum                56 1.4907 [1.3529; 1.6424] 0.0423 0.2057 90.42\ntreatment = patch              51 1.6243 [1.4675; 1.7978] 0.0359 0.1895 65.20\ntreatment = tablets/lozenges    8 1.7175 [1.2230; 2.4118] 0.0965 0.3107 23.81\ntreatment = choice of product   7 1.4198 [1.1907; 1.6929] 0.0075 0.0867 10.40\n                                I^2\ntreatment = gum               39.2%\ntreatment = patch             23.3%\ntreatment = tablets/lozenges  70.6%\ntreatment = choice of product 42.3%\n\nTest for subgroup differences (random effects model):\n                  Q d.f. p-value\nBetween groups 3.41    3  0.3327\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 121)\n\n\nOnce we remove the groups with few studies, we can see that there is no strong (significant) evidence of a difference in the size of the treatment effect based on the type of therapy."
  },
  {
    "objectID": "9_meta2.html#meta-regression",
    "href": "9_meta2.html#meta-regression",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.2 Meta-Regression",
    "text": "9.2 Meta-Regression\nMeta-regression is another technique we can use to evaluate possible factors associated with between-study heterogeneity in the measures of effect. Meta-regression models are mixed-effect regressions that have terms to account for the sampling error \\(\\epsilon_j\\) and the between-study heterogeneity \\(\\zeta_j\\), and one (or more) beta parameters representing our predictor variables of interest. For study \\(j\\), we can express such a model as:\n\\[\n\\hat{\\theta_j} = \\mu + \\beta{x}_j + \\zeta_j + \\epsilon_j\n\\]\nA subgroup analysis is special case of this regression model, where we include a categorical predictor variable as our covariate \\({x}_j\\). When we evaluate continuous predictors (e.g., publication year), a weighted least squares approach is used to estimate the beta coefficient that best fits the data. In addition to estimating the effect of the predictor of interest, the model also calculates an \\(R^2\\) value that represents the proportion of between-study variation that is explained by the predictor.\nSimilar to subgroup analysis, usually at least 10 studies are recommended to conduct meta-regression. Meta-regression has many of the same limitations as subgroup analysis. Additionally, one should not conduct analysis of a predictor variables containing aggregated data from the studies (e.g., mean age in each study), as this type of analysis can be affected by ecological bias.\nIn the R meta package, we can conduct meta-regression on our meta-analysis object (output) using the metareg function.\n\n\n\n\n\n\nMeta-Regression Example\n\n\n\nTo illustrate how to conduct a meta-regression, we will reload data on the effect of music interventions + standard care vs. standard care alone to reduce anxiety levels, from a systematic review of music interventions to improve various health outcomes in people with cancer (Bradt et al. 2021).\n\n\nCode\nBradt_2021 &lt;- import(here(\"assets\", \"Bradt_2021.xlsx\"))\n\nBradt_2021 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4))))\n\n\n\n\n\n\n\nWe will examine whether study publication year as a continuous variable might explain any of the heterogeneity in the measure of effect across studies.\n\n\nCode\nBradt_meta &lt;- Bradt_2021 |&gt; \n  metacont(n.e = int_n,\n           mean.e = int_mean,\n           sd.e = int_sd,\n           n.c = con_n, \n           mean.c = con_mean,\n           sd.c = con_sd,  \n           studlab = study,\n           sm = \"MD\", \n           random = TRUE,\n           fixed = FALSE,\n           method.tau = \"REML\",\n           hakn = TRUE, \n           title = \"Music Intervention and Anxiety\")\n\nBradt_reg &lt;- metareg(Bradt_meta, ~year)\nBradt_reg\n\n\n\nMixed-Effects Model (k = 17; tau^2 estimator: REML)\n\ntau^2 (estimated amount of residual heterogeneity):     32.4437 (SE = 13.9920)\ntau (square root of estimated tau^2 value):             5.6959\nI^2 (residual heterogeneity / unaccounted variability): 96.47%\nH^2 (unaccounted variability / sampling variability):   28.32\nR^2 (amount of heterogeneity accounted for):            1.74%\n\nTest for Residual Heterogeneity:\nQE(df = 15) = 217.2438, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nF(df1 = 1, df2 = 15) = 1.5686, p-val = 0.2296\n\nModel Results:\n\n          estimate        se     tval  df    pval       ci.lb     ci.ub    \nintrcpt  -842.7405  666.6950  -1.2641  15  0.2255  -2263.7674  578.2863    \nyear        0.4150    0.3314   1.2524  15  0.2296     -0.2913    1.1213    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can see in the results output that \\(I^2\\), representing the residual heterogeneity, is very high (&gt;96%). The test for residual heterogeneity also indicates there is still substantial heterogeneity present after accounting for publication year.\nThe \\(R^2\\) value is also only 1.7%, indicating the the publication year variable explain very little to no variability in effects across studies. The moderator test also indicates that this variable is not statistically significant (its 95% CI also crosses the null of no effect). If this variable was significant, we could say that for each additional year, the mean difference for a study is expected to increase by ~0.42 units.\nWe can also visualize the results of meta-regression analyses using a bubble plot.\n\n\nCode\nbubble(Bradt_reg, studlab = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubgroup Analysis and Meta-Regression Exercise\n\n\n\nWe will load data from the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions.\n\n\nCode\nYoung_2019 &lt;- import(here(\"assets\", \"Young_2019_ma.xlsx\"))\n\nYoung_2019 |&gt; datatable(\n  rownames = FALSE,\n  options = list(\n    columnDefs = list(list(className = 'dt-center', \n                           targets = 0:4)))) |&gt; \n  formatStyle(columns = colnames(Young_2019), fontSize = '70%')\n\n\n\n\n\n\n\nUsing these data, first conduct a subgroup analysis to examine the difference between RCTs and non-randomized studies for the behaviour outcome. Given a small number of RCTs, use a common estimate of \\(\\tau^2\\).\n\nWhat is the \\(I^2\\) in each subgroup?\nWhat is the evidence that the overall SMD in each group is different?\nCan you create a forest plot for this analysis that includes these two subgroups?\n\nNow conduct a meta-regression to examine whether publication year of the study is associated with any of the variability in the behaviour outcome. How would you interpret the results?\nNow conduct another meta-regression, but instead of publication year, examine document type (journal or thesis/dissertation) as a predictor. How would you interpret those results?"
  },
  {
    "objectID": "9_meta2.html#publication-bias",
    "href": "9_meta2.html#publication-bias",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.3 Publication Bias",
    "text": "9.3 Publication Bias\nPublication bias is a major concern in systematic reviews. It refers to the probability that studies are more likely to be published, and specific outcomes and results in a study are more likely to be reported, if the findings are statistically significant or confirm the hypothesis of interest. Publication bias is one of several non-reporting biases that might be present in a systematic review. Others include citation bias, time-lag bias, multiple publication bias, language bias, and outcome reporting bias.\nFunnel plots can be used to visualize the possibility of publication bias in a meta-analysis. They are a scatter plot of the measures of effect of each included study against their standard error. If publication bias is not present, we would expect a funnel shape, with more precise (larger) studies clustering at the top and smaller, less precise studies scattered more widely at the bottom. Bias due to missing results presents as funnel plot asymmetry, where smaller studies without statistically significant results are missing on the bottom of the plot.\nHowever, asymmetry can be due to a variety of small-study effects, not just publications bias (e.g., higher risk of bias in smaller studies, true heterogeneity, spurious relationships, or chance). For this reason, it is recommended to use contour-enhanced funnel plots that include shaded regions for statistical significance thresholds (e.g., P = 0.01, 0.05) to identify if the missing pattern is likely due to publication bias or other small-study effects. There are various statistical tests for funnel plot asymmetry to assess whether the associated between effect sizes and precision is greater than expected to occur by chance (Sterne et al. 2011). However, these tests have many limitations. For example, they should not be used whether there are less than 10 studies and when the studies all have a similar sample size. The performance and reliability on the tests on observational studies is also not well researched.\nOne such publication bias (small-study effects) test is Egger’s regression test. However, this test should only be used on continuous outcomes. In can be implemented in the R meta package using the metabias function and the method.bias = \"linreg\" option. For SMD outcomes, it is recommended to use alternative method.bias = \"Pustejovsky\" option when conducting the test, which uses a different method of calculating the SE in each study. This method requires specification of the sample size for each comparison group. For dichotomous outcomes, the Harbord test or Peters test can be used with method.bias = \"Harbord\" and method.bias = \"Peters\", although there are other options as well. Many of these tests may not perform optimally or reliably when heterogeneity is very high.\nIf publication bias or other small-study effects are suspected, there are various statistical methods that can be used to examine the influence of the bias on the pooled measure of effect. The most popular of these methods is called the trim and fill method. This method imputes “missing” studies until the funnel is symmetrical. The pooled effect calculated using this method is considered bias-corrected. This method is also not reliable when heterogeneity is high. There are various other bias-adjustment methods available but no consensus on which approach is best.\n\n\n\n\n\n\nPublication Bias Example\n\n\n\nWe will re-examine the meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018). We will conduct an overall pooled meta-analysis combining all therapies together, then assess for possible small-study effects using a contour-enhanced funnel plot.\n\n\nCode\nHartmann_meta &lt;- Hartmann_2018 |&gt; \n  metabin(event.e = x.nrt,\n          n.e = n.nrt, \n          event.c = x.ctrl, \n          n.c = n.ctrl, \n          studlab = study,\n          sm = \"RR\",\n          method = \"MH\", \n          random = TRUE,\n          fixed = FALSE, \n          method.tau = \"PM\",\n          hakn = TRUE,\n          MH.exact = TRUE,\n          title = \"Nicotine Replacement Therapy and Smoking Cessation\"\n)\nHartmann_meta\n\n\nReview:     Nicotine Replacement Therapy and Smoking Cessation\n\nNumber of studies combined: k = 136\nNumber of observations: o = 64640\nNumber of events: e = 8889\n\n                         RR           95%-CI     t  p-value\nRandom effects model 1.5735 [1.4798; 1.6731] 14.61 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0411 [0.0155; 0.0861]; tau = 0.2028 [0.1245; 0.2934]\n I^2 = 38.2% [23.8%; 49.9%]; H = 1.27 [1.15; 1.41]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 218.60  135 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 135)\n\n\n\n\nCode\ncol.contour = c(\"gray50\", \"gray70\", \"gray90\")  # set custom contour colours\n\nfunnel.meta(Hartmann_meta, \n            contour = c(0.9, 0.95, 0.99),      # set contour P value regions\n            col.contour = col.contour)         # apply custom contour colour\n\n# Add legend in top right position based on x and y coordinates\nlegend(x = 7, y = 0.01, \n       legend = c(\"P &lt; 0.1\", \"P &lt; 0.05\", \"P &lt; 0.01\"),\n       fill = col.contour)\n\n\n\n\n\nWe can see most of the studies with greater precision have clustered at the top, with many in the statistically significant regions. At the bottom, there are a handful of smaller studies, with only one being in the very significant region. Based on this plot, there does not appear to be a strong indication of small-study biases, though if anything, we could be missing some medium-sized studies in the non-significant region. We will now conduct a Peters test for asymmetry.\n\n\nCode\nmetabias(Hartmann_meta, method.bias = \"Peters\")\n\n\nReview:     Nicotine Replacement Therapy and Smoking Cessation\n\nLinear regression test of funnel plot asymmetry\n\nTest result: t = 1.33, df = 134, p-value = 0.1862\n\nSample estimates:\n    bias se.bias intercept se.intercept\n 13.8134 10.3970    0.4256       0.0401\n\nDetails:\n- multiplicative residual heterogeneity variance (tau^2 = 6.8892)\n- predictor: inverse of total sample size\n- weight:    inverse variance of average event probability\n- reference: Peters et al. (2006), JAMA\n\n\nWe can see that the test P value is 0.186, which does not suggest that small-study effects are present, but we cannot rule them out with only this test. For illustration purposes, we will conduct a trim and fill anlaysis to examine the possible magnitude of possible missing studies.\n\n\nCode\nHartmann_meta_trim &lt;- trimfill(Hartmann_meta)\nHartmann_meta_trim\n\n\nReview:     Nicotine Replacement Therapy and Smoking Cessation\n\nNumber of studies combined: k = 159 (with 23 added studies)\nNumber of observations: o = 72976\nNumber of events: e = 9512\n\n                         RR           95%-CI     t  p-value\nRandom effects model 1.4659 [1.3596; 1.5806] 10.03 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.1113 [0.0605; 0.1914]; tau = 0.3337 [0.2460; 0.4375]\n I^2 = 50.2% [40.2%; 58.6%]; H = 1.42 [1.29; 1.55]\n\nTest of heterogeneity:\n      Q d.f.  p-value\n 317.47  158 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Paule-Mandel estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 158)\n- Trim-and-fill method to adjust for funnel plot asymmetry\n\n\nWe can see that data from 23 missing studies were imputed. As expected, the updated RR is slightly more conservative compared to the original RR estimate. We can also produce a new funnel plot with the imputed missing studies.\n\n\nCode\nfunnel.meta(Hartmann_meta_trim, \n            contour = c(0.9, 0.95, 0.99),  \n            col.contour = col.contour)\n\nlegend(x = 7, y = 0.01, \n       legend = c(\"P &lt; 0.1\", \"P &lt; 0.05\", \"P &lt; 0.01\"),\n       fill = col.contour)\n\n\n\n\n\nWe can see the new studies (circles with no fill colour) have been added on the left side of the plot, representing mostly medium-sized studies that would be expected to show no intervention effect.\n\n\n\n\n\n\n\n\nPublication Bias Exercise\n\n\n\nFor this exercise, examine possible publication bias in the meta-analysis of the effect of music interventions + standard care vs. standard care alone to reduce anxiety levels (Bradt et al. 2021). You can use the same, saved Bradt_meta object from the earlier example.\n\nProduce a contour-enhanced funnel plot. Is there evidence of asymmetry?\nBased on the contour regions and meta-analysis results, could this be due to publication bias or other small-study effects?\nConduct Egger’s regression test. How would you interpret the result?\nConduct a trim-and-fill analysis. How many “missing studies” were added, and what impact does the missing study imputation have on the overall MD?"
  },
  {
    "objectID": "9_meta2.html#complex-data-issues",
    "href": "9_meta2.html#complex-data-issues",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.4 Complex Data Issues",
    "text": "9.4 Complex Data Issues\nIn an earlier section Chapter 6, we introduced unit-of-analysis issues. This section will provide some additional details on how to address some of these issues using statistical techniques. We will examine how to adjust standard errors in clustered studies and studies that report more than one outcome of the same type. We will also examine different methods of combining multiple intervention/exposure comparison groups.\n\n9.4.1 Cluster Trial Adjustments\nCluster trials can present an issue if an intervention is applied at the cluster level (e.g., school), but the outcomes are measured and reported on individual participants within the cluster (e.g., students) and the clustering is not accounted for in the analysis. In this case, the reported results can be adjusted for a design effect as per the following formula:\n\\[\nDEFF = 1 + (M - 1)ICC\n\\]\nIn this formula, \\(M\\) refers to the average cluster size in the study, and \\(ICC\\) is the intra-class correlation coefficient. This value is rarely reported, so would need to be estimated from a similar study or other literature. The sample size in each group can then be divided by the design effect to obtain proper estimates. For dichotomous outcomes, both the number of events and sample size should be divided by this effect. Alternatively, the standard error of the measure of effect of the study can be multiplied by the square root fo the design effect.\n\n\n\n\n\n\nCluster Trial Adjustment Example\n\n\n\nIn the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions, several cluster trials were included and a design effect was used to adjust the standard errors of studies that did not report the correct analysis of those trials. A common ICC value of 0.1 was used for the formula, based on a study of similar outcomes in the literature.\nFor example, one of the studies reported an analysis of school food handler knowledge outcomes, with randomization conducted at the school level. There were 33 food handlers in the intervention group and 46 in the control group, with 8 schools in each group. Given these data, the design effect (DEFF) was calculated as:\n\n\nCode\nM &lt;- (33+46)/(8+8)\nICC &lt;- 0.1\nDEFF &lt;- 1 + (M-1)*(ICC)\nDEFF\n\n\n[1] 1.39375\n\n\n\n\n\n\n9.4.2 Multiple Outcomes in the Same Study\nIf a study reports multiple outcomes of the same type, such that they would be included in the same meta-analysis, the outcomes would not be independent and this extra level of variation should be accounted for. This could be due to multiple sub-groups of populations in the same study (hierarchical clustering) or multiple measurements on the same participants in the same study (correlated effects). The most common approaches to address this are as follows:\n\nAveraging of effects approach: in this method, we take a weighted average of the multiple similar outcomes within a study, to produce one estimate per study per meta-analysis. For correlated effects, this requires an estimate of the correlation between the two or more outcome measurements being combined.\nThree-level regression model: this method is appropriate for hierarchical clustering only, not correlated effects. It adds another random-effect for the outcomes being clustered within studies.\nRobust variance estimation: uses a special method of calculating the standard error that is consistent even when assumptions (e.g., independence of observations) are not met.\nCorrelated and hierarchical effects models: use the robust variance approach along with multilevel modelling to account for both correlated and hierarchical effects in the same model.\n\nAdditional reading on these approaches and how to implement them in R is available in the Doing Meta-Anlaysis in R online book. Note that these advanced methods may not be reliable when the number of studies included in the analysis is very small.\n\n\n9.4.3 Multiple Comparison Groups\nIf a study reports a comparison of two or more intervention (or exposure) groups to a control or comparison group, we need to avoid double-counting those outcomes in the analysis. For example, a study might report separate comparison data for different genders or age groups, or different variations (e.g., doses) of an intervention. There are different approaches to address this issue:\n\nPick the most relevant of the intervention/exposure groups and exclude the others. However, the decision of what is most relevant must be justified and this results in a loss of data.\nSplit the sample size of the common control/comparison group: for example, if we had two variations of an intervention compared to a common control group with 200 participants, we would split the control group into two groups of size 100. However, this approach still results in the outcomes being correlated.\nCombine the two (or more) intervention/exposure groups into a weighted average. This is the preferred approach and is discussed and illustrated below.\nIf there are many studies with multiple comparison groups, consider a network meta-analysis.\n\nTo combine two groups together, the sample sizes for the two groups can simply be added together. The means and SDs can be combined with the following formulas:\n\\[\n\\text{Mean} = \\frac{{N_1}{Mean_1}+{N_2}{Mean_2}}{{N_1}+{N_2}}\n\\]\n\\[\n\\text{SD} = \\sqrt\\frac{{({N_1}-1){SD^2_1}+({N_2}-1){SD^2_2}+\n\\frac{{N_1}{N_2}}{{N_1}+{N_2}}}{({Mean^2_1}+{Mean^2_2}-2{Mean_1}{Mean_2})}}\n{{N_1}+{N_2}-1}\n\\]\nThankfully, we can use an R function to automate this calculation.\n\n\nCode\npool.groups = function(n1, n2, m1, m2, sd1, sd2) {\n\n    n1 = n1\n    n2 = n2\n    m1 = m1\n    m2 = m2\n    sd1 = sd1\n    sd2 = sd2\n\n    if (is.numeric(n1) == FALSE) {\n        stop(\"'n1' must by of type numeric().\")\n    }\n\n    if (is.numeric(n2) == FALSE) {\n        stop(\"'n2' must by of type numeric().\")\n    }\n\n    if (n1 &lt; 1 | n2 &lt; 1) {\n        stop(\"'n1' and 'n2' must both the greater than 0.\")\n    }\n\n    if (is.numeric(m1) == FALSE) {\n        stop(\"'m1' must by of type numeric().\")\n    }\n\n    if (is.numeric(m2) == FALSE) {\n        stop(\"'m2' must by of type numeric().\")\n    }\n\n    if (is.numeric(sd1) == FALSE) {\n        stop(\"'sd1' must by of type numeric().\")\n    }\n\n    if (is.numeric(sd2) == FALSE) {\n        stop(\"'sd2' must by of type numeric().\")\n    }\n\n    Npooled = n1 + n2\n    Mpooled = (n1 * m1 + n2 * m2)/(n1 + n2)\n    SDpooled = sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2 + (((n1 * n2)/(n1 + n2)) * (m1^2 + m2^2 - 2 * m1 *\n        m2)))/(n1 + n2 - 1))\n\n    return(data.frame(Mpooled, SDpooled, Npooled))\n}\n\n\nUsing our new pool.groups function, we can now combined two groups. Suppose we had two similar interventions in one study with the following parameters: sample size of 50 in each group, mean 1 = 2.0, SD 1 = 1.5, mean 2 = 2.5, SD 2 = 2.3):\n\n\nCode\npool.groups(n1 = 50, n2 = 50,\n            m1 = 2.0, sd1 = 1.5,\n            m2 = 2.5, sd2 = 2.3)\n\n\n  Mpooled SDpooled Npooled\n1    2.25 1.948089     100\n\n\nIf combining more than two groups, we would first combine two groups together, then combine the results of that new combined group with the third group, and so on.\n\n\n\n\n\n\nCombining Groups Example\n\n\n\nIn practice, if we will be combining interventions groups like this, we will want to create a new dataframe to analyze with the combined groups. The saved combined values can then be inserted into the new dataframe. For example:\n\n\nCode\ndata &lt;- tibble(study_ID = as.factor(c(1,2,3,3)),\n               intervention = c(\"Training\", \"Training\", \"Training-Group 1\", \"Training-Group 2\"),\n               treat_mean = c(1.5, 2.1, 2.2, 2.7),\n               treat_SD = c(1.0, 1.7, 2.0, 2.1),\n               treat_n = c(100, 50, 70, 60),\n               control_mean = c(1.2, 1.5, 2.0, 1.9),\n               control_SD = c(1.0, 1.3, 1.75, 1.8),\n               control_n = c(100, 50, 75, 75),\n                )\ndata\n\n\n# A tibble: 4 × 8\n  study_ID intervention     treat_mean treat_SD treat_n contro…¹ contr…² contr…³\n  &lt;fct&gt;    &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 1        Training                1.5      1       100      1.2    1        100\n2 2        Training                2.1      1.7      50      1.5    1.3       50\n3 3        Training-Group 1        2.2      2        70      2      1.75      75\n4 3        Training-Group 2        2.7      2.1      60      1.9    1.8       75\n# … with abbreviated variable names ¹​control_mean, ²​control_SD, ³​control_n\n\n\nIn this example, we want to combine the two training interventions in Study 3. We use the pool.groups function.\n\n\nCode\n# Reference the required values below using the respective row numbers\ncombined &lt;- pool.groups(n1 = data[3,]$treat_n, \n                        n2 = data[4,]$treat_n,\n                        m1 = data[3,]$treat_mean, \n                        sd1 = data[3,]$treat_SD,\n                        m2 = data[4,]$treat_mean, \n                        sd2 = data[4,]$treat_SD)\n\n\nWe can now create a new version of the dataframe, remove one of the treatment rows, and enter the new combined values in place of the previous intervention values.\n\n\nCode\n# First create the new dataframe with 1 row per study\ndata_combined &lt;- data |&gt; \n  group_by(study_ID) |&gt; \n  slice_head() |&gt; \n  ungroup()\n\n# Update Study 3's description and values with the new combined values\ndata_combined$intervention[\"study_ID\" = 3] &lt;- \"Training-combined\" \ndata_combined$treat_mean[\"study_ID\" = 3] &lt;- combined$Mpooled\ndata_combined$treat_SD[\"study_ID\" = 3] &lt;- combined$SDpooled\ndata_combined$treat_n[\"study_ID\" = 3] &lt;- combined$Npooled\ndata_combined\n\n\n# A tibble: 3 × 8\n  study_ID intervention      treat_mean treat_SD treat_n contr…¹ contr…² contr…³\n  &lt;fct&gt;    &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 1        Training                1.5      1        100     1.2    1        100\n2 2        Training                2.1      1.7       50     1.5    1.3       50\n3 3        Training-combined       2.43     2.05     130     2      1.75      75\n# … with abbreviated variable names ¹​control_mean, ²​control_SD, ³​control_n\n\n\n\n\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nSterne, J A C, A J Sutton, J P Ioannidis, N Terrin, D R Jones, J Lau, J Carpenter, et al. 2011. “Recommendations for Examining and Interpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised Controlled Trials.” BMJ (Clinical Research Ed.) 343: d4002. http://www.scopus.com/inward/record.url?eid=2-s2.0-79961238388&partnerID=40&md5=116d9b214a2fee3add89b254255f8cde.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "10_grade.html#summary-of-findings-tables-and-review-reporting",
    "href": "10_grade.html#summary-of-findings-tables-and-review-reporting",
    "title": "10  Quality of Evidence and Reporting",
    "section": "10.1 Summary of Findings Tables and Review Reporting",
    "text": "10.1 Summary of Findings Tables and Review Reporting\nSummary of findings tables are required for all reviews conducted through the Cochrane Collaboration. They present the main findings of the review in a user-friendly, tabular format, and appear as part of the executive summary of the review report. The concept of summary of findings tables applied to any review, and such a table can be included in non-Cochrane reviews as well. The Cochrane Collaboration requires that summary of findings tables include only the most important outcomes of the review, limited to no more than seven.\nThe general template for a summary of findings table is as follows:\n\nDescription of population and setting of available evidence and the comparison groups (at the top of the table).\nList of the most important outcomes (as separate rows in the table).\nFor each outcome the table, a measure of the typical burden, relative magnitude of effect, number of participants and studies, and certainty of evidence (as separate column for each outcome).\nThe final column includes overall comments, and explanations (footnotes) should be added as needed at the bottom of the table.\n\nThe Cochrane Collaboration has created an online software called GRADEpor GDT to create summary of findings tables. The software is currently free for groups of up to three researchers.\n\n10.1.1 Statistical Considerations\nMeta-analysis is typically conducted using relative measures of effect (e.g., risk ratio, odds ratio) because these are more consistent across studies than absolute measures (e.g., risk difference). However, for the summary of findings table, the effect for each outcome should be presented as both a relative and absolute effect. To determine an absolute effect from a meta-analysis estimate, we require an estimate of the assumed comparator risk (ACR). The ACR refers to the risk of the outcome in the comparison (or control) group (that does not receive the intervention).\nThere are different methods to calculate an ACR for each outcome. One option is to take the median value of the risk of the outcome among participants in the control groups of studies included in the review. However, if there is significant variation in the control group risk among studies in the review, then another option is to select two representative values for high and low risk groups. This could be the second highest and second lowest risks among the included studies for that particular outcome. A corresponding intervention risk can then be calculated using the relative measure of effect from meta-analysis and the ACR. For dichotomous outcomes, such risks are generally presented as a risk per 1000 (or 100) people. The formulas to calculate this risk for risk ratio (RR) and odds ratio (OR) outcomes are shown below. In both cases, the lower and upper 95% confidence limits can be calculated by replacing the RR or OR value with its lower and upper confidence limits, respectively.\n\\[\n\\text{Corresponding intervention risk per 1000} = 1000\\times ACR\\times RR\n\\]\n\\[\n\\text{Corresponding intervention risk per 1000} = 1000\\times \\left(\\frac{OR\\times ACR}{1-ACR+ \\left( OR\\times ACR \\right)} \\right)\n\\]\nFor time-to-event outcomes, measured using a hazard ratio (HR), there are three methods of calculating the corresponding intervention risk. The first calculates the absolute risk of event-free survival within a particular time period, the second calculates the absolute risk of an event within a particular time period, and the third calculates the median time to the event (in months or years). The formulas for calculating these risks are as follows:\n\\[\n\\text{Corresponding intervention risk per 1000} = 1000\\times e^\\left[\\ln(\\text{proportion of event-free participants})\\times HR \\right]\n\\]\n\\[\n\\text{Corresponding intervention risk per 1000} = 1000\\times e^\\left[\\ln(1-\\text{proportion of event-free participants})\\times HR \\right]\n\\]\n\\[\n\\text{Corresponding median survival time} = \\frac{\\text{comparator group median survival time}}{HR}\n\\]\nFor continuous outcomes (e.g., SMD), the corresponding intervention risk is usually the mean difference or SMD and CI of the meta-analysis estimate.\n\n\n10.1.2 Summary of Findings Table Contents\nEach row in a summary of findings table reflects one of the pre-selected, most important outcomes (up to seven). For each outcome, you should provide the measurement scale (if applicable) and the measurement time frame (e.g., length of follow-up). Outcomes that were pre-selected but that had no data available from the studies in the review should still be included in this table.\nThe first column in the table will be the list of outcomes. The second column is the ACR and the third is the corresponding intervention risk. The relative effect and CI (e.g., RR, OR) is presented in the fourth column (this column is not required for continuous outcomes). The final three columns include data on the number of participants and studies included for each outcome, the GRADE rating, and any comments.\n\n\n\n\n\n\nSummary of Findings Table Examples\n\n\n\nAn example of a summary of findings table from a systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). In this review, separate summary of findings tables were created for evidence in two main population groups (adult cancer patients and pediatric cancer patients). The example below shows the first three outcomes from the adult cancer patient table.\n\nIn this table, footnote 1 indicates that all outcomes were downgraded two levels in the GRADE assessment for high risk of bias (because participants could not be blinded to the music intervention and the outcomes were measured using self‐report). The outcomes shown above were also downgraded 1-2 levels due to inconsistency because of high levels of heterogeneity in estimates.\nAnother example of a summary of findings table showing illustrative comparative risks is provided below, from a systematic review about the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018).\n\nThe footnotes in this table indicated that most studies were “judged to be at unclear or high risk of bias, but restricting to only studies at low risk of bias did not significantly alter the effect”. Additionally, while some publication bias was expected, results were not likely to change significantly due to this.\n\n\n\n\n\n\n\n\nSummary of Findings Table Exercise\n\n\n\nIn a systematic review that assessed the association between long-term exposure to residential green spaces and mortality in adults (Rojas-Rueda et al. 2019), the authors found that the pooled hazard ratio (HR) for green space exposure on all-cause mortality was 0.96 (95% CI: 0.94 to 0.97). Green space exposure was defined as a 0.1 increment in NDVI (normalized difference vegetation index) in a residential buffer zone of 500 m or less.\n\nAssuming a proportion of event-free participants of 0.9 (90%) over 10 years, what is the absolute risk of event-free survival within this time period? Calculate for the HR and its 95% CI.\nExamine the details of the studies contributing to the meta-analysis in this review see Table 1 and Figure 2 here. Draft a mock summary of findings table for this outcome. What other details would you include in the table? (Disregard the GRADE rating column for now)."
  },
  {
    "objectID": "10_grade.html#certainty-of-evidence-assessment",
    "href": "10_grade.html#certainty-of-evidence-assessment",
    "title": "10  Quality of Evidence and Reporting",
    "section": "10.2 Certainty of Evidence Assessment",
    "text": "10.2 Certainty of Evidence Assessment\nThe Grades of Recommendation, Assessment, Development and Evaluation Working Group has developed a certainty of evidence assessment approach for systematic reviews called GRADE, which has been adopted by the Cochrane Collaboration. GRADE is applied to each outcome in a systematic review, and determines the level of confidence that the calculated measure of effect or association (from meta-analysis) represents the true effect. There are four possible GRADE ratings: high, moderate, low, and very low.\nStudies begin at a high rating and can be rated down based on five criteria. In intervention reviews, non-randomized studies are typically rated down to a low rating automatically due to risks of bias. There are also three possible criteria that can increase the GRADE rating. The criteria are described below.\n\n10.2.1 Risk of bias\nThe risk of bias of individual studies contributing to a specific outcome can affect our confidence in that outcome. This criterion uses the risk-of-bias judgement from the Cochrane RoB Tool for RCTs or from the ROBINS-I/ROBINS-E for observational studies (as applicable). When most of the evidence comes from studies that have a crucial limitation (high risk of bias) for one risk-of-bias item, or some concerns for multiple items, the certainty of evidence can be downgraded by one level. In severe cases, certainty can be downgraded by two levels.\n\n\n10.2.2 Inconsistency\nThis criterion aims to assess the extent of inconsistency or heterogeneity in the estimate of effect that is unexplained after investigating subgroup and meta-regression analyses. Inconsistency can be measured using \\(I^2\\), \\(\\tau\\), and/or prediction intervals, as previously covered in the Chapter 8 session. If heterogeneity in the estimate is considered substantial or important, and there are no identified explanations for the heterogeneity, then consider downgrading by one level.\n\n\n10.2.3 Indirectness\nThis criterion assesses whether the studies contributing evidence to a particular outcome do not directly match with the review question’s population, intervention/exposure, comparison, or outcome. For example, if in a systematic review about the effectiveness of food handler education and training interventions (Young et al. 2019), studies were considered to provide indirect evidence on the population if their participants included a mix of food handlers (working in the food service industry) and consumers who cook food at home. The same principle applies to the other PICO/PECO elements. It might be appropriate to downgrade if most of the evidence for an outcome has indirect measurement of one or more PICO/PECO elements.\n\n\n10.2.4 Imprecision\nIf there is sufficient uncertainty about the magnitude and direction of the meta-analysis estimate, one can rate downgrade for imprecision. In the most recent suggested approach to this criterion (Schünemann et al. 2022), it is recommended to pre-identify key thresholds for what is a considered a small, moderate, or large effect for the outcome of interest. This imprecision assessment and the threshold should be on the absolute scale, so should use the corresponding intervention risk approach as described above. Then determine if you expect the meta-analysis effect will lie between two thresholds or beyond a threshold (e.g., small or greater). This is the target rating of the uncertainty.\nThe next step is to assess whether the CI for the corresponding intervention risk crosses one or more threshold boundaries, and downgrade a number of levels for each threshold crossed. For example, if we set our target at detecting a small or greater effect, and the effect was moderate but the CI also include the null (zero effect), evidence would be downgraded one level for crossing below the “small” threshold.\nFor continuous outcomes, one can use the well-defined SMD thresholds of small = 0.2, moderate = 0.5, and large = 0.8 (Schünemann et al. 2022). If analyzing a raw mean difference, one should use established thresholds for the outcome scale (if there are any), otherwise you can re-express the outcome as a SMD for the purposes of conducting the imprecision rating.\nNote that when conducting random-effects meta-analysis, high heterogeneity can also cause imprecision (wide CIs), so authors may need to consider this and may decide to rate down only for inconsistency and not for imprecision (especially if the total number of participants across included studies is reasonably large). For example, to detect a SMD of at least 0.2, a sufficient sample size will often be 800 (400 per comparison group) (Schünemann et al. 2022).\n\n\n10.2.5 Publication bias\nThe final downgrading criterion relates to possible publication bias. This criterion can be informed by a separate risk-of-bias assessment for missing data, as described in Chapter 5, to determine the extent that additional studies might have been excluded from the meta-analysis because they were not published. Evaluation of meta-analysis funnel plots and publication bias tests (if appropriate) can also inform this criterion.\n\n\n10.2.6 Upgrading criteria\nThere are three possible criteria that could lead to increasing the GRADE rating for an outcome: large effect, dose response, or all plausible confounding and bias would reduce (rather than increase) the effect. However, it is rare that the evidence will meet the necessary criteria to be upgraded based on any of these factors.\nLarge effects: If evidence predominantly comes from well-conducted, low risk of bias studies and the effect estimate is large (e.g., RR&gt;2 or &lt;0.5), one could consider upgrading.\nDose-response: The presence of a dose-response gradient for the effect can increase confidence and certainty in the effect, potentially warranting upgrading.\nPlausible confounding: Sometimes all plausible confounding factors would be expected to under-estimate the apparent effect estimate, which increases our confidence that the actual effect should be larger than what is reported.\n\n\n\n\n\n\nGRADE Example\n\n\n\nAn example of GRADE rating from a summary-of-findings table in systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). This GRADE rating and explanation is shown for effects of the intervention on anxiety levels in pediatric patients.\n\nIn this GRADE assessment, footnote 1 indicates the evidence was downgraded two levels for high risk of bias. The two trials were at high risk of bias because participants could not be blinded to the music intervention and outcome was measured using self-report. Footnote 2 indicates that evidence was downgraded one level for serious inconsistency across studies as evidenced by \\(I^2 = 76\\%\\). Footnote 3 indicates that evidence was downgraded two levels for imprecision due to a small number of participants (from only two trials).\n\n\n\n\n\n\n\n\nSummary of Findings Table Exercise\n\n\n\nUsing the systematic review of the association between long-term exposure to residential green spaces and mortality in adults (Rojas-Rueda et al. 2019), as investigated in the earlier exercise, we will now attempt to conduct a GRADE evaluation of the evidence contributing to the main all-cause mortality outcome.\n\nReview the article, including tables and figures, and come up with a proposed GRADE assessment for each of the five downgrading criteria. Note that you will need to examine the supplementary material for detailed results of the risk-of-bias and publication bias assessments.\nBased on the information available, would you consider upgrading any levels based on the upgrading criteria?\nWhat is your proposed overall GRADE rating for this outcome?\n\n\n\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSchünemann, Holger J., Ignacio Neumann, Monica Hultcrantz, Romina Brignardello-Petersen, Linan Zeng, M. Hassan Murad, Ariel Izcovich, et al. 2022. “GRADE Guidance 35: Update on Rating Imprecision for Assessing Contextualized Certainty of Evidence and Making Decisions.” Journal of Clinical Epidemiology 150 (October): 225–42. https://doi.org/10.1016/j.jclinepi.2022.07.015.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "11  References",
    "section": "",
    "text": "Bartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr\nOtahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A\nSystematic Review and Meta-Analysis of Workplace Mindfulness Training\nRandomized Controlled Trials.” Journal of Occupational Health\nPsychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo.\n2021. “Music Interventions for Improving Psychological and\nPhysical Outcomes in People with Cancer.” Cochrane Database\nof Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees.\n2021. “The Association Between Silica Exposure, Silicosis and\nTuberculosis: A Systematic Review and Meta-Analysis.” BMC\nPublic Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nHargreaves, Sally, Kieran Rustage, Laura B Nellums, Alys McAlpine,\nNicola Pocock, Delan Devakumar, Robert W Aldridge, et al. 2019.\n“Occupational Health Outcomes Among International Migrant Workers:\nA Systematic Review and Meta-Analysis.” The Lancet Global\nHealth 7 (7): e872–82. https://doi.org/10.1016/S2214-109X(19)30204-9.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and\nTim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control\nfor Smoking Cessation.” Cochrane Database of Systematic\nReviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nHiggins, J P T, S G Thompson, J J Deeks, and D G Altman. 2003.\n“Measuring Inconsistency in Meta-Analyses.” BMJ\n(Clinical Research Ed.) 327 (7414): 557–60. https://doi.org/10.1136/bmj.327.7414.557.\n\n\nHiggins, JPT, J Thomas, J Chandler, M Cumpston, T Li, MJ Page, and VA\nWelch, eds. 2022. Cochrane Handbook for\nSystematic Reviews of Interventions.\nCochrane. www.training.cochrane.org/handbook.\n\n\nJarach, Carlotta M., Alessandra Lugo, Marco Scala, Piet A. van den\nBrandt, Christopher R. Cederroth, Anna Odone, Werner Garavello, Winfried\nSchlee, Berthold Langguth, and Silvano Gallus. 2022. “Global\nPrevalence and Incidence of\nTinnitus: A Systematic Review and Meta-analysis.” JAMA Neurology 79\n(9): 888–900. https://doi.org/10.1001/jamaneurol.2022.2189.\n\n\nLangan, Dean, Julian P. T. Higgins, Dan Jackson, Jack Bowden, Areti\nAngeliki Veroniki, Evangelos Kontopantelis, Wolfgang Viechtbauer, and\nMark Simmonds. 2019. “A Comparison of Heterogeneity Variance\nEstimators in Simulated Random-Effects Meta-Analyses.”\nResearch Synthesis Methods 10 (1): 83–98. https://doi.org/10.1002/jrsm.1316.\n\n\nMoher, David, Larissa Shamseer, Mike Clarke, Davina Ghersi, Alessandro\nLiberati, Mark Petticrew, Paul Shekelle, and Lesley A Stewart. 2015.\n“Preferred Reporting Items for Systematic Review and Meta-Analysis\nProtocols (PRISMA-P) 2015 Statement.” Systematic\nReviews 4 (1): 1. https://doi.org/10.1186/2046-4053-4-1.\n\n\nMunn, Zachary, Sandeep Moola, Karolina Lisy, Dagmara Riitano, and\nCatalin Tufanaru. 2015. “Methodological Guidance for Systematic\nReviews of Observational Epidemiological Studies Reporting Prevalence\nand Cumulative Incidence Data.” International Journal of\nEvidence-Based Healthcare 13 (3): 147–53. https://doi.org/10.1097/XEB.0000000000000054.\n\n\nMunn, Zachary, Cindy Stern, Edoardo Aromataris, Craig Lockwood, and Zoe\nJordan. 2018. “What Kind of Systematic Review Should\nI Conduct? A Proposed Typology and Guidance\nfor Systematic Reviewers in the Medical and Health Sciences.”\nBMC Medical Research Methodology 18 (1): 5. https://doi.org/10.1186/s12874-017-0468-4.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela\nPerez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality:\nA Systematic Review and Meta-Analysis of Cohort Studies.” The\nLancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSchünemann, Holger J., Ignacio Neumann, Monica Hultcrantz, Romina\nBrignardello-Petersen, Linan Zeng, M. Hassan Murad, Ariel Izcovich, et\nal. 2022. “GRADE Guidance 35: Update on Rating\nImprecision for Assessing Contextualized Certainty of Evidence and\nMaking Decisions.” Journal of Clinical Epidemiology 150\n(October): 225–42. https://doi.org/10.1016/j.jclinepi.2022.07.015.\n\n\nSchwarzer, Guido, Hiam Chemaitelly, Laith J. Abu-Raddad, and Gerta\nRücker. 2019. “Seriously Misleading Results Using Inverse of\nFreeman-Tukey Double Arcsine Transformation in\nMeta-Analysis of Single Proportions.” Research Synthesis\nMethods 10 (3): 476–83. https://doi.org/10.1002/jrsm.1348.\n\n\nShamseer, Larissa, David Moher, Mike Clarke, Davina Ghersi, Alessandro\nLiberati, Mark Petticrew, Paul Shekelle, et al. 2015. “Preferred\nReporting Items for Systematic Review and Meta-Analysis Protocols\n(Prisma-p) 2015: Elaboration and Explanation.”\nBMJ (Online) 349 (January): 1–25. https://doi.org/10.1136/bmj.g7647.\n\n\nSim, Julius, and Chris C Wright. 2005. “The Kappa\nStatistic in Reliability Studies: Use, Interpretation, and Sample Size\nRequirements.” Physical Therapy 85 (3): 257–68. https://doi.org/10.1093/ptj/85.3.257.\n\n\nSterne, J A C, A J Sutton, J P Ioannidis, N Terrin, D R Jones, J Lau, J\nCarpenter, et al. 2011. “Recommendations for Examining and\nInterpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised\nControlled Trials.” BMJ (Clinical Research Ed.) 343:\nd4002. http://www.scopus.com/inward/record.url?eid=2-s2.0-79961238388&partnerID=40&md5=116d9b214a2fee3add89b254255f8cde.\n\n\nSterne, Jonathan A. C., Jelena Savović, Matthew J. Page, Roy G. Elbers,\nNatalie S. Blencowe, Isabelle Boutron, Christopher J. Cates, et al.\n2019. “RoB 2: A Revised Tool for\nAssessing Risk of Bias in Randomised Trials.” The BMJ\n366. https://doi.org/10.1136/bmj.l4898.\n\n\nTricco, A C, J Tetzlaff, and D Moher. 2011. “The Art and Science\nof Knowledge Synthesis.” Journal of Clinical\nEpidemiology 64 (1): 11–20. http://www.scopus.com/inward/record.url?eid=2-s2.0-78149388685&partnerID=40&md5=cff0dc293630269631887e1b25835cd7.\n\n\nVeroniki, Areti Angeliki, Dan Jackson, Wolfgang Viechtbauer, Ralf\nBender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian P. T. Higgins,\nDean Langan, and Georgia Salanti. 2016. “Methods to Estimate the\nBetween-Study Variance and Its Uncertainty in Meta-Analysis.”\nResearch Synthesis Methods 7 (1): 55–79. https://doi.org/10.1002/jrsm.1164.\n\n\nWilker, Elissa H., Marwa Osman, and Marc G. Weisskopf. 2023.\n“Ambient Air Pollution and Clinical Dementia: Systematic Review\nand Meta-Analysis.” BMJ 381 (April): e071620. https://doi.org/10.1136/bmj-2022-071620.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019.\n“Effectiveness of Food Handler Training and Education\nInterventions: A Systematic Review and Meta-Analysis.”\nJournal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review\nand Meta-Regression of the Knowledge, Practices, and Training of\nRestaurant and Food Service Personnel Toward Food Allergies and\nCeliac Disease.” Edited by Louise Emilsson. PLOS\nONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496."
  }
]