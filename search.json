[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OH8004 Knowledge Synthesis in Occupational and Public Health",
    "section": "",
    "text": "Overview\nThis is the course website for OH8004 Knowledge Synthesis in Occupational and Public Health in the School of Occupational and Public Health, Toronto Metropolitan University.\nThe course calendar description for OH8004 is as follows:\n\nThis course will cover structured methods of comprehensively synthesizing and reviewing research evidence. Detailed guidance will be provided on the design and conduct of systematic reviews and meta-analysis in occupational and public health. Students will be required to identify an individual occupational or public health review topic of key relevance to their thesis. They will prepare a detailed systematic review protocol on this topic of publishable quality, which will describe the rationale, hypothesis, and planned methods of their review.\n\nFor the most recent course outline and all announcements, please visit the D2L website for this course. This website will be used to review core content, examples, and exercises during in-class sessions.\nThis e-book was prepared using Quarto in RStudio (Posit). Additionally, the book uses WebR, implemented using quarto-webr, in certain chapters to implement data management and meta-analysis in your web browser with interactive coding. This book is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means you are free to share and adapt this material in any format or medium, as long as appropriate credit is given, a link to the license is provided, the material is used for non-commercial purposes, and any shared or adapted work is distributed under the same license.\n\n\n\nCC BY-NC-SA 4.0",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Systematic Review Team\nBefore embarking on a systematic review or other knowledge synthesis, consider the expertise you might require on your team:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_intro.html#systematic-review-team",
    "href": "1_intro.html#systematic-review-team",
    "title": "1  Introduction",
    "section": "",
    "text": "Knowledge of systematic review or knowledge synthesis methods\nA research librarian or information scientist to assist with the search strategy\nKnowledge of the topic area being investigated\nAny additional methodological expertise relevant to the topic or methods used (e.g., advanced meta-analysis, qualitative analysis)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_intro.html#systematic-review-protocol",
    "href": "1_intro.html#systematic-review-protocol",
    "title": "1  Introduction",
    "section": "1.2 Systematic Review Protocol",
    "text": "1.2 Systematic Review Protocol\nSystematic review protocols are critically important to ensure all review steps, assumptions, and procedures are justified and outlined in advance of conducting the review. This helps to reduce author biases (e.g., selective reporting) and arbitrary decision-making in the review process. Additionally, protocols are important to:\n\nHelp reviewers organize their review and anticipate potential problems\nHelp reduce duplication of efforts and (potentially) enhance collaboration\nImprove transparency and allow replication of the methods\n\nThere is a reporting standard for systematic review protocols, called PRIMSA-P. The reporting checklist is available form the PRISMA-P website as a PDF or Word file. See the published paper and the elaboration document for more information (Moher et al. 2015; Shamseer et al. 2015).\n\n\n\n\n\n\nImportant\n\n\n\nYour systematic review protocol final assignment must include all relevant PRISMA-P items. The checklist will be used as evaluation criteria.\n\n\nIt is recommended that protocols of reviews you intend to publish be registered at PROSPERO, an international database of protocols of systematic reviews. This is important reduce the duplication of synthesis efforts. PROSPERO is:\n\n“An international database of prospectively registered systematic reviews in health and social care, welfare, public health, education, crime, justice, and international development, where there is a health related outcome”.\n\nProtocols can also be published as journal articles in publications as such Systematic Reviews, BMJ Open, and PLoS ONE.\n\n\n\n\n\n\nPROSPERO Database\n\n\n\nThe PROSPERO database does not accept protocols for scoping reviews, only systematic reviews and rapid reviews.\n\n\n\n\n\n\n\n\nSystematic Review Protocol Example\n\n\n\nExamine the following systematic review protocol by Burns et et. (2022), as published in Systematic Reviews. They included the PRISMA-P checklist as a supplementary file.\nReview the checklist and compare to the relevant sections of the article. Was each criterion appropriately addressed?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_intro.html#key-knowledge-synthesis-resources",
    "href": "1_intro.html#key-knowledge-synthesis-resources",
    "title": "1  Introduction",
    "section": "1.3 Key Knowledge Synthesis Resources",
    "text": "1.3 Key Knowledge Synthesis Resources\nThe following organizations are important resources for knowledge synthesis research:\n\nThe Cochrane Collaboration publishes the Handbook of Systematic Reviews of Interventions, as well as systematic reviews of various healthcare and related topics in the Cochrane Library.\n\nJBI has published a Manual for Evidence Synthesis of different types of reviews and questions. Of particular note are critical appraisal tools for various types of study designs.\nThe McGill Library has compiled numerous resources and suggested articles related to the process of conducting a knowledge synthesis.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "1_intro.html#homework",
    "href": "1_intro.html#homework",
    "title": "1  Introduction",
    "section": "1.4 Homework",
    "text": "1.4 Homework\nFor next week, identify and obtain the PDF of one systematic review article published on an occupational or public health topic related to your thesis. Place the article in the shared folder on D2L. We will discuss each article next week in class, and in the following weeks.\n\n\n\n\nMoher, David, Larissa Shamseer, Mike Clarke, Davina Ghersi, Alessandro Liberati, Mark Petticrew, Paul Shekelle, and Lesley A Stewart. 2015. “Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols (PRISMA-P) 2015 Statement.” Systematic Reviews 4 (1): 1. https://doi.org/10.1186/2046-4053-4-1.\n\n\nShamseer, Larissa, David Moher, Mike Clarke, Davina Ghersi, Alessandro Liberati, Mark Petticrew, Paul Shekelle, et al. 2015. “Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols (Prisma-p) 2015: Elaboration and Explanation.” BMJ (Online) 349 (January): 1–25. https://doi.org/10.1136/bmj.g7647.\n\n\nTricco, A C, J Tetzlaff, and D Moher. 2011. “The Art and Science of Knowledge Synthesis.” Journal of Clinical Epidemiology 64 (1): 11–20. http://www.scopus.com/inward/record.url?eid=2-s2.0-78149388685&partnerID=40&md5=cff0dc293630269631887e1b25835cd7.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "2_question.html",
    "href": "2_question.html",
    "title": "2  Review Question and Eligibility Criteria",
    "section": "",
    "text": "2.1 Review Question\nIn defining a question, different approaches are used depending on the type of question being considered (e.g., intervention, exposure-outcome relationship, prevalence or incidence of an outcome). Types of questions can be categorized as follows (Munn et al. 2018).\nFor the purposes of this course, we will mainly focus on intervention reviews, which are the most common, but will address aspects of other reviews, which are common in occupational and public health research. These scenarios are further defined below. For more details on the other types of reviews, see Munn et al. (2018) and the JBI Manual for Evidence Synthesis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review Question and Eligibility Criteria</span>"
    ]
  },
  {
    "objectID": "2_question.html#review-question",
    "href": "2_question.html#review-question",
    "title": "2  Review Question and Eligibility Criteria",
    "section": "",
    "text": "Intervention (effectiveness) reviews\nEtiology or risk factor reviews\nPrevalence or incidence reviews\nDiagnostic test accuracy reviews\nQualitative (experiential) reviews\n“Umbrella” reviews (reviews of reviews)\nOthers (e.g., economic evaluations, policy reviews)\n\n\n\n2.1.1 Intervention Reviews\nSystematic reviews of interventions should have questions that follow the PICO framework. PICO refers to the Population, Intervention, Comparison, and Outcome. Sometimes an S is added for Study Design.\nPopulation includes the study participants and settings of interests. This could include populations with specific socio-demographic characteristics (e.g., age groups, sex, ethnicity), different occupational or other groups, or specific settings (e.g., community-based, institutions).\nIntervention includes the specific process, technique, policy, or other change being evaluated in the populations of interest. It is important to define exactly what is being delivered (e.g., materials, procedures), by whom and to who, how, where, when and how much. The TIDieR Checklist is a useful framework to consider these factors and which types of interventions might be relevant. Interventions can then be grouped or sub-grouped into specific labels based on their characteristics, if appropriate, with separate analyses conducted on each group. E.g., a review of falls prevention interventions might have groups such as exercise, medication, supportive environments.\nComparisons includes the control groups being considered. This could include placebo groups, those receiving no intervention or a current “standard” or “typical” procedure, or groups receiving an alternative or competing intervention.\nOutcome includes the set of measures that will determine intervention effectiveness. You should select a limited number of the most important outcomes relevant to your topic, and avoid extracting data on all possible outcomes. The Cochrane Collaboration recommends no more than seven critical outcomes for their review summary documents. Similar to interventions, outcomes should be grouped into similar categories or domains (e.g., knowledge, behaviours, biological); this can include hierarchical groupings if appropriate. Consider what time points of measurement will be used, and measurement methods (e.g., lab-based, self-reported, objective). It can be useful to consider logic models to map out the possible outcomes and their relationships when deciding what is relevant to the review.\nStudy designs usually focus on RCTs for intervention reviews, though non-randomized trials, and sometimes observational studies, can also be included. For occupational and public health questions (compared to clinical questions), RCTs may be limited or not feasible, so it is often beneficial to include non-randomized studies.\n\n\n\n\n\n\nPICO Question Exercise\n\n\n\nConsider the following intervention review questions. Identify the PICO elements of each one, and discuss how the questions could be made broader (more inclusive) or more targeted (restrictive).\n\nWhat is the efficacy of different training and education interventions to improve the food safety knowledge, attitudes, and behaviours of food handlers working at retail and food service? (Young et al. 2019)\n\n\nWhat is the effect of workplace-delivered mindfulness-based interventions for non-clinical working populations? (Bartlett et al. 2019) https://www.crd.york.ac.uk/prospero/display_record.php?RecordID=36650\n\n\n\n\n\n2.1.2 Etiology/Risk Factor Reviews\nThese reviews assess the association between an exposure of interest and outcomes of interest. Review questions of this type can be defined using the PECO framework, which is similar to PICO but replaces the intervention with the exposure of interest. These reviews rely heavily on observational studies (e.g., cohort, case-control), so there are additional concerns related to possible biases that need to be considered at the risk of bias stage.\nExposures could include a risk factor or other determinant of a health outcome. The time period or length of time of the exposure should also be specified if relevant.\n\n\n2.1.3 Prevalence/Incidence Reviews\nThese reviews assess the health or disease burden in a specific population, and are useful to describe the distribution of a health outcome in different locations (e.g., countries) and population groups. This type of review is used in global burden of disease studies. Questions of this type can be expressed using the CoCoPop framework: Condition, Context, and Population (Munn et al. 2015). The population consideration is similar to what is mentioned above for interventions.\nCondition refers to the variable of interest, and could be a disease, other health outcome, symptom, or other factor (e.g., knowledge, behaviour).\nContext refers to the specific setting (e.g., seasons, communities) or geographical areas of interest to the review.\n\n\n2.1.4 Qualitative Reviews\nThese reviews aim to investigate the human experiences or meaningfulness of an issue or phenomenon. Qualitative review questions can be expressed using the PICo framework: Population, Phenomena of Interest, and Context (Munn et al. 2018). The population consideration is similar to other question frameworks above. Note that there is no outcome explicitly considered in these types of reviews.\nPhenomenon of interest refers to the experience or issue of investigation (e.g., participants’ views about an intervention, or determinants of their health-related behaviours).\nContext, similar to prevalence reviews, depends on the review objectives, but may include geographical, cultural, or other sociodemographic areas of focus for the review.\n\n\n\n\n\n\nOther Questions Exercise\n\n\n\nConsider the following review questions. Identify the elements of each one, and discuss how the questions could be made broader (more inclusive) or more targeted (restrictive).\n\nWhat is the evidence on the association between long-term exposure to residential green spaces and mortality in adults? (Rojas-Rueda et al. 2019)\n\n\nWhat is the global prevalence and incidence of tinnitus? (Jarach et al. 2022)\n\n\nWhat is the global prevalence of occupational morbidity in migrant workers, and what are the occupational health risks and outcomes associated with specific industries? (Hargreaves et al. 2019)\n\n\nHow are paramedics’, ambulance officers’, ambulance volunteers’, and call-takers’ mental health and well-being effected by workflow, the nature of work, and their changing roles? (Lawn et al. 2020)\n\n\nWhat is the evidence for the association between (1) silicosis and pulmonary tuberculosis and (2) silica exposure and pulmonary tuberculosis, excluding or controlling for radiological silicosis (Ehrlich et al. 2021)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review Question and Eligibility Criteria</span>"
    ]
  },
  {
    "objectID": "2_question.html#additional-eligibility-criteria",
    "href": "2_question.html#additional-eligibility-criteria",
    "title": "2  Review Question and Eligibility Criteria",
    "section": "2.2 Additional Eligibility Criteria",
    "text": "2.2 Additional Eligibility Criteria\n\n2.2.1 Document Type\nOne of the first considerations in deciding which types of studies will be eligible for inclusion in the review is the document source. Most of your relevant articles will be peer-reviewed journal articles. Other documents to consider include:\n\nConference papers or abstracts: care must be taken to avoid double-counting studies that are subsequently published as journal articles. Also, information in an abstract is often very limited.\nDissertations and theses: may be useful if the research was not published. Research has shown that disserations are often not published. But the research may not have been peer-reviewed (especially master’s theses) beyond the advisory committee.\nResearch reports: unpublished reports, such as those published by government and non-profit agencies, could be relevant depending on the review topic.\n\n\n\n\n\n\n\nUnit of analysis\n\n\n\nIn systematic reviews, the unit of analysis is individual studies, which may be reported in multiple references or reports. These should be identified and linked together, usually easiest at the full-text assessment stages. They can be identified by noting similar author names, locations/settings, study details, and results.\n\n\n\n\n2.2.2 Language\nLanguage of articles is another important consideration. Limiting inclusion to only articles in English can lead to a language bias. While practical considerations may limit ability to review in additional languages, it is important to at least capture such articles in the search strategy (if possible) and report on the number of articles excluded due to language. The role of language and the impact of excluding non-English articles likely depends on the specific topic being investigated.\nTools such as Google Translate have been found to be quite reliable for data extraction in systematic reviews. These translation capabilities and other tools (e.g., large language AI models) are likely to continue to improve in the future.\n\n\n\n\n\n\nReview Question and Eligibility Criteria Exercise\n\n\n\nUsing the article you identified for your homework exercise from last week, determine:\n\nThe type of review conducted (using the categorization system described above)\nThe review question\nThe specific elements of the question (e.g., population, intervention/exposure)\nOther eligibility criteria (e.g., document type, language)\n\n\n\n\n\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees. 2021. “The Association Between Silica Exposure, Silicosis and Tuberculosis: A Systematic Review and Meta-Analysis.” BMC Public Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nHargreaves, Sally, Kieran Rustage, Laura B Nellums, Alys McAlpine, Nicola Pocock, Delan Devakumar, Robert W Aldridge, et al. 2019. “Occupational Health Outcomes Among International Migrant Workers: A Systematic Review and Meta-Analysis.” The Lancet Global Health 7 (7): e872–82. https://doi.org/10.1016/S2214-109X(19)30204-9.\n\n\nJarach, Carlotta M., Alessandra Lugo, Marco Scala, Piet A. van den Brandt, Christopher R. Cederroth, Anna Odone, Werner Garavello, Winfried Schlee, Berthold Langguth, and Silvano Gallus. 2022. “Global Prevalence and Incidence of Tinnitus: A Systematic Review and Meta-analysis.” JAMA Neurology 79 (9): 888–900. https://doi.org/10.1001/jamaneurol.2022.2189.\n\n\nLawn, Sharon, Louise Roberts, Eileen Willis, Leah Couzner, Leila Mohammadi, and Elizabeth Goble. 2020. “The Effects of Emergency Medical Service Work on the Psychological, Physical, and Social Well-Being of Ambulance Personnel: A Systematic Review of Qualitative Research.” BMC Psychiatry 20 (1): 348. https://doi.org/10.1186/s12888-020-02752-4.\n\n\nMunn, Zachary, Sandeep Moola, Karolina Lisy, Dagmara Riitano, and Catalin Tufanaru. 2015. “Methodological Guidance for Systematic Reviews of Observational Epidemiological Studies Reporting Prevalence and Cumulative Incidence Data.” International Journal of Evidence-Based Healthcare 13 (3): 147–53. https://doi.org/10.1097/XEB.0000000000000054.\n\n\nMunn, Zachary, Cindy Stern, Edoardo Aromataris, Craig Lockwood, and Zoe Jordan. 2018. “What Kind of Systematic Review Should I Conduct? A Proposed Typology and Guidance for Systematic Reviewers in the Medical and Health Sciences.” BMC Medical Research Methodology 18 (1): 5. https://doi.org/10.1186/s12874-017-0468-4.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Review Question and Eligibility Criteria</span>"
    ]
  },
  {
    "objectID": "3_searching.html",
    "href": "3_searching.html",
    "title": "3  Search Strategy",
    "section": "",
    "text": "3.1 Peer-Reviewed Literature\nThe most important place to search for potentially relevant studies for a systematic review is bibliographic databases. The list of databases accessible through the TMU library is avialable here. The selection of which and how many databases to search should be conducted in consultation with a librarian. Some common options include:\nIn addition to the above, subject-specific and specialized databases may be useful to search depending on the topic. Examples of these include: PsycINFO for psychological literature, CINAHL for nursing and allied health literature, OSH References Collection for occupational health and safety literature, and Food Science and Technology Abstracts for food safety and nutrition literature. There are no specific rules for the number of databases to include, as this will depend on the review question, topic, and other factors, but typically reviews might search 3-7 different databases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#peer-reviewed-literature",
    "href": "3_searching.html#peer-reviewed-literature",
    "title": "3  Search Strategy",
    "section": "",
    "text": "MEDLINE: is publicly accessible through the PubMed interface, or the OVID interface via the TMU library. It contains &gt;35 million references for biomedical (including occupational and public health) literature.\nEmbase: is a biomedical database with &gt;30 million references, hosted by Elsevier.\nScopus: is a multi-disciplinary database that covers multiple research fields, including all journals indexed in MEDLINE. It covers &gt;14,000 scholarly sources.\nWeb of Science: is another multi-disciplinary database containing references from &gt;8500 international research journals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#search-algorithm",
    "href": "3_searching.html#search-algorithm",
    "title": "3  Search Strategy",
    "section": "3.2 Search Algorithm",
    "text": "3.2 Search Algorithm\nA search algorithm needs to be developed based on the review question, and implemented in each bibliographic database. The algorithm should be developed and tested in one of the databases before being adapted to the other databases. Developing a search algorithm for a systematic review is very complex and librarian assistance is strongly recommended.\nPrior to developing a search algorithm, it is useful to identify some seed articles (i.e., examples of relevant articles). These are articles that you would expect to identify and include as relevant in your review based on the review question and eligibility criteria. Specifically, the title, abstract, and keywords of these articles should be reviewed to identify keywords that can be used to build the search algorithm. The keywords can also be used to help develop controlled vocabulary terms (see below for more details). Once the algorithm is fully developed, it should be tested in a database to ensure that it captures each of the seed articles.\nThe search algorithm should aim to balance sensitivity (i.e, the proportion of relevant studies identified from all relevant studies) with precision (i.e, the proportion of relevant studies identified from all studies/references identified). Increasing the sensitivity of the search will reduce its precision, and lead to a need to review more references.\nThe search algorithm will typically include combinations of search terms for each category of the question. For example, an intervention review would have search terms related to the population, the intervention, and the outcome, with additional potential terms for the topic or setting or other criteria. Within each category, terms are combined in parentheses (or through multiple individual searches) with the OR Boolean operator, and across categories with the AND operator. A NOT operator can also be used to exclude specific terms, but should be used with caution. To search for an exact phrase, it should be placed within quotation marks (e.g., “food safety”, “occupational health”).\n\n\n\n\n\n\nExample Search Algorithms\n\n\n\nAn example search algorithm from Young et al. (2019) is shown below:\n\nAnother example is shown below for an occupational health risk factor review (Ehrlich et al. 2021), as implemented in PubMed:\n\n\n\nIn addition to the Boolean operators above, you can also consider proximity operators, which will search for a word within a certain number of places of another word (e.g., food adj3 safety in the OVID platform will search for those terms within three places of other).\nMany databases have controlled vocabulary (i.e., standardized subject terms) that should also be searched as part of the search strategy in addition to text words (i.e., keywords). In MEDLINE, these are called MeSH terms. Finally, specific limits can be placed on certain search fields, such as publication dates or languages, if desired. The approach to implement such limits varies by database. An example of implementing a complex search in MEDLINE accessed through OVID is available here.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#mehs-headings-examples",
    "href": "3_searching.html#mehs-headings-examples",
    "title": "3  Search Strategy",
    "section": "3.3 MeHS Headings Examples",
    "text": "3.3 MeHS Headings Examples\nYou can see all MeSH terms related to public health in the following tree branch.\nIf using the PubMed interface, you can include a MeSH term in your search by adding ‘MeSH’ in square brackets beside the term (e.g., “Public Health”[Mesh]).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#saving-search-results-and-de-duplication",
    "href": "3_searching.html#saving-search-results-and-de-duplication",
    "title": "3  Search Strategy",
    "section": "3.4 Saving Search Results and De-duplication",
    "text": "3.4 Saving Search Results and De-duplication\nOnce the searches are implemented, the records can be saved as RIS (or other) files and imported into a reference management program, systematic review management program, or de-duplication tool for de-duplication. Some databases have restrictions on how many references can be exported at once, in which case the results will need to be exported in batches. All results should be documented, including saving the exact algorithm used in each database, search dates, and number of hits in each database.\nSome systematic review management programs (see below) will automatically de-duplicate references (e.g., Covidence). Reference management programs (e.g., Zotero, Mendeley) can also be used for de-duplication, with the de-duplicated file exported and imported into the systematic review management program of choice. Finally, there is an online Systematic Review De-duplicator Tool that can be used for this purpose.\n\n\n\n\n\n\nSearch Exercise\n\n\n\nUsing Scopus via the TMU Library, conduct an “Advanced document search” by reproducing the following algorithm from Young et al. (2019).\nTITLE-ABS-KEY((\"food safety\" OR \"food-borne\" OR foodborne OR \"food poisoning\" OR “food hygiene” OR “safe food” OR HACCP) AND (handler* OR worker* OR employee* OR manager* OR chef* OR cater* OR vendor* OR staff OR service OR business* OR restaurant* OR premise* OR retail) AND (behaviour* OR behavior* OR practice* OR attitude* OR knowledge OR belief* OR perceive* OR perception OR understanding OR violation*) AND (intervention OR train* OR educat* OR course))\n\nHow many hits did you get?\nTry setting a publication date restriction to exclude publications from 2018 to present to approximately reproduce the original search (which was conducted in January, 2018). Now how many hits to do you get?\nNow try saving the results as an RIS file. When exporting, make sure you select the abstract and keywords fields to include in your exported file. Try importing it into a reference management program such as Zotero or Mendeley.\nFinally, how might you translate this search algorithm into MEDLINE (OVID)? You can access MEDLINE through the TMU OVID account. What MeSH terms might you use for this algorithm?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#grey-literature",
    "href": "3_searching.html#grey-literature",
    "title": "3  Search Strategy",
    "section": "3.5 Grey Literature",
    "text": "3.5 Grey Literature\nIt may be useful to search for grey literature as part of your search strategy. This may help to identify studies that are not published in peer-reviewed journal articles, or those not indexed in major bibliographic databases. Some grey literature search options are noted below:\n\nGoogle Scholar: will specifically search for scholarly publications, most of which should be captured by your bibliographic database search, but could uncover additional studies.\nOpenGrey: is another online database based in Europe that contains reports, dissertations, and other types of research documents.\nTargeted websites of organizations: you can search through the websites of specific organizations (e.g., governments, non-profit groups) that are known to publish research reports or other documents that might be relevant. This can also include targeted searching of the conference proceedings from prior relevant conferences (if not already indexed elsewhere).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#search-verification-and-peer-review",
    "href": "3_searching.html#search-verification-and-peer-review",
    "title": "3  Search Strategy",
    "section": "3.6 Search Verification and Peer Review",
    "text": "3.6 Search Verification and Peer Review\nIt is important to verify that your search did not miss any important studies. One common method used for verification is to search the reference lists of all of your relevant articles (this can be done after you have reached the data extraction stage). You can also search the reference lists of other literature review articles conducted on the same topic as your review.\nIt is increasingly recommended to consider having your search strategy peer reviewed for accuracy before implementation. The Peer Review of Electronic Search Strategies (PRESS) Statement can be used as a checklist for this purpose.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#review-management",
    "href": "3_searching.html#review-management",
    "title": "3  Search Strategy",
    "section": "3.7 Review Management",
    "text": "3.7 Review Management\nTo facilitate the management of the relevance screening and data extraction steps, it is highly recommended to use a systematic review management program. Some commonly used programs include:\n\nCovidence\nDistillerSR\nRayyan\nHubMeta\nCADIMA\n\nMost of these programs require a subscription to use, though some may offer free trials or discounts for students (e.g., Covidence). HubMeta and CADIMA are currently free to use. Other non-subscription based options could include spreadsheets or online forms (e.g., Google Forms), but these would be challenging to use for larger projects and could be prone to errors.\nAn example of a Covidence project and interface is shown below. TMU currently does not offer an institutional subscription to Covidence, though this may be provided in future years. Covidence does provide a free trial for students, which allows screening of up to 500 references and up to two reviewers per project. As such, it will be used for your course assignment.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "3_searching.html#homework",
    "href": "3_searching.html#homework",
    "title": "3  Search Strategy",
    "section": "3.8 Homework",
    "text": "3.8 Homework\nFor next week, and to help with preparing your second assignment, identify 1-2 different seed articles related to your proposed systematic review topic. Use these seed articles, along with your review question, to identify some keywords and keyword categories required to start building your search algorithm. We will discuss your findings at the start of next week’s class.\n\n\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees. 2021. “The Association Between Silica Exposure, Silicosis and Tuberculosis: A Systematic Review and Meta-Analysis.” BMC Public Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Search Strategy</span>"
    ]
  },
  {
    "objectID": "4_selecting.html",
    "href": "4_selecting.html",
    "title": "4  Selecting Studies",
    "section": "",
    "text": "4.1 Relevance Screening\nA relevance screening form should be developed, pilot tested, and included in the review protocol. It is easiest and most straightforward for this to contain only one key question if possible to streamline the process. The form should explicitly identify the eligibility criteria and any key definitions. An example from Young et al. (2019) is shown below:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selecting Studies</span>"
    ]
  },
  {
    "objectID": "4_selecting.html#relevance-screening",
    "href": "4_selecting.html#relevance-screening",
    "title": "4  Selecting Studies",
    "section": "",
    "text": "Relevance Screening Form Example\n\n\n\n4.1.1 Pilot Testing\nThe relevance screening form should be pilot tested before use to ensure the eligibility and inclusion decisions are valid and reliable. The recommended process is as follows:\n\nPurposively select a small number of abstracts for pre-testing (e.g, 30-50) to ensure the sample includes those known to be relevant, not relevant, and potentially relevant\nEach reviewer assesses each abstract using the form and records their results\nAssess agreement in the pilot test using the kappa statistic\nA kappa of &gt;0.8 is often recommended for “almost perfect agreement” (e.g, Sim and Wright 2005)\n\n\n\n\n\n\n\nKappa Agreement Example\n\n\n\nWe will simulate reviewer inclusion/exclusion (i.e., yes/no) ratings for a sample of 30 abstracts. Assume that Reviewer 1 included 50% of the abstracts, including two that Reviewer 2 did not include. Assume Reviewer 2 included 16 abstracts, including three of the abstracts that Reviewer 1 did not include.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow we can evaluate the kappa agreement between the two reviewer’s ratings.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe agreement is moderate, and below an ideal level of 0.8 or higher.\nWhat happens if we edit the simulated data so that this time each reviewer only differs by one abstract each (i.e., there are only two conflicts)?\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nYou can further modify the simulated dataset using the code above to explore how different patterns of conflicts affect the Kappa measure of agreement.\n\n\n\n\n4.1.2 Systematic Review Management Software\nSystematic management programs, such as Covidence, automatically calculate a kappa agreement statistic, along with other inter-rater reliability measures. The relevance screening form in Covidence defaults to having three options: yes, no, or maybe. Answers of maybe are considered the same as yes for screening and inter-rated reliability calculation purposes. For Covidence, a CSV file can be downloaded to show the screening reliability statistics for each pair of reviewers. An example of this output is shown in the figure below.\n\n\n\n4.1.3 Machine Learning for Relevance Screening\nMany systematic management programs have implemented or are developing machine learning tools to semi-automate the relevance screening process. The tools work by prioritizing the abstracts that are most likely to be included (screening prioritization), and suggesting a percentage that can be “safely” excluded (screening truncation), which can both enhance review efficiency. The screening truncation process usually uses a stopping rule or threshold, such as 95% estimated recall, after which point the review team can decide not to review the remaining references or to use a modified process (e.g., only one reviewer).\nThese tools requires careful training and pilot testing on a subset of abstracts (e.g., 2% of total abstracts) to ensure the algorithm can make more accurate predictions. Research has shown that such machine learning tools can perform with high accuracy and reliability, and can result in substantial time savings for review teams (Hamel et al. 2020; Tsou et al. 2020). Recent guidance is available for reviewers who are interested to adopt these tools in their review (Hamel et al. 2021).\n\n\n\n\n\n\nRelevance Screening Exercise\n\n\n\nUsing the example relevance screening form above, from Young et al. (2019), assess the relevance of the following five abstracts captured in the search from that review and record your answers for discussion.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selecting Studies</span>"
    ]
  },
  {
    "objectID": "4_selecting.html#article-procurement",
    "href": "4_selecting.html#article-procurement",
    "title": "4  Selecting Studies",
    "section": "4.2 Article Procurement",
    "text": "4.2 Article Procurement\nFollowing relevance screening, full-text articles (e.g., PDF files) should be obtained for all references considered relevant. These can be saved to a folder and uploaded/linked to the applicable reference IDs in the systematic review management program used.\n\n\n\n\n\n\nFinding Articles\n\n\n\nYou should be able to identify and access most articles using a combination of simple Google Searching (for open-access papers) and the TMU scholarly paper search feature.\nOccasionally, you may need to search for TMU access to the specific journal of interest, then search for the paper in specific databases if the global search does not find the paper(s) you need.\nIn cases where you cannot find the paper via these methods, you can ask the librarian for assistance, or make a special request for the paper through the Interlibrary Loan Service.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selecting Studies</span>"
    ]
  },
  {
    "objectID": "4_selecting.html#article-characterization",
    "href": "4_selecting.html#article-characterization",
    "title": "4  Selecting Studies",
    "section": "4.3 Article Characterization",
    "text": "4.3 Article Characterization\nIt is often useful to include a separate screening confirmation (sometimes referred to as second-level screening) step and an article characterization step (i.e., data charting). The relevance confirmation step can be used to exclude irrelevant papers, which can be combined with article characterization to subsequently extract key characteristics of the relevant articles. This step is sometimes combined with data extraction, where study outcomes are also extracted. The confirmation of relevance should include a checkbox to indicate the primary reason for exclusion for articles that are not relevant.\nThe data captured in this form should be summarized in a characteristics of included studies table in the review results. Additionally, some of the variables captured in this form may also be used in the analysis (e.g., subgroup analysis or meta-regression). Some frequently important variables to capture include:\n\nDocument type, year of publication, language\nStudy design\nStudy dates and location(s)\nRecruitment and sampling methods\nCharacteristics of participants (e.g., age, gender)\nCharacteristics of the intervention or exposure (e.g., dose, length, how they were measured)\nCharacteristics of the control or comparison group (if applicable)\nTypes of outcomes measured and how they were measured\n\nIt is often useful to develop outlines of the anticipated figures and tables that you plan to include in the results section of your systematic review. This will help you to decide on which important characteristics to extract.\n\n\n\n\n\n\nMultiple reports of the same study\n\n\n\nIt is sometimes the case that the same study is reported in multiple publications. In the case of authors splitting results of a study across multiple journal papers, it might be easiest to treat these as separate studies at this stage and note that they are from the same larger study. In cases where preliminary data are published in a thesis or abstract, usually you will want to keep only the published journal article as the main source for that study.\n\n\n\n4.3.1 Pilot Testing\nIt is also important to pilot test the article characterization form. Usually this can be done on a small selection of articles (e.g., 5-10). Instead of checking kappa agreement, you should compare reviewer answers, look for discrepancies in interpretation of questions, discuss, and modify the form to enhance its clarify and ensure consistent interpretation.\n\n\n\n\n\n\nArticle Characterization Exercise\n\n\n\nExamine the example article characterization form from Young et al. (2019). Use this form to extract data on a few different characteristics from the three full-text articles captured and screened as potentially relevant in that review. Then record your answers for discussion.\nThe three articles are available at the following shared folder: https://drive.google.com/drive/folders/1mvBj074odovMpWIQ-UQ9BJprNzRiB9cv?usp=sharing.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selecting Studies</span>"
    ]
  },
  {
    "objectID": "4_selecting.html#homework",
    "href": "4_selecting.html#homework",
    "title": "4  Selecting Studies",
    "section": "4.4 Homework",
    "text": "4.4 Homework\nOne to two students each will be assigned one risk-of-bias domain from Chapter 5 to review and present in class next week. The bias domains to be assigned include:\n\nBias arising from the randomization process\nBias due to deviations from intended interventions\nBias due to missing outcome data\nBias in measurement of the outcome\nBias in selection of the reported result\nRisk of bias due to confounding\nRisk of bias arising from measurement of the exposure\nRisk of bias in selection of participants into the study\nRisk of bias due to post-exposure interventions\n\nYou can read in detail about the first five domains in the Cochrane RCT risk-of-bias tool guidance document. Information on the other four domains can be found in the ROBINS-E tool for cohort studies.\n\n\n\n\nHamel, C., M. Hersi, S. E. Kelly, A. C. Tricco, Sharon Straus, G. Wells, B. Pham, and B. Hutton. 2021. “Guidance for Using Artificial Intelligence for Title and Abstract Screening While Conducting Knowledge Syntheses.” BMC Medical Research Methodology 21 (1): 285. https://doi.org/10.1186/s12874-021-01451-2.\n\n\nHamel, C., S. E. Kelly, K. Thavorn, D. B. Rice, G. A. Wells, and B. Hutton. 2020. “An Evaluation of DistillerSR’s Machine Learning-Based Prioritization Tool for Title/Abstract Screening – Impact on Reviewer-Relevant Outcomes.” BMC Medical Research Methodology 20 (1): 256. https://doi.org/10.1186/s12874-020-01129-1.\n\n\nSim, Julius, and Chris C Wright. 2005. “The Kappa Statistic in Reliability Studies: Use, Interpretation, and Sample Size Requirements.” Physical Therapy 85 (3): 257–68. https://doi.org/10.1093/ptj/85.3.257.\n\n\nTsou, Amy Y., Jonathan R. Treadwell, Eileen Erinoff, and Karen Schoelles. 2020. “Machine Learning for Screening Prioritization in Systematic Reviews: Comparative Performance of Abstrackr and EPPI-Reviewer.” Systematic Reviews 9 (1): 73. https://doi.org/10.1186/s13643-020-01324-7.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Selecting Studies</span>"
    ]
  },
  {
    "objectID": "5_bias.html",
    "href": "5_bias.html",
    "title": "5  Risk-of-Bias Assessment",
    "section": "",
    "text": "5.1 Risk of bias\nThere is strong evidence that certain study design features (e.g., lack of blinding in clinical studies, lack of allocation concealment) can lead to a higher risk of bias, which can affect the magnitude and direction of the results. Biases can be introduced into a study through different mechanisms. These can be classified into different domains, which form the basis for risk-of-bias assessment tools. Risk of bias should be assessment differently for each unique outcome in a review, because some domains (e.g., blinding) may be important for some outcomes (e.g., subjective measures) but less important for others (e.g., death).\nRisk-of-bias assessments should use recommended, and ideally validated tools, and should be conducted by two independent reviewers, especially as judgement is frequently required in these assessments. Pilot testing of risk-of-bias assessment tools is important for all reviewers involved in a review (e.g., on 3-5 papers) to ensure criteria are applied consistently.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-of-Bias Assessment</span>"
    ]
  },
  {
    "objectID": "5_bias.html#cochrane-tool-for-rcts",
    "href": "5_bias.html#cochrane-tool-for-rcts",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.2 Cochrane Tool for RCTs",
    "text": "5.2 Cochrane Tool for RCTs\nThe Cochrane Collaboration has developed a structured, domain-based tool to assess risk of bias in RCTs (Sterne et al. 2019). The tool, called RoB 2, uses a series of signalling questions to determine the risk of bias within each domain. Questions are answered with options of “yes”, “probably yes”, “probably no”, “no”, and “no information”. There are also variations of the tool available for cluster-randomized trials and crossover trials. As noted above, the tool should be applied at the outcome level, so multiple assessments may be needed for a study if it reports multiple relevant outcomes.\nBased on the results of the signalling questions, each domain will then receive a risk-of-bias judgement (or rating) of low, some concerns, or high. The tool includes flow charts to guide which judgement is most appropriate given the responses to the signalling questions. Each signalling question also contains a free-text box to record supporting information about the answer. Reviewers can copy and paste direct quotes from the article in this section to support their answers and add transparency. Optionally, reviewers can make a judgement about the likely direction of bias for each domain and overall, if this is possible to assess.\nOnce the risk of bias of each domain is assessed, an overall risk-of-bias judgement is made for the study-specific outcome. The overall risk judgement cannot be lower than the judgement assigned to any individual domain (e.g., if one domain is judged as “some concerns” and all others are “low risk”, then the overall risk must be at least “some concerns”).\nOne of the first decisions in applying this tool is determine whether you are interested in the effect of assignment to the interventions at baseline (i.e., intention-to-treat effect), or the effect of adhering to the interventions as specified in the protocol (i.e., per-protocol effect). The former is usually more important for occupational and public health reviews, as it reflects population-level realities of intervention administration.\n\n5.2.1 RCT Bias Domains\nBias arising from the randomization process\nThis domain assesses whether intervention allocation process was random, and whether it was adequately concealed. Proper randomization is critical to mitigate the possibility of confounding bias. Randomization can be conducted in multiple different ways (e.g., simple, blocked, stratified, minimization). It is also important to prevent participants or trial personnel from knowing the next assignment (e.g., intervention or control), as this can enable selective enrollment. Baseline differences between intervention groups can be examined, and if identified (beyond what is expected by chance), suggest a problem with the randomization process.\nBias due to deviations from intended interventions\nThis domain assesses whether there were any important deviations from the intended interventions that could bias results. For example, if participants or trial personnel were not “blinded” to the intervention group status, this could affect certain outcomes (e.g., behaviours). If blinding was not conducted or not possible, the possibility that deviations arose because of the trial context should be considered. If so, whether these deviations could have affected the outcome should also be considered, including whether any deviations were balanced across groups. This domain also considers whether the analysis correctly followed intention-to-treat principles (assuming the interest is in the effect of assignment to interventions).\nBias due to missing outcome data\nThis domain assesses whether outcome data were available for all, or nearly all, of the trial participants. For continuous outcomes, data from at least 95% of participants will often be sufficient to mitigate missing data biases. For dichotomous outcomes, the proportion of acceptable missing data depends on the risk of the event. If missing data are a possible concern, we should look for evidence in the study that the result was not biased due to the missing data (e.g., bias-corrected analysis or sensitivity analysis). We can use additional information in the study to determine whether missing data could or likely depended on the true value of the outcome, including any differences between groups in missing data, reported reasons for missing data, and circumstances of the trial.\nBias in measurement of the outcome\nThis domain assesses the appropriateness of the method of measuring the outcome, whether the measurement method differed across intervention groups, and whether outcome assessors were blinded to the intervention group status of participants. It aims to determine if there were any differential measurement or misclassification errors. If outcome assessors were not blinded to group status, then it should be considered whether this knowledge could have influenced their assessment.\nBias in selection of the reported result\nThis domain assesses whether the result reported in the study is likely to have been selected from multiple outcome measurements or multiple analyses (e.g., due to being statistically significant). Assessors should consider whether the trial was analyzed in accordance with a pre-specific analysis plan (e.g., published protocol). Evidence of selecting reporting should be examined, including reporting only one or a subset of multiple instruments, scales, time points, or analyses.\n\n\n\n\n\n\nRCT Risk-of-Bias Example\n\n\n\nAn example of the outcome-specific risk-of-bias assessment results for two studies from a systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). In this review, separate assessments were made for objective (e.g., blood pressure) and subjective (e.g., self-reported anxiety) outcomes.\n\n\n\nExample 1: Liao 2013\n\n\n\n\n\nExample 2: Chen 2013\n\n\n\n\n\n\n\n\n\n\nRCT Risk-of-Bias Exercise\n\n\n\nConsider the Young et al. (2019) systematic review about the effectiveness of food handler training interventions. Using two example RCT articles from that review, conduct a risk-of-bias assessment using the Cochrane RoB tool with the templates provided. For the Mancini et al. article, the outcome to assess is food inspection scores, while for Richard et al., the relevant outcome is food safety knowledge.\n\nWhat is your risk-of-bias judgement for each domain?\nWhat is your overall risk-of-bias judgement for each study?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-of-Bias Assessment</span>"
    ]
  },
  {
    "objectID": "5_bias.html#non-randomized-and-observational-study-designs",
    "href": "5_bias.html#non-randomized-and-observational-study-designs",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.3 Non-Randomized and Observational Study Designs",
    "text": "5.3 Non-Randomized and Observational Study Designs\nNon-randomized studies may be important to include in reviews of the effects of interventions. For example, population-level interventions (e.g., legislation changes) and investigations of long-term or rare outcomes are often more feasible to investigate in non-randomized studies than RCTs. Non-randomized intervention studies often have at a higher risk of bias than RCTs due to the lack of randomization and other study features. The Cochrane Collaboration has developed a special risk-of-bias tool called ROBINS-I to evaluate risks of bias in non-randomized studies.\nObservational studies are critical to answer questions about exposures and health outcomes. For these types of questions, RCTs are often not feasible or ethical to conduct, and they suffer other limitations (e.g., shorter follow-up times). An international collaboration has developed a risk-of-bias assessment tool for observational studies in exposure/risk factor reviews, called ROBINS-E. ROBINS-E is based on the same framework and approach as the Cochrane RoB tool and ROBINS-I. We will consider how to apply this tool below. However, it is designed for cohort studies, so for reviews that focus on other designs (e.g.., case-control studies), other tools may be needed.\n\n5.3.1 ROBINS-E Tool\nOne of the first steps in using the ROBINS-E tool is to pre-specify important confounding factors that are likely to influence the association between the exposure and outcome of interest. These will be specified for each outcome in each study and details recorded in Part E. The outcome of interest is first identified (Part A), then studies are screened to assess if they are at very high risk of bias (Part B), which would avoid the need for a detailed assessment. This involves answering key questions from the tool about whether confounding was controlled for and may be a concern, and whether the method of measuring exposure and outcome was inappropriate.\nFor each study, details of the participants, exposure, and outcome are extracted (Part C). Then the causal effect estimated by the study must be specified (Part D). This represents the target effect assuming no potential for confounding (e.g., target experiment). Bias risks are then assessed across seven domains using signalling questions (Part E), and an overall risk judgement is also determined. Risks of bias in each domain is assessed as low, some concerns, high, or very high. The predicted direction of bias and whether it may threaten conclusions is also assessed for each outcome.\nThere is a spreadsheet version of the tool online you can download and use to help complete the risk-of-bias assessments.\n\n5.3.1.1 ROBINS-E Domains\nRisk of bias due to confounding\nConfounding occurs when there are common causes of the exposure and outcome of interest. This domain assesses whether the study appropriately measured and controlled for all of the important confounding factors. Common control methods include stratification, matching, regression, standardization, or inverse probability weighting. This domain also assesses whether any post-exposure variables were controlled for in the analysis, which can introduce bias. If time-varying confounding is also a concern, additional questions are used to assess the impact of this bias. Time-varying confounding occurs when the exposure changes over time and prognostic factors affect exposure status during the exposure time period.\n\n\n\n\n\n\nUncontrolled Confounding\n\n\n\nEven if all important confounders were identified and adjusted for in analysis, there is always a possibility for uncontrolled confounding. This could be due to unmeasured factors, imprecise measurement of confounding factors, or measured confounding factors were not adjusted for in the analysis. Therefore, there will always be some concerns about uncontrolled confounding even with a low risk judgement.\n\n\nRisk of bias arising from measurement of the exposure\nThis domain addresses the risk of possible measurement error (continuous variables) or misclassification (categorical variables). Different variants of this domain are used depending on whether there are multiple measures of the exposure over time and how those measurements are used in the analysis. This domain considers how well the exposure variable represents the target exposure of interest, whether the exposure was likely to be measured with error (or was misclassified), and whether any measurement errors or misclassification could have been differential (i.e., related to the outcome or risk of outcome). An example of differential error is recall bias in case-control studies.\nRisk of bias in selection of participants into the study\nThis domain assesses possible selection bias, where some participants, follow-up time of some participants, or some outcome events are excluded in a way that leads to an association between the exposure and outcome. Questions assess whether selection was based on characteristics observed after the start of the exposure window, and whether these characteristics were related to both the exposure and outcome. Questions also assess whether follow-up began at or close to the start of the exposure window, and whether the exposure is likely to be constant over time. Methods of analysis to correct for potential selection biases are also considered.\nRisk of bias due to post-exposure interventions\nThis domain considers whether the exposure leads to administration of interventions that can alter the exposure-outcome relationship. For example, in a study of individuals exposed to asbestos, the most highly exposed were more likely to receive a CT scan, which could affect their risk of lung cancer mortality. If no interventions are administered to alleviate the effects of the exposure, there should be no issues with this domain.\nRisk of bias due to missing data\nThis domain assesses risks of missing data for exposures, outcomes, or confounding variables, and whether this led to bias. The approach to analysis is considered in making this assessment (complete case analysis or multiple imputation).\nRisk of bias arising from measurement of the outcome\nThis domain assesses risks of non-differential or differential measurement errors in the outcome. It considers if measurements could have differed by group, whether outcome assessors were aware of participants’ exposure history, and whether this could have affected the outcome measurement.\nRisk of bias in selection of the reported result\nThis domain assesses selective reporting of the exposure and outcome (from multiple measurement methods), the results (from multiple analysis methods), and the participants (from a larger cohort). Unlike RCTs, it is unlikely that pre-specified analysis plans are available to compare with a study’s results, and clues often must be gained from comparing the methods and results sections.\n\n\n\n\n\n\nROBINS-E Example\n\n\n\nA systematic review and meta-analysis of the association between ambient air pollution and clinical dementia used the ROBINS-E tool to assess risk of bias in relevant studies (Wilker, Osman, and Weisskopf 2023). A total of 51 relevant studies were included. Results of the ROBINS-E assessment were summarized in a table. The first several studies of this summary table are shown below:\n\nIn this table, an asterisk (*) Indicates that the likely direction of bias would be towards the null. The table legend is as follows: A = Confounding; B = Post-exposure intervention; C = Missing data; D = Measurement of the outcome. All studies were rated as some concerns in the domains of “Measurement of the exposure” and “Selection of reported results”, all studies were rated as “Low risk” in the “Selection of participants” domain.\n\n\n\n\n\n5.3.2 Other Risk-of-Bias Tools\nThere are various other risk-of-bias tools that have been developed and applied to different study designs. However, not all of these tools have been validated, and some use outdated methods (e.g., scoring systems). For reviews of interventions, the Cochrane Collaboration tools are highly recommended. For reviews of exposures, the ROBINS-E tool is recommended if the predominant design is cohort studies. For other types of reviews, the JBI Critical Appraisal Tools are recommended. For example, JBI has developed critical appraisal tools for:\n\nAnalytical cross-sectional studies\nCase-control studies\nCase reports and case series\nPrevalence studies\nQualitative research\nText and opinion articles\nEconomic evaluations, and many others.\n\n\n\n\n\n\n\nObservational Study Risk-of-Bias Exercise\n\n\n\nWe will practice using the ROBINS-E tool on two studies that were included in the systematic review of the association between green spaces and mortality (Rojas-Rueda et al. 2019). The two articles and spreadsheet templates are available in the following shared folder.\nFor the purposes of this exercise, consider confounders of interest to be any of the following socio-economic factors: age, sex, ethnicity, marital status, education, income, and employment .\n\nWhat is your risk-of-bias judgement for each domain?\nWhat is your overall risk-of-bias judgement for each study?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-of-Bias Assessment</span>"
    ]
  },
  {
    "objectID": "5_bias.html#risk-of-bias-for-missing-data",
    "href": "5_bias.html#risk-of-bias-for-missing-data",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.4 Risk-of-Bias for Missing Data",
    "text": "5.4 Risk-of-Bias for Missing Data\nThe risk-of-bias tools covered above include an item to assess possible risks from selective reporting of results when multiple measurements or analyses were made. In cases where the result for an outcome is omitted entirely, the risk of bias should be assessed at the overall synthesis level. A new risk-of-bias tool called ROB-ME has been developed to assess these factors at the synthesis level (Page et al. 2023). The tool uses the following process to assess possible biases due to missing data in a meta-analysis:\n\nSelect syntheses that will be assessed for risk of missing data.\nDetermine which studies that are eligible for the synthesis have missing results.\nConsider the potential for missing studies across the systematic review.\nAssess risk of bias due to missing evidence in each meta-analysis.\n\nThis process aims to first determine if any studies might have measured relevant results for a synthesis, but did not report them at all or in a format usable for the synthesis (e.g., meta-analysis). This is especially important if the results were not reported because they were not statistically significant or did not support the authors’ hypotheses (e.g., results with null findings not reported in a study). This process may involve checking a study’s protocol (if published), methods, analysis plan, or other supporting documents.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-of-Bias Assessment</span>"
    ]
  },
  {
    "objectID": "5_bias.html#reporting-risk-of-bias-results",
    "href": "5_bias.html#reporting-risk-of-bias-results",
    "title": "5  Risk-of-Bias Assessment",
    "section": "5.5 Reporting Risk-of-Bias Results",
    "text": "5.5 Reporting Risk-of-Bias Results\nRisk-of-bias summary tables or figures should be provided in the systematic review report. A table or plot should be included that contains the risk-of-bias judgement for each domain for each study outcome, or this data can be made available in the supplementary materials if there are many included studies. In that case, it can be useful to provide a summary table in the manuscript text. An example from Young et al. (2019) is shown below. Additionally, the domain-specific judgements or overall judgement can also be included alongside each study in meta-analysis forest plots.\n\nThere is an online R Shiny App called robvis that can be used to create high-quality risk-of-bias figures for the Cochrane Collaboration risk-of-bias tools. The app contains template spreadsheets that can be altered with your risk-of-bias results, which are then uploaded to create a Cochrane-style risk-of-bias plot as per below.\n\nThe risk-of-bias results should be incorporated into the summary assessment for each outcome in a review. For example, if conducting meta-analysis, a subgroup analysis (Chapter 9) could be conducted to examine differences in measures of effect for studies at different risks of bias. Alternatively, the analysis could be restricted to only studies at low risk of bias, but if this is done, a sensitivity analysis should be conducted to compare the results with those including all of the studies. If all studies are included, the summary risk of bias should be incorporated into a certainty of evidence assessment (Chapter 10) (e.g., certainty of evidence would be lowered if many studies have bias concerns).\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nPage, Matthew J., Jonathan A. C. Sterne, Isabelle Boutron, Asbjørn Hróbjartsson, Jamie J. Kirkham, Tianjing Li, Andreas Lundh, et al. 2023. “ROB-ME: A Tool for Assessing Risk of Bias Due to Missing Evidence in Systematic Reviews with Meta-Analysis.” BMJ 383 (November): e076754. https://doi.org/10.1136/bmj-2023-076754.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSterne, Jonathan A. C., Jelena Savović, Matthew J. Page, Roy G. Elbers, Natalie S. Blencowe, Isabelle Boutron, Christopher J. Cates, et al. 2019. “RoB 2: A Revised Tool for Assessing Risk of Bias in Randomised Trials.” The BMJ 366. https://doi.org/10.1136/bmj.l4898.\n\n\nWilker, Elissa H., Marwa Osman, and Marc G. Weisskopf. 2023. “Ambient Air Pollution and Clinical Dementia: Systematic Review and Meta-Analysis.” BMJ 381 (April): e071620. https://doi.org/10.1136/bmj-2022-071620.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Risk-of-Bias Assessment</span>"
    ]
  },
  {
    "objectID": "6_extraction.html",
    "href": "6_extraction.html",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "",
    "text": "6.1 Types of Outcomes and Measures of Effect\nA key first step in designing the outcome data extraction form and planning for analysis is to determine the relevant types of outcome data measures for the review. The most common types are listed below:\nEach of these are discussed further below. For each study, we will want to either directly extract a measure of effect (i.e., effect measure, effect size) from the study, or extract the raw data and use that data to calculate a measure of effect. Typically, for reviews of RCTs, the raw data is sufficient for extraction (if available), as the randomization process should mitigate the impact of confounding bias. When extracting data from observational studies and non-randomization trials, typically the adjusted measure of effect should be extracted as it reduces to the impact of confounding variables. Additionally, it is preferable to extract measures of effect directly from RCTs that have adjusted for clustering or baseline measurements.\nMeasures of effect can be expressed as ratios (e.g., odds ratio, risk ratio, hazard ratio) or differences (e.g., mean difference, risk difference). For reviews of prevalence and incidence questions, the measure of effect is simply the prevalence or incidence point estimate(s) of interest.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#types-of-outcomes-and-measures-of-effect",
    "href": "6_extraction.html#types-of-outcomes-and-measures-of-effect",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "",
    "text": "Dichotomous (AKA binary)\nContinuous\nOrdinal\nCounts and rates\nTime-to-event (survival data)\n\n\n\n\n\n\n\n\n\nRatio Measures of Effect\n\n\n\nRatio measures of effect, such as odds ratios, are analyzed on the natural log scale, so must be log-transformed for all studies prior to analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#unit-of-analysis",
    "href": "6_extraction.html#unit-of-analysis",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.2 Unit of Analysis",
    "text": "6.2 Unit of Analysis\nWhen extracting outcomes from studies you will likely encounter unit-of-analysis issues. These are situations where outcomes are nested within larger units, or there are repeated measurements on the same participants, and this extra level of variation must be accounted for when conducting analysis. The following are common situations of this:\n\nThe study has clusters (e.g., schools, communities) with outcomes measured and reported on participants within the cluster\nThere are repeated measurements on participants (e.g., outcomes are reported at multiple follow-up periods)\nEach participant receives multiple treatments or interventions\nThere are multiple variations of the same outcome reported for participants in the same study\nThere are multiple intervention groups compared in the same study\n\nStatistical approaches to address some of these issues will be covered in a later section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#data-extraction-form",
    "href": "6_extraction.html#data-extraction-form",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.3 Data Extraction Form",
    "text": "6.3 Data Extraction Form\nThe data extraction form should be designed so that multiple outcomes can be extracted for each study. The suggested approach is to pre-determine outcome categories for analysis based on your review question. These should represent specific groups of similar outcomes that will combined together in the same analysis. You can also include additional questions on the form that might have different responses for different outcome categories (e.g., how the outcome was measured). You may also have different categories for intervention or exposure groups of interest.\nAn example of a data extraction form from Young et al. (2019) is available for download here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#dichotomous-measures",
    "href": "6_extraction.html#dichotomous-measures",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.4 Dichotomous Measures",
    "text": "6.4 Dichotomous Measures\nDichotomous outcomes are those with only two possibilities (e.g., disease positive or negative). Two common measures of effect for these outcomes are odds ratios (OR) and risk ratios (RR). An RR is a ratio of the risk of the event in each group, while the OR is ratio of the odds of event (compared to a non-event) in each group. The table below shows how these two measures are calculated:\n\n2 X 2 table for an intervention study\n\n\n\nEvent\nNon-Event\nTotal\n\n\n\n\nIntervention\nA\nB\n\\({N_1}\\)\n\n\nControl\nC\nD\n\\({N_2}\\)\n\n\n\n\\[\n\\text{RR} = \\frac{\\text{A}/{N_1}}{\\text{C}/{N_2}}\n\\]\n\\[\n\\text{OR} = \\frac{\\text{A}/{B}}{\\text{C}/{D}}\n\\]\nThe RR and the OR are only similar when the event is rare.\nAnother possible measure for dichotomous outcomes is the risk difference (RD). This measure reflects the difference between the two observed groups risks. The practical significance of the RD depends greatly on the baseline risk of the event in the population.\n\\[\n\\text{RD} = \\left(\\frac{\\text{A}}{N_1}\\right)-\\left(\\frac{\\text{C}}{N_2}\\right)\n\\]\nTo conduct a meta-analysis of dichotomous data, we typically need one of the following sets of values for the intervention (or association) effect:\n\nNumerator and denominator in each group\nProportion + EITHER numerator or denominator in each group, or\nMeasure of effect (e.g., OR, RR) + a measure of variability",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#continuous-measures",
    "href": "6_extraction.html#continuous-measures",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.5 Continuous Measures",
    "text": "6.5 Continuous Measures\nContinuous outcomes can theoretically take any value within a specified range. The two most common measures of effect for continuous data are the mean difference (MD) and the standardization mean difference (SMD).\nThe MD can be used when the outcome is reported on a meaningful scale (e.g., weight in kg, blood pressure in mmHg), and the same scale is used in all studies. This measure is easy and intuitive to interpret if possible to calculate.\nThe SMD can be used when the studies in a SR measure the same outcome, but in different ways (e.g., different scales). The SMD is calculated by dividing a study’s mean difference by its standard deviation (SD) to create an index. There are different SMD metrics, the most common of which is called Hedges’ g, which uses a pooled SD in the denominator based on outcome data in both groups being compared, and assuming that the SDs of the two groups are similar.\nFor continuous data outcomes, we typically require the mean, sample size, and SD in each group to calculate either the MD or SMD. Alternatively, a pre-calculated measure of effect (e.g., MD) and its measure of variability (e.g., standard error) can also be used directly in a meta-analysis.\n\n\n\n\n\n\nChange from baseline data\n\n\n\nSome studies will report baseline and post-intervention values (often with different follow-up time points). It can be useful to extract both values if they are available and compare any differences. However, often the standard deviation (SD) of the change score is missing and needs to be imputed or estimated to include the score in a meta-analysis. This process involves many assumptions that should be evaluated through sensitivity analysis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#other-outcomes",
    "href": "6_extraction.html#other-outcomes",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.6 Other Outcomes",
    "text": "6.6 Other Outcomes\nOrdinal outcomes occur when there are multiple, ordered categories of an outcome (e.g., low, moderate, and high knowledge levels). As the number of ordinal categories increases, they start to resemble continuous outcomes, and could potentially be analyzed using those methods. If there is an obvious cut-point, the categories could be combined and used in a dichotomous analysis. Otherwise, if the same categories are used across all studies, this outcome can be analyzed using a proportional odds model.\nCount outcomes occur when the outcome can happen multiple times for each participant. These are usually expressed as rates (e.g., number of events per person-year), with the comparison of two rates being expressed as a rate ratio. The most common situation involving rate data is that studies directly report rate ratios and a measure of variability (e.g., standard error) from regression models, which can be extracted and combined in a meta-analysis.\nTime-to-event outcomes refer to the measurement of the time elapsed until some event (e.g., death) is experienced (e.g., survival data). These data are usually analyzed in studies using survival analysis and expressed as a hazard ratio (HR). Hazard measures instantaneous risk and can change continuously over time. As above, studies usually report the HR and a measure of variability which can be extracted directly and included in a meta-analysis.\n\n\n\n\n\n\nData Extraction Exercise\n\n\n\nConsider again the Young et al. (2019) systematic review about the effectiveness of food handler training interventions. Using three example articles, identify the outcomes of interest in each study and which data should be extracted.\n\nHow many relevant outcomes are reported in each study?\nAre any important outcomes or values (e.g., measures of variability) missing?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "6_extraction.html#data-required-for-meta-analysis",
    "href": "6_extraction.html#data-required-for-meta-analysis",
    "title": "6  Data Extraction and Outcome Measures",
    "section": "6.7 Data Required for Meta-Analysis",
    "text": "6.7 Data Required for Meta-Analysis\nIf you are interested or planning to conduct a meta-analysis, data must be sufficiently reported in the article to allow calculation of a summary measure of effect and to allow inclusion in a meta-analysis. Often, many studies will be missing information on one of the common inputs required. Thankfully, in many cases we can use various formulas to estimate the missing information. Some common scenarios are highlighted below.\n\n6.7.1 Convert Confidence Interval to a Standard Error (SE)\nIf only a 95% confidence interval (CI) is reported for an absolute measure of effect (e.g., standardized mean difference, risk difference), the SE can be calculated from the following formula:\n\\[\n\\text{SE} = \\frac{(\\text{Upper limit}-\\text{Lower limit})}{3.92}\n\\]\nFor ratio measures (e.g., odds ratio, risk ratio), the upper and lower CI limits, and intervention effect estimate, should be on the natural log scale.\nThe same formula can also be used to convert a CI of a mean value within each group (e.g., intervention, control) to its SE. However, in this case, if the sample size in each group is small (e.g., &lt;60), then the CIs should have been calculated using a t distribution and the divisor (3.92) should be replaced by a different number from a t distribution.\n\n\n6.7.2 Convert SE for Each Group to a SD for Each Goup\nIf only a SE, and not a SD, is reported within each group being compared, the SD can be obtained from this formula:\n\\[\n\\text{SD} = \\text{SE}\\sqrt{N}\n\\]\n\n\n\n\n\n\nExample SE to SD conversion\n\n\n\nWe can illustrate this conversion with some simulated data in R.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow calculate a SD variable for each group using the formula above.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n6.7.3 Convert from SE, CI, t value, or P value to a SD for Mean Differences\nWhen using the raw MD as the outcome, missing SD values can be estimated from a SE, CI, t value, or P value. These conversions assume that the SD values are the same in both groups being compared.\nTo calculate the t value from a P value (from a t-test), we can use the qt function, inputting the P value first (divided by 2), and the degrees of freedom next (equal to the sample size in each group minus 2). For example, for a study with a P value of 0.03 and a sample size of 20 in each group, we could use this formula:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow try calculating the t value from a study with a P value of 0.001 and a sample size of 150 participants in each group.\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nTo convert from a t value to a SE, we can use the following formula:\n\\[\n\\text{SE} = |\\frac{\\text{MD}}{\\text{t value}}|\n\\]\nThe SE can then be converted to a within-group SD using the following formula, where \\(N_1\\) and \\(N_2\\) represent the sample size in each group:\n\\[\n\\text{SD} = \\frac{\\text{SE}}{\\sqrt{\\frac{1}{N_1}+{\\frac{1}{N_2}}}}\n\\]\nThe SD is the average of the SDs of each group, and should be entered for both groups.\n\n\n6.7.4 Conversions to a Standardized Mean Difference\nThere are various formulas available to convert from SEs, ANOVA F values, t-test values, regression beta coefficients to SMD measures such as Hedges’ g, a log odds ratio, or a correlation coefficient r.\nThese use formulas from the practical meta-analysis effect size calculator and are implemented in the esc package in R. Some examples are given in the Doing Meta-Analysis in R online textbook.\nTwo examples are shown below for a one-way ANOVA F test and for a \\(chi^2\\) test.\n\n\n\n\n\n\nExample F Test and \\(chi^2\\) Test Converison\n\n\n\nSuppose we have a study that only reports the F value from a one-way ANOVA test, instead of means and SDs within each group or a MD value. If the F value is 6.23, and the sample size is 190 in the intervention group and 80 in the control group, we can calculate the SMD (as Hedges’ g) as follows:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilarly, if we had study that only reported a \\(chi^2\\) test value for a dichotomous association, we could convert that to a log odds ratio and its SE. Note that this conversion assumes a degrees of freedom of 1. For example, suppose the \\(chi^2\\) value was 2.5 and the sample size was 120:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can then load the data from the saved object into the applicable row in the dataset. For example, suppose the previous conversion was conducted for Study 3 as per the mock dataset below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can then use the following code to insert the saved conversion values into the applicable columns for Study 3:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nData Conversions Exercise\n\n\n\nUsing the systematic review article you identified earlier in the course, review the article to examine whether meta-analysis was conducted. If it wasn’t, you can answer the following questions based on this systematic review and meta-analysis of the effectiveness of workplace mindfulness training interventions (Bartlett et al. 2019). Consider the following questions for the article analyzed:\n\nWhat was the main outcome measure extracted and analyzed?\nDid the authors report whether any data conversions were necessary? If so, which ones, how many, and how were they conducted?\nDid the authors provide any references or rationale for any conversions conducted?\nWere any sensitivity analyses conducted related to the data conversions or assumptions made?\n\n\n\n\n\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Extraction and Outcome Measures</span>"
    ]
  },
  {
    "objectID": "7_analysis.html",
    "href": "7_analysis.html",
    "title": "7  Data Analysis",
    "section": "",
    "text": "7.1 Review Flow Chart\nUsually the first part of data analysis and reporting results includes summarizing details of the number of studies captured in the review and included/excluded at each step. These details should be shown through a flow chart diagram or figure. The Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) reporting guideline has a template for flow charts. A copy of the flow chart template is shown below:\nThe flow chart should capture exact numbers of references identified from each source (e.g., bibliographic databases, grey literature, etc.) It should clearly indicate the number of references excluded and included at each step of the review, along with reasons for excluding references or articles. There is also an online Shiny app for creating flow charts using this template. An example flow chart from Young et al. (2019) is show below.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "7_analysis.html#descriptive-analysis-of-article-characterization-results",
    "href": "7_analysis.html#descriptive-analysis-of-article-characterization-results",
    "title": "7  Data Analysis",
    "section": "7.2 Descriptive Analysis of Article Characterization Results",
    "text": "7.2 Descriptive Analysis of Article Characterization Results\nThe next step in analysis is usually to descriptively summarize the article characteristics (e.g., PICO/PECO elements, and other relevant article and study details). It can be helpful at this stage to start categorizing the PICO/PECO elements into groups that can facilitate summary displays (e.g., tables) and further analysis. For example, specific types of interventions and outcomes might be grouped together. A table can then be presented that has each relevant study as a row and specific columns for each of the relevant PICO/PECO elements or groups. An example of such a table from Bartlett et al. (2019) is shown below (note this shows only the first several studies included in the table):\n\nIn addition to showing a table with a the summary characteristics and PICO/PECO details for each study, in can also be useful, especially in cases where the number of relevant studies included in the review is very large, to create a tabulation table to show overall numbers and percentages of studies that investigated specific PICO/PECO elements and reported specific details. An example of such a table from Young et al. (2019) is shown below (note that only the first several rows of the table are shown here). In the example below, we will examine how to reproduce such a table in R.\n\n\n\n\n\n\n\nSummary Article Characterization Table Example\n\n\n\nAs an illustrative example of calculating a summary tabulation table, we will load some selected article characterization data from the relevant articles in Young et al. (2019).\n\n\n\n\n\n\nNow we can produce some summary statistics and tabulations of the variables and organize them into a table to present in our results.\n\n\nCode\nlibrary(dplyr)\nlibrary(gtsummary)\n\n# First convert Year variable to numeric\nYoung_2019_chart &lt;- Young_2019_chart |&gt; \n  mutate(Year = as.numeric(Year))\n\n# Display preferences\ntheme_gtsummary_language(\"en\", big.mark = \"\")\n\n# Create and display summary table\nYoung_2019_summary &lt;- Young_2019_chart |&gt; \n  select(-Refid, -\"Author (year)\") |&gt;  # Remove Refid and author (year) columns from summary\n  tbl_summary(type = Year ~ \"continuous\",\n              statistic = Year ~ \"{median} ({min}-{max})\",\n              sort = list(all_categorical() ~ \"frequency\"),\n              digits = list(all_categorical() ~ c(0, 1)))\nYoung_2019_summary\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 471\n\n\n\n\nYear\n2010 (1979-2017)\n\n\nDocument type\n\n\n\n\n    Journal article\n35 (74.5%)\n\n\n    Thesis\n12 (25.5%)\n\n\nLanguage\n\n\n\n\n    English\n45 (95.7%)\n\n\n    Italian\n1 (2.1%)\n\n\n    Korean\n1 (2.1%)\n\n\nCountry\n\n\n\n\n    US\n22 (46.8%)\n\n\n    Canada\n7 (14.9%)\n\n\n    India\n3 (6.4%)\n\n\n    South Korea\n3 (6.4%)\n\n\n    Brazil\n2 (4.3%)\n\n\n    UK\n2 (4.3%)\n\n\n    Bahrain\n1 (2.1%)\n\n\n    Iran\n1 (2.1%)\n\n\n    Italy\n1 (2.1%)\n\n\n    Kenya\n1 (2.1%)\n\n\n    Malaysia\n1 (2.1%)\n\n\n    Myanmar\n1 (2.1%)\n\n\n    Nigeria\n1 (2.1%)\n\n\n    Saudi Arabia\n1 (2.1%)\n\n\nStudy design\n\n\n\n\n    Controlled before-and-after study\n19 (40.4%)\n\n\n    Randomized controlled trial (RCT)\n18 (38.3%)\n\n\n    Non-randomized controlled trial\n10 (21.3%)\n\n\nSetting includes restaurants\n16 (34.0%)\n\n\nIntervention type\n\n\n\n\n    Training course or session\n31 (66.0%)\n\n\n    Multi-faceted\n11 (23.4%)\n\n\n    Messaging materials\n4 (8.5%)\n\n\n    Consulting services\n1 (2.1%)\n\n\nIntervention content described\n36 (76.6%)\n\n\nIntervention informed by theory\n\n\n\n\n    No / not reported\n40 (85.1%)\n\n\n    Yes\n7 (14.9%)\n\n\nIntervention informed by research\n\n\n\n\n    Yes\n35 (74.5%)\n\n\n    No / not specified\n12 (25.5%)\n\n\nStakeholder engagement used\n\n\n\n\n    No / not reported\n30 (63.8%)\n\n\n    Yes\n17 (36.2%)\n\n\nIntervention duration\n\n\n\n\n    Not reported\n15 (31.9%)\n\n\n    Less than 1 day\n14 (29.8%)\n\n\n    1 day or longer\n12 (25.5%)\n\n\n    N/a\n6 (12.8%)\n\n\nComparison group\n\n\n\n\n    No intervention\n39 (83.0%)\n\n\n    Standard/traditional intervention\n8 (17.0%)\n\n\nOutcome - knowledge\n32 (68.1%)\n\n\nOutcome - attitudes\n12 (25.5%)\n\n\nOutcome - behaviour\n19 (40.4%)\n\n\nOutcome - inspection scores\n17 (36.2%)\n\n\nCollection tool pre-tested\n\n\n\n\n    Yes\n29 (61.7%)\n\n\n    No / not specified\n18 (38.3%)\n\n\n\n1 Median (Minimum-Maximum); n (%)\n\n\n\n\n\n\n\n\n\nWe can further adjust the settings and presentation of the table as needed, see the Epi R Handbook section on the gtsummary package for more information. We can also copy and paste our table from the HTML viewer in RStudio into a Word document for easy editing and inclusion in our report.\n\n\n\n\n\n\n\n\nFlow Chart and Article Summary Exercise\n\n\n\nUsing the systematic reviews you identified in previous weeks related to your thesis topic, review the article to examine how they displayed their flow chart and article summary data. Consider the following questions:\n\nHow does the flow chart compare to the PRIMSA template shown above?\nWhat format is used for the article summary information?\nAre any important article characteristics missing?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "7_analysis.html#methods-of-analyzing-measures-of-effect",
    "href": "7_analysis.html#methods-of-analyzing-measures-of-effect",
    "title": "7  Data Analysis",
    "section": "7.3 Methods of Analyzing Measures of Effect",
    "text": "7.3 Methods of Analyzing Measures of Effect\nThe recommended approach to analyze quantitative outcome data (i.e., measures of effect), if possible and reasonable to do so, is meta-analysis. Meta-analysis is defined as statistical method for combining the results of two or more studies.\nThere are some situations where it may not be possible to use meta-analysis, for example:\n\nThere is only one or no studies containing the outcome of interest\nThe outcome data are not sufficiently reported and cannot be estimated from available statistics\nThe study designs, methods, or outcome measures are too diverse and heterogeneous that it doesn’t make biological or reasonable sense to combine them\nThere are major concerns about bias in the studies\n\nIn these cases, one can consider conducting a descriptive and narrative summary of the evidence. This includes presenting summary tables of the results of each study and some discussion of the results of each study. Tables can be grouped by relevant criteria from the question (e.g., intervention groups, outcome domains), or by other factors such as strength of evidence or risk of bias.\nAnother option that can be used when summary measures of effect are available from studies, but measures of variability needed to conduct meta-analysis are not available, is to summarize the effect estimates directly using box (and/or violin) plots. This type of summary provides information about the size and range of effects across studies, but does not account for the differences in sample size across studies. Combining P values is another method that can be used when only P values are available, and aims to determine is there is evidence of an effect in at least one study. For more information on these and other alternative analysis methods, see the relevant chapter in the Cochrane Collaboration Handbook.\n\n\n\n\n\n\nExample of when Meta-Analysis was Not Feasible\n\n\n\nIn a systematic review about the prevalence of knowledge, practices, and training outcomes among restaurant and food service personnel toward food allergies and Celiac disease (Young and Thaivalappil 2018), the authors did not conduct meta-analysis on the outcome due to substantive differences in the populations assessed, their characteristics, and how outcomes were measured. Instead, they summarized the distribution of prevalence outcomes across studies within each outcome domain using box plots. The figure below shows the prevalence of food allergy and Celiac disease behaviour outcomes reported across studies:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "7_analysis.html#introduction-to-meta-analysis",
    "href": "7_analysis.html#introduction-to-meta-analysis",
    "title": "7  Data Analysis",
    "section": "7.4 Introduction to Meta-Analysis",
    "text": "7.4 Introduction to Meta-Analysis\nMeta-analysis is a statistical technique that involves combining the measure of effect for each study along with its weight to calculate a weighted average of the measure of effect. Each study receives a weight based on its standard error (SE), so studies with greater precision (i.e., less error) receive more weight. There are two main approaches to meta-analysis, which determine how weights are calculated: fixed-effect and random-effects.\nA fixed-effect meta-analysis assumes there is one true effect size among all of the included studies, and that any observed differences in the effects are due only to sampling error. In contrast, random-effects meta-analysis models assume that the study effects represent a random sample of possible effect sizes (that usually follows a normal distribution). The null hypothesis under a fixed-effect model is that there is zero effect in every study, while for a random-effects model the null hypothesis is that the average effect is zero. Both models will give identical results when there is no statistical heterogeneity among the studies.\nAssuming an overall true effect of \\(\\theta\\), under a fixed-effect model, the observed effect of study j, \\(\\theta_j\\), differs from the overall true effect only because of its sampling error (\\(\\epsilon_j\\)):\n\\[\n\\hat{\\theta_j} = \\theta + \\epsilon_j\n\\] Studies are then weighted by the inverse of the variance (which is equal to the SE squared). For study j, the weight would be calculated as follows:\n\\[\n{W_j} = \\frac{1}{SE_j^2}\n\\]\nThe weighted average, or pooled effect, can then be calculated by multiplying each study’s effect size by its weight, adding those together, and then dividing by the sum of all study weights. This is referred to as the generic inverse-variance weighting approach:\n\\[\n\\hat{\\theta} = \\frac{\\sum_{j=1}^J \\hat{\\theta_j}{W_j}} {\\sum_{j=1}^J {W_j}}\n\\]\nIn contrast, random-effects models incorporate an additional source of error, denoted \\(\\zeta_j\\), to indicate the true effect size of study j is part of a distribution of true effect sizes with a mean value of \\(\\mu\\):\n\\[\n\\hat{\\theta_j} = \\mu + \\zeta_j + \\epsilon_j\n\\]\nIn a random-effects model, study weights incorporate a measure of the variance of the distribution of true effect sizes, denoted \\(\\tau^2\\):\n\\[\n{W_j} = \\frac{1}{{SE_j^2}+{\\tau^2}}\n\\]\nThe adjusted weight is then used to calculate the weighted average measure of effect. There are several methods to estimate \\(\\tau^2\\). The most famous is called the DerSimonian-Laird (DL) method. This method has straightforward calculations, and is often the default method in various software packages. However, this method can be biased and results in confidence intervals that are too narrow when the number of studies is small and heterogeneity is high.\nBased on simulation studies, the restricted maximum likelihood estimator (REML) approach seems to be good choice for continuous outcome data (Langan et al. 2019; Veroniki et al. 2016). For dichotomous (binary) outcome data, the Paule-Mandel estimator or the Empirical Bayes method seem to be good choices. You can always conduct a sensitivity analysis to compare with other approaches or the standard DL method.\nAdditionally, it is generally recommended to apply Knapp-Hartung adjustments if the option is available, as this method widens the confidence interval of the weighted measure of effect to account for uncertainty in the estimation of \\(\\tau^2\\).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "7_analysis.html#fixed-vs.-random-effects-models",
    "href": "7_analysis.html#fixed-vs.-random-effects-models",
    "title": "7  Data Analysis",
    "section": "7.5 Fixed vs. Random-Effects Models",
    "text": "7.5 Fixed vs. Random-Effects Models\nFixed-effect models are usually not appropriate for reviews in occupational and public health, as they assume that all studies are essentially identical, which is usually only the case under highly controlled and replicable conditions. The results from such an analysis are only applicable to the participants in each study, and cannot be generalized for other populations.\nIn contrast, random-effects models are more appropriate when there are differences across studies beyond what we would expect from sampling error alone (e.g., differences in interventions, populations, how outcomes are measured). However, this approach gives more weight to smaller studies, which can exacerbate potential biases that may be present in smaller studies. Additionally, if the number of included studies is very small, the estimate of between-study variance \\(\\tau^2\\) will have poor precision and is less reliable. Unfortunately, there is no clear guidance about which approach should be used in this situation. Bayesian methods are probably the best option, but require more advanced expertise.\n\n\n\n\n\n\nMeta-Analysis Exercise\n\n\n\nUsing the systematic review article you identified earlier in the course, review the article to examine whether meta-analysis was conducted. If not, search for another paper on the topic that conducted meta-analysis or examine one of the other example articles given in Chapter 12. Consider the following questions for the article analyzed:\n\nDid they conduct a fixed-effect or random-effects model, and what rationale (if any) was given for the choice?\nIf a random-effects model was used, which method was used to estimate \\(\\tau^2\\)?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "7_analysis.html#qualitative-syntheses",
    "href": "7_analysis.html#qualitative-syntheses",
    "title": "7  Data Analysis",
    "section": "7.6 Qualitative Syntheses",
    "text": "7.6 Qualitative Syntheses\nMany of the systematic review methods discussed in the course can similarly be applied to qualitative syntheses, with some adaptations. The analysis, or synthesis, stage in particular is very different (Barnett-Page and Thomas 2009). The data that is synthesized in such reviews can vary depending on the review objectives and types of evidence included, but is typically themes supported by participant quotes from qualitative research studies (e.g., focus groups, grounded theory studies, etc.) or mixed-method studies. Sometimes, such reviews might also include data from review articles or policy documents to be synthesized into a narrative synthesis, which is common for reviews that incorporate multiple types and sources of evidence.\nOne of the more common syntheses approaches for qualitative reviews is called thematic synthesis (Thomas and Harden 2008). Thomas and Harden (2008) describe a three-step process of conducting this analysis, which consists of:\n\nLine-by-line coding of the findings (e.g., participant quotes and themes) of primary studies\nOrganization and grouping of the codes into related areas, which is used to develop descriptive themes\nDevelopment of analytical themes, which aim to go beyond the findings of the original studies and produce overarching themes.\n\nThe latter step requires some judgement of reviewers and involves making inferences about the mechanisms behind the descriptive themes.\nThere are other approaches to qualitative synthesis as well, such as meta-aggregation as described by JBI, which follow a similar process (Lockwood, Munn, and Porritt 2015).\n\n\n\n\n\n\nQualitative Synthesis Exercise\n\n\n\nExamine the following qualitative systematic review by Rawlings et al. (2019) that investigated the perceptions and experiences of sedentary behaviour in adults.\n\nDid their search differ from that of a standard systematic review?\nWhat was their quality assessment process?\nHow did they conduct their synthesis?\nHow did they report their synthesis results?\n\n\n\n\n\n\n\nBarnett-Page, Elaine, and James Thomas. 2009. “Methods for the Synthesis of Qualitative Research: A Critical Review.” BMC Medical Research Methodology 9 (1): 59. http://www.biomedcentral.com/1471-2288/9/59.\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr Otahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A Systematic Review and Meta-Analysis of Workplace Mindfulness Training Randomized Controlled Trials.” Journal of Occupational Health Psychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nLangan, Dean, Julian P. T. Higgins, Dan Jackson, Jack Bowden, Areti Angeliki Veroniki, Evangelos Kontopantelis, Wolfgang Viechtbauer, and Mark Simmonds. 2019. “A Comparison of Heterogeneity Variance Estimators in Simulated Random-Effects Meta-Analyses.” Research Synthesis Methods 10 (1): 83–98. https://doi.org/10.1002/jrsm.1316.\n\n\nLockwood, Craig, Zachary Munn, and Kylie Porritt. 2015. “Qualitative Research Synthesis: Methodological Guidance for Systematic Reviewers Utilizing Meta-Aggregation.” JBI Evidence Implementation 13 (3): 179. https://doi.org/10.1097/XEB.0000000000000062.\n\n\nRawlings, G. H., R. K. Williams, D. J. Clarke, C. English, C. Fitzsimons, I. Holloway, R. Lawton, G. Mead, A. Patel, and A. Forster. 2019. “Exploring Adults’ Experiences of Sedentary Behaviour and Participation in Non-Workplace Interventions Designed to Reduce Sedentary Behaviour: A Thematic Synthesis of Qualitative Studies.” BMC Public Health 19 (1): 1099. https://doi.org/10.1186/s12889-019-7365-1.\n\n\nThomas, J, and A Harden. 2008. “Methods for the Thematic Synthesis of Qualitative Research in Systematic Reviews.” BMC Medical Research Methodology 8 (July): 45. https://doi.org/10.1186/1471-2288-8-45.\n\n\nVeroniki, Areti Angeliki, Dan Jackson, Wolfgang Viechtbauer, Ralf Bender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian P. T. Higgins, Dean Langan, and Georgia Salanti. 2016. “Methods to Estimate the Between-Study Variance and Its Uncertainty in Meta-Analysis.” Research Synthesis Methods 7 (1): 55–79. https://doi.org/10.1002/jrsm.1164.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review and Meta-Regression of the Knowledge, Practices, and Training of Restaurant and Food Service Personnel Toward Food Allergies and Celiac Disease.” Edited by Louise Emilsson. PLOS ONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Analysis</span>"
    ]
  },
  {
    "objectID": "8_meta.html",
    "href": "8_meta.html",
    "title": "8  Meta-Analysis",
    "section": "",
    "text": "8.1 Pooling Measures of Effect\nIn the previous chapter, we reviewed the basics of meta-analysis, including differences between fixed-effect and random-effects models. We will now cover procedures for conducting meta-analysis for different types of outcome data. All examples will use the meta package in R.\nThe most common method of pooling measures of effect is called the generic inverse-variance method, the formula for which was provided in Chapter 7. In this method, studies are weighted by the inverse of their precision. This method is used for continuous data (e.g., mean difference, SMD), and can also be used for dichotomous data. It is also the method used when conducting meta-analysis on pre-calculated measures of effect (e.g., odds ratios, risk ratios).\nWhen analyzing dichotomous raw data (e.g., number of events and sample size in each comparison group), there are alternative approaches available. The most common of these is the Mantel-Haenszel method. This method is preferred for dichotomous data, especially when the event is rare or when the sample size is small. The Peto odds ratio method is another approach, but it has more limitations and can only be used to calculate odds ratios.\nOne issue for dichotomous outcomes is that there may be zero events in one or both groups. In this case, the traditional, and often default approach in meta-analysis software, is to replace zero values with 0.5 (continuity correction). However, this correction is only required when using the generic inverse-variance approach; it should not be used when using the other methods.\nIn the meta package, the following functions can be used for meta-analysis of different types of data inputs:\nNote that we will not cover meta-analysis of all of these data types. If interested to see examples of others, you can visit the online book Doing Meta-Analysis in R.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "8_meta.html#pooling-measures-of-effect",
    "href": "8_meta.html#pooling-measures-of-effect",
    "title": "8  Meta-Analysis",
    "section": "",
    "text": "Change-From-Baseline Data\n\n\n\nAnalyses of an intervention based on a change from baseline can be more efficient, increasing precision of estimates. They are ideally analyzed by including the baseline measurement as a covariate in a regression analysis or ANCOVA. In meta-analysis, change-from-baseline scores and post-intervention values can be combined in the same analysis if using a raw MD outcome. However, they should not be combined when using a SMD outcome, as the standard deviations are not comparable.\n\n\n\n\n\n\nmetagen for pre-calculated measures of effect\nmetacont for continuous data\nmetabin for dichotomous data\nmetaprop for prevalence (proportion) data\nmetainc for incidence rate ratio or incidence rate difference data\nmetarate for incidence data in a single group\nmetacor for correlation coefficients\n\n\n\n\n\n\n\n\nContinuous Data Meta-Analysis Example\n\n\n\nTo illustrate how to conduct a meta-analysis using continuous data, we will examine part of the dataset from a systematic review of music interventions to improve various health outcomes in people with cancer (Bradt et al. 2021). Specifically, we will examine 17 studies from that review that evaluated music interventions plus standard care compared to standard care alone in adults to improve anxiety. Anxiety was measured in all studies using the Spielberger State Anxiety Index (STAI) scale, with lower scores representing lower anxiety.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWe can see that each study reported the mean anxiety score, SD, and sample size in each comparison group. Because all studies measured the outcome on the same scale, we can calculate a raw mean difference (MD). For continuous data, we will use the metacont function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nBased on this analysis, we can see that the anxiety scores were lower among participants that received music interventions in addition to standard care vs. standard care alone.\nIf we wanted to change any settings, we can use the update function to update our analysis. For example, we could run an updated analysis to compare how the results might change with a different method of calculating \\(\\tau^2\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe obtain slightly different results when using the alternative method of calculating \\(\\tau^2\\). How do the results change if we instead compare to the DerSimonian-Laird (DL) method?\n\n\n\n\n\n\n\n\nDichotomous Data Meta-Analysis Example\n\n\n\nTo illustrate how to conduct a meta-analysis using dichotomous data, we will examine a dataset of 136 studies used in a meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018), as prepared by White et al. in the metadat repository.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nAs can been seen above, the dataset contains information from each study on the number of participants in each group that continued to abstain from smoking at 6+ months of follow-up. Also included is a column that specifies the type of treatment received. Since the data are dichotomous and all include raw data, we can calculate an measure of effect using the Mantel-Haenszel method. We will use the metabin function, but first we will subset our data to only conduct the meta-analysis on studies that investigated the nicotine patch as an intervention.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this analysis, we can see that participants in the intervention group were more likely to continue to abstain from smoking at follow-up compared to those in the control group.\nHow do the results change, if at all, if we instead do not apply the Hartung-Knapp adjustment?\n\n\n\n8.1.1 Prevalence and Incidence Data\nMeta-analysis of prevalence and incidence data uses a slightly different approach than continuous and dichotomous data. The recommended approach for synthesizing such data is to use a generalized linear mixed-effects model (GLMM) (Schwarzer et al. 2019). Prevalence data should be logit transformed prior to meta-analysis, while incidence data should be log transformed (this is done automatically in the meta package). For prevalence data, the GLMM approach fits an intercept-only logistic regression to the data, with a random-effect to account for the between-study variation. A Poisson GLMM model is used for incidence data.\nUsing the GLMM approach has some limitations. It is not possible to obtain individual study weights using this method. Additionally, there is only one method to calculate \\(\\tau^2\\), the maximum-likelihood (ML) estimator, and there will be no confidence internals for \\(\\tau^2\\). In case those details are needed, the inverse-variance approach can be used with the arcsine or logit transformation for prevalence data, or the log transformation for incidence data.\n\n\n\n\n\n\nPrevalence Meta-Analysis Example\n\n\n\nAn example is shown below of a meta-analysis of the prevalence of selected outcomes from a systematic review of the knowledge, behaviours, and training of restaurant and food service personnel toward food allergies and Celiac disease (Young and Thaivalappil 2018).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWe can see that some article and study characteristics are also included in this dataset. For these data, we will subset the training outcome only for illustration purposes. This outcome shows the proportion of participants (i.e., restaurant and food service staff) in each study that reported receiving training about food allergies.\n\n# Note that this model does not work in webr, and must be run in R/RStudio. \nYoung_2018_meta &lt;- \n  metaprop(event = n.positive, \n           n = n.total,\n           studylab = \"author.year\", \n           subset = outcome.category == \"Training status/policies\",\n           data = Young_2018,\n           method = \"GLMM\",\n           sm = \"PLOGIT\",\n           random = TRUE,\n           fixed = FALSE,\n           hakn = TRUE, \n           title = \"Food Allergy Training Prevalence\"\n)\n\nsummary(Young_2018_meta)\n\nReview:     Food Allergy Training Prevalence\n\n   proportion           95%-CI\n79     0.4200 [0.3220; 0.5229]\n80     0.3333 [0.2374; 0.4405]\n81     0.3057 [0.2416; 0.3759]\n82     0.1500 [0.0571; 0.2984]\n83     0.4933 [0.3758; 0.6114]\n84     0.7909 [0.7030; 0.8626]\n85     0.3668 [0.3043; 0.4328]\n86     0.4085 [0.3701; 0.4477]\n87     0.1709 [0.1331; 0.2145]\n88     0.4557 [0.3998; 0.5124]\n89     0.2500 [0.1766; 0.3357]\n90     0.2609 [0.1834; 0.3510]\n\nNumber of studies: k = 12\nNumber of observations: o = 2382\nNumber of events: e = 871\n\n                     proportion           95%-CI\nRandom effects model     0.3587 [0.2578; 0.4739]\n\nQuantifying heterogeneity:\n tau^2 = 0.5203; tau = 0.7213; I^2 = 93.2% [89.9%; 95.4%]; H = 3.84 [3.15; 4.67]\n\nTest of heterogeneity:\n           Q d.f.  p-value\n Wald 161.87   11 &lt; 0.0001\n LRT  195.78   11 &lt; 0.0001\n\nDetails on meta-analytical method:\n- Random intercept logistic regression model\n- Maximum-likelihood estimator for tau^2\n- Random effects confidence interval based on t-distribution (df = 11)\n- Logit transformation\n- Clopper-Pearson confidence interval for individual studies\n\n\nWe can see the pooled prevalence value is ~36%, with a 95% CI of 26-47%.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "8_meta.html#assessing-heterogeneity",
    "href": "8_meta.html#assessing-heterogeneity",
    "title": "8  Meta-Analysis",
    "section": "8.2 Assessing Heterogeneity",
    "text": "8.2 Assessing Heterogeneity\nHeterogeneity refers to differences between studies that is beyond what we would expect from chance (or random error) alone. It can be due to differences in how interventions (exposures) and outcomes were defined, implemented, and measured, differences in the characteristics of the populations assessed, or other factors (e.g., differences in study methods, context, and bias).\nThere is a \\(\\chi2\\) statistical test for heterogeneity, called Cochran’s Q, that has been traditionally used and is included in the the R meta results output. This test evaluates whether this is more variation than would be expected by sampling error alone. However, this test has lower power when the number of studies is small. Additionally, its use is controversial, as some argue that since there are always differences expected between studies, some heterogeneity will always be present.\nFor this reason, the \\(I^2\\) statistic was developed to quantify heterogeneity (J. P. T. Higgins et al. 2003). The formula, based on Cochran’s \\(Q\\), is shown below, with \\(N\\) referring to the number of studies in the analysis:\n\\[\nI^2 = \\frac{Q-(N-1)}{Q}\n\\]\n\\(I^2\\) refers to the percentage of variation in measures of effect across studies that is due to heterogeneity rather than sampling error. While thresholds are often used in practice, these are discouraged. Instead, the amount of heterogeneity that is important depends on the context (e.g., magnitude and direction of effects, strength of evidence for heterogeneity). In general, \\(I^2\\) values of 0-40% might not be important, values of 75-100% usually indicate considerable heterogeneity, while values in 30-60% and 50-90% might indicate moderate to substantial heterogeneity (J. Higgins et al. 2022).\nHowever, because \\(I^2\\) is a relative measure, it should not be the only measure of heterogeneity reported. We can also examine \\(\\tau\\) and its 95% CI, which represents the estimated SD of the true effects across studies (it is on the same scale as the measure of effect used in the analysis).\nPrediction intervals (PIs) are recommended to be included alongside other estimates of heterogeneity. A 95% PI estimates the range of values the measure of effect would be expected to fall within in 95% of similar studies that might be conducted in the future. In cases of heterogeneity, the PI covers a wider range of values than a CI. In R meta, we can add a PI to our output by adding the argument prediction = TRUE to the function input options.\nWe will explore in the next session Chapter 9 how to investigate different causes of heterogeneity using subgroup analysis and meta-regression.\n\n\n\n\n\n\nHeterogeneity Example\n\n\n\nWe will go back to our first meta-analysis example that examined the effect of music interventions plus standard care vs. standard care alone to reduce anxiety levels among people with cancer (Bradt et al. 2021). We will update the analysis to include a prediction interval (PI), then interpret the heterogeneity.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see in the results that the \\(Q\\) test for heterogeneity is significant, and the \\(I^2\\) value is also very high at ~93% (95% CI: 90.9-95.3%). While the pooled measure of effect (MD) has a 95% CI that excludes the null, suggesting a consistent positive effect of the music intervention, the PI crosses above zero. This indicates that we cannot rule out that the intervention might have no effect or a negative effect in future studies.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "8_meta.html#forest-plots",
    "href": "8_meta.html#forest-plots",
    "title": "8  Meta-Analysis",
    "section": "8.3 Forest Plots",
    "text": "8.3 Forest Plots\nForest plots are the most common way to visualize meta-analysis results. They show the measure of effect estimate, confidence interval, and weight of each study, and the pooled or average estimate at the bottom. They can also include a prediction interval.\nBelow we will create a forest plot from the meta-analysis of music interventions on reducing anxiety levels in people with cancer, from the earlier example (Bradt et al. 2021). Using the meta package, we can generate a forest plot for our saved meta-analysis results using the forest function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can clean up the display of this plot by removing the data on the left side of the plot, and also adding in a prediction interval at the bottom. Other customization options can be made as needed.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nMeta-Analysis Exercise\n\n\n\nWe will load data from the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions. The dataset can be loaded and visualized as per below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWe can see that there are 214 unique outcomes, with multiple outcomes reported in many studies. We will first conduct a meta-analysis of a subset of data that only examines RCTs and behaviour outcomes:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nExamine and interpret the results.\n\nHow many studies and unique outcomes were included?\nWhat does the overall evidence say about the intervention?\nHow much heterogeneity is present? Is it significantly different than zero?\nHow does the result change if you use a different method of calculating \\(\\tau^2\\)?\n\nNow create a customized forest plot to visualize the results and include the prediction interval. How would you interpret the interval?\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow conduct a meta-analysis for the subset of non-randomized studies and the inspection scores outcome. Answer the same questions above for this analysis.\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nHiggins, J P T, S G Thompson, J J Deeks, and D G Altman. 2003. “Measuring Inconsistency in Meta-Analyses.” BMJ (Clinical Research Ed.) 327 (7414): 557–60. https://doi.org/10.1136/bmj.327.7414.557.\n\n\nHiggins, JPT, J Thomas, J Chandler, M Cumpston, T Li, MJ Page, and VA Welch, eds. 2022. Cochrane Handbook for Systematic Reviews of Interventions. Cochrane. www.training.cochrane.org/handbook.\n\n\nSchwarzer, Guido, Hiam Chemaitelly, Laith J. Abu-Raddad, and Gerta Rücker. 2019. “Seriously Misleading Results Using Inverse of Freeman-Tukey Double Arcsine Transformation in Meta-Analysis of Single Proportions.” Research Synthesis Methods 10 (3): 476–83. https://doi.org/10.1002/jrsm.1348.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review and Meta-Regression of the Knowledge, Practices, and Training of Restaurant and Food Service Personnel Toward Food Allergies and Celiac Disease.” Edited by Louise Emilsson. PLOS ONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Meta-Analysis</span>"
    ]
  },
  {
    "objectID": "9_meta2.html",
    "href": "9_meta2.html",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "",
    "text": "9.1 Subgroup Analysis\nSubgroup analysis, also known as moderator analysis, can be conducted to determine if specific study-related factors might explain some of the heterogeneity in our analysis. We essentially conduct one or more meta-analyses that are stratified by categorical variables of interest. Any such analyses should be pre-specified and identified in the review protocol. We should keep these analyses to a minimum, and focus on only those that are of most importance and have a strong rationale. For example, we may want to conduct a subgroup analysis to examine differences in pooled estimates across two or more variations of the intervention (or exposure), different control groups, study risk-of-bias ratings, study design, population characteristics (e.g., age group), or setting.\nThe process of subgroup analysis involves calculating a separate pooled measure of effect within each subgroup, then comparing the pooled estimates across groups with a statistical test. when comparing estimates across subgroups, an omnibus \\(Q\\) test is used, which compares whether all subgroup pooled estimates are equal. We have the option of calculating \\(\\tau^2\\) separately within each group, or assuming a common estimate of \\(\\tau^2\\) that is applied for each subgroup (the latter is recommended if there are only 5 or fewer studies in a subgroup).\nHowever, subgroup analyses have several limitations. They have low power to detect differences with small numbers of studies in each group, pooled estimates in each group will have lower precision, an absence of a detectable difference does not mean that the groups are equivalent, and any observed differences are only observational (and could be confounded by biases or other factors). At least 10 studies are recommended if conducting a subgroup analysis.\nTo conduct subgroup analysis in R, we can specify the subgroup option with any of the meta-analysis functions in the meta package.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subgroup Analysis, Meta-Regression, and Complex Data</span>"
    ]
  },
  {
    "objectID": "9_meta2.html#subgroup-analysis",
    "href": "9_meta2.html#subgroup-analysis",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "",
    "text": "Subgroup Analysis Example\n\n\n\nWe will re-load the same dataset from the last session that contained 136 studies used in a meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018), as prepared by White et al. in the metadat repository.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWe can see that there is a variable called treatment reflecting the type of nicotine replacement therapy assessed. We will conduct a subgroup analysis to compare differences between the types of treatments.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see that there are some differences in the pooled RR estimates across subgroups, but most groups contain very few studies. We could re-run our analysis and include only the four sub-groups which have more than 5 studies for a more reliable comparison.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOnce we remove the groups with few studies, we can see that there is no strong (significant) evidence of a difference in the size of the treatment effect based on the type of therapy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subgroup Analysis, Meta-Regression, and Complex Data</span>"
    ]
  },
  {
    "objectID": "9_meta2.html#meta-regression",
    "href": "9_meta2.html#meta-regression",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.2 Meta-Regression",
    "text": "9.2 Meta-Regression\nMeta-regression is another technique we can use to evaluate possible factors associated with between-study heterogeneity in the measures of effect. Meta-regression models are mixed-effect regressions that have terms to account for the sampling error \\(\\epsilon_j\\) and the between-study heterogeneity \\(\\zeta_j\\), and one (or more) beta parameters representing our predictor variables of interest. For study \\(j\\), we can express such a model as:\n\\[\n\\hat{\\theta_j} = \\mu + \\beta{x}_j + \\zeta_j + \\epsilon_j\n\\]\nA subgroup analysis is special case of this regression model, where we include a categorical predictor variable as our covariate \\({x}_j\\). When we evaluate continuous predictors (e.g., publication year), a weighted least squares approach is used to estimate the beta coefficient that best fits the data. In addition to estimating the effect of the predictor of interest, the model also calculates an \\(R^2\\) value that represents the proportion of between-study variation that is explained by the predictor.\nSimilar to subgroup analysis, usually at least 10 studies are recommended to conduct meta-regression. Meta-regression has many of the same limitations as subgroup analysis. Additionally, one should not conduct analysis of a predictor variables containing aggregated data from the studies (e.g., mean age in each study), as this type of analysis can be affected by ecological bias.\nIn the R meta package, we can conduct meta-regression on our meta-analysis object (output) using the metareg function.\n\n\n\n\n\n\nMeta-Regression Example\n\n\n\nTo illustrate how to conduct a meta-regression, we will reload data on the effect of music interventions + standard care vs. standard care alone to reduce anxiety levels, from a systematic review of music interventions to improve various health outcomes in people with cancer (Bradt et al. 2021).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nWe will examine whether study publication year as a continuous variable might explain any of the heterogeneity in the measure of effect across studies.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see in the results output that \\(I^2\\), representing the residual heterogeneity, is very high (&gt;96%). The test for residual heterogeneity also indicates there is still substantial heterogeneity present after accounting for publication year.\nThe \\(R^2\\) value is also only 1.7%, indicating the the publication year variable explain very little to no variability in effects across studies. The moderator test also indicates that this variable is not statistically significant (its 95% CI also crosses the null of no effect). If this variable was significant, we could say that for each additional year, the mean difference for a study is expected to increase by ~0.42 units.\nWe can also visualize the results of meta-regression analyses using a bubble plot.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nSubgroup Analysis and Meta-Regression Exercise\n\n\n\nWe will load data from the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nUsing these data, first conduct a subgroup analysis to examine the difference between RCTs and non-randomized studies for the behaviour outcome. Given a small number of RCTs, use a common estimate of \\(\\tau^2\\).\n\nWhat is the \\(I^2\\) in each subgroup?\nWhat is the evidence that the overall SMD in each group is different?\nCan you create a forest plot for this analysis that includes these two subgroups?\n\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow conduct a meta-regression to examine whether publication year of the study is associated with any of the variability in the behaviour outcome. How would you interpret the results? Now change the predictor variable to document type (journal or thesis/disseration) instead. How would you interpret these results?\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subgroup Analysis, Meta-Regression, and Complex Data</span>"
    ]
  },
  {
    "objectID": "9_meta2.html#publication-bias",
    "href": "9_meta2.html#publication-bias",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.3 Publication Bias",
    "text": "9.3 Publication Bias\nPublication bias is a major concern in systematic reviews. It refers to the probability that studies are more likely to be published, and specific outcomes and results in a study are more likely to be reported, if the findings are statistically significant or confirm the hypothesis of interest. Publication bias is one of several non-reporting biases that might be present in a systematic review. Others include citation bias, time-lag bias, multiple publication bias, language bias, and outcome reporting bias.\nFunnel plots can be used to visualize the possibility of publication bias in a meta-analysis. They are a scatter plot of the measures of effect of each included study against their standard error. If publication bias is not present, we would expect a funnel shape, with more precise (larger) studies clustering at the top and smaller, less precise studies scattered more widely at the bottom. Bias due to missing results presents as funnel plot asymmetry, where smaller studies without statistically significant results are missing on the bottom of the plot.\nHowever, asymmetry can be due to a variety of small-study effects, not just publications bias (e.g., higher risk of bias in smaller studies, true heterogeneity, spurious relationships, or chance). For this reason, it is recommended to use contour-enhanced funnel plots that include shaded regions for statistical significance thresholds (e.g., P = 0.01, 0.05) to identify if the missing pattern is likely due to publication bias or other small-study effects. There are various statistical tests for funnel plot asymmetry to assess whether the associated between effect sizes and precision is greater than expected to occur by chance (Sterne et al. 2011). However, these tests have many limitations. For example, they should not be used whether there are less than 10 studies and when the studies all have a similar sample size. The performance and reliability on the tests on observational studies is also not well researched.\nOne such publication bias (small-study effects) test is Egger’s regression test. However, this test should only be used on continuous outcomes. In can be implemented in the R meta package using the metabias function and the method.bias = \"linreg\" option. For SMD outcomes, it is recommended to use alternative method.bias = \"Pustejovsky\" option when conducting the test, which uses a different method of calculating the SE in each study. This method requires specification of the sample size for each comparison group. For dichotomous outcomes, the Harbord test or Peters test can be used with method.bias = \"Harbord\" and method.bias = \"Peters\", although there are other options as well. Many of these tests may not perform optimally or reliably when heterogeneity is very high.\nIf publication bias or other small-study effects are suspected, there are various statistical methods that can be used to examine the influence of the bias on the pooled measure of effect. The most popular of these methods is called the trim and fill method. This method imputes “missing” studies until the funnel is symmetrical. The pooled effect calculated using this method is considered bias-corrected. This method is also not reliable when heterogeneity is high. There are various other bias-adjustment methods available but no consensus on which approach is best.\n\n\n\n\n\n\nPublication Bias Example\n\n\n\nWe will re-examine the meta-analysis of the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018). We will conduct an overall pooled meta-analysis combining all therapies together, then assess for possible small-study effects using a contour-enhanced funnel plot.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see most of the studies with greater precision have clustered at the top, with many in the statistically significant regions. At the bottom, there are a handful of smaller studies, with only one being in the very significant region. Based on this plot, there does not appear to be a strong indication of small-study biases, though if anything, we could be missing some medium-sized studies in the non-significant region. We will now conduct a Peters test for asymmetry.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see that the test P value is 0.186, which does not suggest that small-study effects are present, but we cannot rule them out with only this test. For illustration purposes, we will conduct a trim and fill anlaysis to examine the possible magnitude of possible missing studies.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see that data from 23 missing studies were imputed. As expected, the updated RR is slightly more conservative compared to the original RR estimate. We can also produce a new funnel plot with the imputed missing studies.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can see the new studies (circles with no fill colour) have been added on the left side of the plot, representing mostly medium-sized studies that would be expected to show no intervention effect.\n\n\n\n\n\n\n\n\nPublication Bias Exercise\n\n\n\nFor this exercise, examine possible publication bias in the meta-analysis of the effect of music interventions + standard care vs. standard care alone to reduce anxiety levels (Bradt et al. 2021). You can use the same, saved Bradt_meta object from the earlier example.\n\nProduce a contour-enhanced funnel plot. Is there evidence of asymmetry?\nBased on the contour regions and meta-analysis results, could this be due to publication bias or other small-study effects?\nConduct Egger’s regression test. How would you interpret the result?\n\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow conduct a trim-and-fill analysis. How many “missing studies” were added, and what impact does the missing study imputation have on the overall MD?\n\nCode EditorAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subgroup Analysis, Meta-Regression, and Complex Data</span>"
    ]
  },
  {
    "objectID": "9_meta2.html#complex-data-issues",
    "href": "9_meta2.html#complex-data-issues",
    "title": "9  Subgroup Analysis, Meta-Regression, and Complex Data",
    "section": "9.4 Complex Data Issues",
    "text": "9.4 Complex Data Issues\nIn an earlier section Chapter 6, we introduced unit-of-analysis issues. This section will provide some additional details on how to address some of these issues using statistical techniques. We will examine how to adjust standard errors in clustered studies and studies that report more than one outcome of the same type. We will also examine different methods of combining multiple intervention/exposure comparison groups.\n\n9.4.1 Cluster Trial Adjustments\nCluster trials can present an issue if an intervention is applied at the cluster level (e.g., school), but the outcomes are measured and reported on individual participants within the cluster (e.g., students) and the clustering is not accounted for in the analysis. In this case, the reported results can be adjusted for a design effect as per the following formula:\n\\[\nDEFF = 1 + (M - 1)ICC\n\\]\nIn this formula, \\(M\\) refers to the average cluster size in the study, and \\(ICC\\) is the intra-class correlation coefficient. This value is rarely reported, so would need to be estimated from a similar study or other literature. The sample size in each group can then be divided by the design effect to obtain proper estimates. For dichotomous outcomes, both the number of events and sample size should be divided by this effect. Alternatively, the standard error of the measure of effect of the study can be multiplied by the square root of the design effect.\n\n\n\n\n\n\nCluster Trial Adjustment Example\n\n\n\nIn the Young et al. (2019) systematic review and meta-analysis of the effectiveness of food handler training and education interventions, several cluster trials were included and a design effect was used to adjust the standard errors of studies that did not report the correct analysis of those trials. A common ICC value of 0.1 was used for the formula, based on a study of similar outcomes in the literature.\nFor example, one of the studies reported an analysis of school food handler knowledge outcomes, with randomization conducted at the school level. There were 33 food handlers in the intervention group and 46 in the control group, with 8 schools in each group. Given these data, the design effect (DEFF) was calculated as:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n9.4.2 Multiple Outcomes in the Same Study\nIf a study reports multiple outcomes of the same type, such that they would be included in the same meta-analysis, the outcomes would not be independent and this extra level of variation should be accounted for. This could be due to multiple sub-groups of populations in the same study (hierarchical clustering) or multiple measurements on the same participants in the same study (correlated effects). The most common approaches to address this are as follows:\n\nAveraging of effects approach: in this method, we take a weighted average of the multiple similar outcomes within a study, to produce one estimate per study per meta-analysis. For correlated effects, this requires an estimate of the correlation between the two or more outcome measurements being combined.\nThree-level regression model: this method is appropriate for hierarchical clustering only, not correlated effects. It adds another random-effect for the outcomes being clustered within studies.\nRobust variance estimation: uses a special method of calculating the standard error that is consistent even when assumptions (e.g., independence of observations) are not met.\nCorrelated and hierarchical effects models: use the robust variance approach along with multilevel modelling to account for both correlated and hierarchical effects in the same model.\n\nAdditional reading on these approaches and how to implement them in R is available in the Doing Meta-Anlaysis in R online book. Note that these advanced methods may not be reliable when the number of studies included in the analysis is very small.\n\n\n9.4.3 Multiple Comparison Groups\nIf a study reports a comparison of two or more intervention (or exposure) groups to a control or comparison group, we need to avoid double-counting those outcomes in the analysis. For example, a study might report separate comparison data for different genders or age groups, or different variations (e.g., doses) of an intervention. There are different approaches to address this issue:\n\nPick the most relevant of the intervention/exposure groups and exclude the others. However, the decision of what is most relevant must be justified and this results in a loss of data.\nSplit the sample size of the common control/comparison group: for example, if we had two variations of an intervention compared to a common control group with 200 participants, we would split the control group into two groups of size 100. However, this approach still results in the outcomes being correlated.\nCombine the two (or more) intervention/exposure groups into a weighted average. This is the preferred approach and is discussed and illustrated below.\nIf there are many studies with multiple comparison groups, consider a network meta-analysis.\n\nTo combine two groups together, the sample sizes for the two groups can simply be added together. The means and SDs can be combined with the following formulas:\n\\[\n\\text{Mean} = \\frac{{N_1}{Mean_1}+{N_2}{Mean_2}}{{N_1}+{N_2}}\n\\]\n\\[\n\\text{SD} = \\sqrt\\frac{{({N_1}-1){SD^2_1}+({N_2}-1){SD^2_2}+\n\\frac{{N_1}{N_2}}{{N_1}+{N_2}}}{({Mean^2_1}+{Mean^2_2}-2{Mean_1}{Mean_2})}}\n{{N_1}+{N_2}-1}\n\\]\nThankfully, we can use an R function to automate this calculation.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUsing our new pool.groups function, we can now combined two groups. Suppose we had two similar interventions in one study with the following parameters: sample size of 50 in each group, mean 1 = 2.0, SD 1 = 1.5, mean 2 = 2.5, SD 2 = 2.3):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf combining more than two groups, we would first combine two groups together, then combine the results of that new combined group with the third group, and so on.\n\n\n\n\n\n\nCombining Groups Example\n\n\n\nIn practice, if we will be combining interventions groups like this, we will want to create a new dataframe to analyze with the combined groups. The saved combined values can then be inserted into the new dataframe. For example:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn this example, we want to combine the two training interventions in Study 3. We use the pool.groups function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can now create a new version of the dataframe, remove one of the treatment rows, and enter the new combined values in place of the previous intervention values.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nSterne, J A C, A J Sutton, J P Ioannidis, N Terrin, D R Jones, J Lau, J Carpenter, et al. 2011. “Recommendations for Examining and Interpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised Controlled Trials.” BMJ (Clinical Research Ed.) 343: d4002. http://www.scopus.com/inward/record.url?eid=2-s2.0-79961238388&partnerID=40&md5=116d9b214a2fee3add89b254255f8cde.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Subgroup Analysis, Meta-Regression, and Complex Data</span>"
    ]
  },
  {
    "objectID": "10_grade.html",
    "href": "10_grade.html",
    "title": "10  Quality of Evidence and Reporting",
    "section": "",
    "text": "10.1 Summary of Findings Tables and Review Reporting\nSummary of findings tables are required for all reviews conducted through the Cochrane Collaboration. They present the main findings of the review in a user-friendly, tabular format, and appear as part of the executive summary of the review report. The concept of summary of findings tables applied to any review, and such a table can be included in non-Cochrane reviews as well. The Cochrane Collaboration requires that summary of findings tables include only the most important outcomes of the review, limited to no more than seven.\nThe general template for a summary of findings table is as follows:\nThe Cochrane Collaboration has created an online software called GRADEpro GDT to create summary of findings tables. The software is currently free for groups of up to three researchers.\nEach row in a summary of findings table reflects one of the pre-selected, most important outcomes (up to seven). For each outcome, you should provide the measurement scale (if applicable) and the measurement time frame (e.g., length of follow-up). Outcomes that were pre-selected but that had no data available from the studies in the review should still be included in this table.\nThe first column in the table will be the list of outcomes. The second column is the comparative risk in each group. Then additional columns are provided for the relative effect and CI (e.g., RR, OR), the number of participants and studies included for each outcome, the GRADE rating, and any comments.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality of Evidence and Reporting</span>"
    ]
  },
  {
    "objectID": "10_grade.html#summary-of-findings-tables-and-review-reporting",
    "href": "10_grade.html#summary-of-findings-tables-and-review-reporting",
    "title": "10  Quality of Evidence and Reporting",
    "section": "",
    "text": "Description of population and setting of available evidence and the comparison groups (at the top of the table).\nList of the most important outcomes (as separate rows in the table).\nFor each outcome the table, a measure of the typical burden, relative magnitude of effect, number of participants and studies, and certainty of evidence (as separate column for each outcome).\nThe final column includes overall comments, and explanations (footnotes) should be added as needed at the bottom of the table.\n\n\n\n\n\n\n\n\n\n\nSummary of Findings Table Examples\n\n\n\nAn example of a summary of findings table from a systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). In this review, separate summary of findings tables were created for evidence in two main population groups (adult cancer patients and pediatric cancer patients). The example below shows the first three outcomes from the adult cancer patient table.\n\nIn this table, footnote 1 indicates that all outcomes were downgraded two levels in the GRADE assessment for high risk of bias (because participants could not be blinded to the music intervention and the outcomes were measured through self-reporting). The outcomes shown above were also downgraded 1-2 levels due to inconsistency because of high levels of heterogeneity in estimates.\nAnother example of a summary of findings table showing illustrative comparative risks is provided below, from a systematic review about the effectiveness of nicotine replacement therapy vs. control for smoking cessation (Hartmann‐Boyce et al. 2018).\n\nThe footnotes in this table indicated that most studies were “judged to be at unclear or high risk of bias, but restricting to only studies at low risk of bias did not significantly alter the effect”. Additionally, while some publication bias was expected, results were not likely to change significantly due to this.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality of Evidence and Reporting</span>"
    ]
  },
  {
    "objectID": "10_grade.html#certainty-of-evidence-assessment",
    "href": "10_grade.html#certainty-of-evidence-assessment",
    "title": "10  Quality of Evidence and Reporting",
    "section": "10.2 Certainty of Evidence Assessment",
    "text": "10.2 Certainty of Evidence Assessment\nThe Grades of Recommendation, Assessment, Development and Evaluation Working Group has developed a certainty of evidence assessment approach for systematic reviews called GRADE, which has been adopted by the Cochrane Collaboration. GRADE is applied to each outcome in a systematic review, and determines the level of confidence that the calculated measure of effect or association (from meta-analysis) represents the true effect. There are four possible GRADE ratings: high, moderate, low, and very low.\nStudies begin at a high rating and can be rated down based on five criteria. In intervention reviews, non-randomized studies are typically rated down to a low rating automatically due to risks of bias. There are also three possible criteria that can increase the GRADE rating. The criteria are described below.\n\n10.2.1 Risk of bias\nThe risk of bias of individual studies contributing to a specific outcome can affect our confidence in that outcome. This criterion uses the risk-of-bias judgement from the Cochrane RoB tool for RCTs or from the ROBINS-I/ROBINS-E tool for observational studies (as applicable). When most of the evidence comes from studies that have a crucial limitation (high risk of bias) for one risk-of-bias item, or some concerns for multiple items, the certainty of evidence can be downgraded by one level. In severe cases, certainty can be downgraded by two levels.\n\n\n10.2.2 Inconsistency\nThis criterion aims to assess the extent of inconsistency or heterogeneity in the estimate of effect that is unexplained after investigating subgroup and meta-regression analyses. Inconsistency can be measured using \\(I^2\\), \\(\\tau\\), and/or prediction intervals, as previously covered in the Chapter 8 session. If heterogeneity in the estimate is considered substantial or important, and there are no identified explanations for the heterogeneity, then consider downgrading by one level.\n\n\n10.2.3 Indirectness\nThis criterion assesses whether the studies contributing evidence to a particular outcome do not directly match with the review question’s population, intervention/exposure, comparison, or outcome. For example, if in a systematic review about the effectiveness of food handler education and training interventions (Young et al. 2019), studies were considered to provide indirect evidence on the population if their participants included a mix of food handlers (working in the food service industry) and consumers who cook food at home. The same principle applies to the other PICO/PECO elements. It might be appropriate to downgrade if most of the evidence for an outcome has indirect measurement of one or more PICO/PECO elements.\n\n\n10.2.4 Imprecision\nIf there is sufficient uncertainty about the magnitude and direction of the meta-analysis estimate, one can rate downgrade for imprecision. In the most recent suggested approach to this criterion (Schünemann et al. 2022), it is recommended to pre-identify key thresholds for what is a considered a small, moderate, or large effect for the outcome of interest. This imprecision assessment and the threshold should be on the absolute scale, so should use the corresponding intervention risk. Then determine if you expect the meta-analysis effect will lie between two thresholds or beyond a threshold (e.g., small or greater). This is the target rating of the uncertainty.\nThe next step is to assess whether the CI for the corresponding intervention risk crosses one or more threshold boundaries, and downgrade a number of levels for each threshold crossed. For example, if we set our target at detecting a small or greater effect, and the effect was moderate but the CI also included the null (zero effect), evidence would be downgraded one level for crossing below the “small” threshold.\nFor continuous outcomes, one can use the well-defined SMD thresholds of small = 0.2, moderate = 0.5, and large = 0.8 (Schünemann et al. 2022). If analyzing a raw mean difference, one should use established thresholds for the outcome scale (if there are any), otherwise you can re-express the outcome as a SMD for the purposes of conducting the imprecision rating.\nNote that when conducting random-effects meta-analysis, high heterogeneity can also cause imprecision (wide CIs), so authors may need to consider this and may decide to rate down only for inconsistency and not for imprecision (especially if the total number of participants across included studies is reasonably large). For example, to detect a SMD of at least 0.2, a sufficient sample size will often be 800 (400 per comparison group) (Schünemann et al. 2022).\n\n\n10.2.5 Publication bias\nThe final downgrading criterion relates to possible publication bias. This criterion can be informed by a separate risk-of-bias assessment for missing data, as described in Chapter 5, to determine the extent that additional studies might have been excluded from the meta-analysis because they were not published. Evaluation of meta-analysis funnel plots and publication bias tests (if appropriate) can also inform this criterion.\n\n\n10.2.6 Upgrading criteria\nThere are three possible criteria that could lead to increasing the GRADE rating for an outcome: large effect, dose response, or all plausible confounding and bias would reduce (rather than increase) the effect. However, it is rare that the evidence will meet the necessary criteria to be upgraded based on any of these factors.\nLarge effects: If evidence predominantly comes from well-conducted, low risk of bias studies and the effect estimate is large (e.g., RR&gt;2 or &lt;0.5), one could consider upgrading.\nDose-response: The presence of a dose-response gradient for the effect can increase confidence and certainty in the effect, potentially warranting upgrading.\nPlausible confounding: Sometimes all plausible confounding factors would be expected to under-estimate the apparent effect estimate, which increases our confidence that the actual effect should be larger than what is reported.\n\n\n\n\n\n\nGRADE Example\n\n\n\nAn example of GRADE rating from a summary-of-findings table in systematic review of music interventions to improve various health outcomes in people with cancer is shown below (Bradt et al. 2021). This GRADE rating and explanation is shown for effects of the intervention on anxiety levels in pediatric patients.\n\nIn this GRADE assessment, footnote 1 indicates the evidence was downgraded two levels for high risk of bias. The two trials were at high risk of bias because participants could not be blinded to the music intervention and the outcome was measured through self-reporting. Footnote 2 indicates that evidence was downgraded one level for serious inconsistency across studies as evidenced by \\(I^2 = 76\\%\\). Footnote 3 indicates that evidence was downgraded two levels for imprecision due to a small number of participants (from only two trials).\n\n\n\n\n\n\n\n\nGRADE Exercise\n\n\n\nUsing the systematic review of the association between long-term exposure to residential green spaces and mortality in adults (Rojas-Rueda et al. 2019), as investigated in an earlier exercise, we will now attempt to conduct a GRADE evaluation of the evidence contributing to the main all-cause mortality outcome.\n\nReview the article, including tables and figures, and come up with a proposed GRADE assessment for each of the five downgrading criteria. Note that you will need to examine the supplementary material for detailed results of the risk-of-bias and publication bias assessments.\nBased on the information available, would you consider upgrading any levels based on the upgrading criteria?\nWhat is your proposed overall GRADE rating for this outcome?",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality of Evidence and Reporting</span>"
    ]
  },
  {
    "objectID": "10_grade.html#certainty-of-evidence-assessment-qualitative-reviews",
    "href": "10_grade.html#certainty-of-evidence-assessment-qualitative-reviews",
    "title": "10  Quality of Evidence and Reporting",
    "section": "10.3 Certainty of Evidence Assessment: Qualitative Reviews",
    "text": "10.3 Certainty of Evidence Assessment: Qualitative Reviews\nA qualitative research subgroup of the GRADE Working Group has developed a certainty of evidence assessment approach for systematic reviews of qualitative evidence called GRADE-CERQual. CERQual stands for Confidence in the Evidence from Reviews of Qualitative research. The GRADE-CERQual approach and process mimics the GRADE approach described above, but tailored to qualitative research evidence, and has been described in a series of publications in the Implementation Science journal.\nGRADE-CERQual is based on consideration and assessment of four components for each main finding in a review (Lewin et al. 2018):\n\nMethodological Limitations: is based on a critical appraisal of individual studies reporting on the review finding using a standard tool. A specific tool has been developed for this purpose called: CochrAne qualitative Methodological LimitatiOns Tool (CAMELOT) (Munthe-Kaas et al. 2024).\nCoherence: assesses how clear and coherent the fit is between the data reported in each study and the review finding. Reviewers will examine for possible contradictions or outlying findings in the study data.\nAdequacy of Data: assesses the richness and quantity of data supporting a review finding. Richness refers to the amount of detail used to describe individual study results, while quantity of data refers to the number and size of studies contributing to a finding.\nRelevance: refers to the extent of applicability of the primary studies supporting a review finding to the target context.\n\nA fifth component called Dissemination/Publication Bias may also be considered. Based these assessments, confidence in each review finding is then rated as: high, moderate, low, or very low. Results are then used to make a Summary of Qualitative Findings table.\n\n\n\n\n\n\nGRADE-CERQual Example\n\n\n\nAn example of GRADE-CERQual assessment and Summary of Qualitative Findings table is shown below from a qualitative systematic review of barriers and facilitators to implementing workplace interventions for promoting mental health (Paterson et al. 2024). An exert of the Summary of Qualitative Findings table from the review is shown below, highlighting the CERQual ratings and explanations for four of the review’s key findings.\n\nIn this table, the authors have provided separate GRADE-CERQual assessments for specific review findings for each of their pre-determined research questions. The authors noted that one reviewer conducted the assessments, which were reviewed for consistency by a second reviewer. Each finding was classified as low, moderate, or high confidence. Detailed results of the assessment are shown in the article’s Additional file 11.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality of Evidence and Reporting</span>"
    ]
  },
  {
    "objectID": "10_grade.html#critical-appraisal-of-systematic-reviews",
    "href": "10_grade.html#critical-appraisal-of-systematic-reviews",
    "title": "10  Quality of Evidence and Reporting",
    "section": "10.4 Critical Appraisal of Systematic Reviews",
    "text": "10.4 Critical Appraisal of Systematic Reviews\nThe risk of bias tools discussed in Chapter 5 are designed for individual studies within a systematic review, while the PRISMA reporting guideline is primarily meant to assess the completeness and transparency of reporting of a systematic review to allow reproducibility. To critically appraise published systematic reviews, there are a few different tools that can be used:\n\nAMSTAR 2: contains 16 individual assessment items, which contribute to an overall rating of confidence in the results of the review (critically low, low, moderate, or high) (Shea et al. 2017).\nROBIS: designed to assess risk of bias in systematic reviews through three phases (identifying relevance, identifying concerns with the review process, and judging risk of bias). The second phase covers numerous questions across four domains: study eligibility criteria, identification and selection of studies, data collection and study appraisal, and synthesis and findings (Whiting et al. 2016). The overall risk of bias is then determined as low, high, or unclear.\nJBI Systematic Review Checklist: contains 10 key questions to assess risk of bias in systematic reviews (Aromataris et al. 2015).\n\nRecent studies have compared the AMSTAR 2 and ROBIS tools, finding that they are both valid, moderately reliable, and comparable (Lorenz et al. 2019; Perry et al. 2021). However, AMSTAR 2 is more straightforward and easier to use.\n\n\n\n\n\n\nCritical Appraisal Exercise\n\n\n\nUsing the AMSTAR 2 Checklist, practice conducting a critical appraisal of the systematic review of the association between long-term exposure to residential green spaces and mortality in adults (Rojas-Rueda et al. 2019).\n\nReview the article, including tables and figures, including the supplementary material (as needed).\nBased on the information available, how would you answer each of the 16 assessment items?\nWere any items not met, and if so, why not?\n\n\n\n\n\n\n\nAromataris, Edoardo, Ritin Fernandez, Christina M. Godfrey, Cheryl Holly, Hanan Khalil, and Patraporn Tungpunkom. 2015. “Summarizing Systematic Reviews: Methodological Development, Conduct and Reporting of an Umbrella Review Approach.” International Journal of Evidence-Based Healthcare 13 (3): 132–40. https://doi.org/10.1097/XEB.0000000000000055.\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo. 2021. “Music Interventions for Improving Psychological and Physical Outcomes in People with Cancer.” Cochrane Database of Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and Tim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control for Smoking Cessation.” Cochrane Database of Systematic Reviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nLewin, Simon, Andrew Booth, Claire Glenton, Heather Munthe-Kaas, Arash Rashidian, Megan Wainwright, Meghan A. Bohren, et al. 2018. “Applying GRADE-CERQual to Qualitative Evidence Synthesis Findings: Introduction to the Series.” Implementation Science 13 (S1): 2. https://doi.org/10.1186/s13012-017-0688-3.\n\n\nLorenz, Robert C., Katja Matthias, Dawid Pieper, Uta Wegewitz, Johannes Morche, Marc Nocon, Olesja Rissling, Jacqueline Schirm, and Anja Jacobs. 2019. “A Psychometric Study Found AMSTAR 2 to Be a Valid and Moderately Reliable Appraisal Tool.” Journal of Clinical Epidemiology 114 (October): 133–40. https://doi.org/10.1016/j.jclinepi.2019.05.028.\n\n\nMunthe-Kaas, Heather M., Andrew Booth, Isolde Sommer, Sara Cooper, Ruth Garside, Karin Hannes, Jane Noyes, and The CAMELOT Development Group. 2024. “Developing CAMELOT for Assessing Methodological Limitations of Qualitative Research for Inclusion in Qualitative Evidence Syntheses.” Cochrane Evidence Synthesis and Methods 2 (6): e12058. https://doi.org/10.1002/cesm.12058.\n\n\nPaterson, Charlotte, Caleb Leduc, Margaret Maxwell, Birgit Aust, Heather Strachan, Ainslie O’Connor, Fotini Tsantila, et al. 2024. “Barriers and Facilitators to Implementing Workplace Interventions to Promote Mental Health: Qualitative Evidence Synthesis.” Systematic Reviews 13 (1, 1): 1–24. https://doi.org/10.1186/s13643-024-02569-2.\n\n\nPerry, R., A. Whitmarsh, V. Leach, and P. Davies. 2021. “A Comparison of Two Assessment Tools Used in Overviews of Systematic Reviews: ROBIS Versus AMSTAR-2.” Systematic Reviews 10 (1): 273. https://doi.org/10.1186/s13643-021-01819-x.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela Perez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality: A Systematic Review and Meta-Analysis of Cohort Studies.” The Lancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSchünemann, Holger J., Ignacio Neumann, Monica Hultcrantz, Romina Brignardello-Petersen, Linan Zeng, M. Hassan Murad, Ariel Izcovich, et al. 2022. “GRADE Guidance 35: Update on Rating Imprecision for Assessing Contextualized Certainty of Evidence and Making Decisions.” Journal of Clinical Epidemiology 150 (October): 225–42. https://doi.org/10.1016/j.jclinepi.2022.07.015.\n\n\nShea, Beverley J., Barnaby C. Reeves, George Wells, Micere Thuku, Candyce Hamel, Julian Moran, David Moher, et al. 2017. “AMSTAR 2: A Critical Appraisal Tool for Systematic Reviews That Include Randomised or Non-Randomised Studies of Healthcare Interventions, or Both.” BMJ 358 (September): j4008. https://doi.org/10.1136/bmj.j4008.\n\n\nWhiting, Penny, Jelena Savović, Julian P. T. Higgins, Deborah M. Caldwell, Barnaby C. Reeves, Beverley Shea, Philippa Davies, Jos Kleijnen, and Rachel Churchill. 2016. “ROBIS: A New Tool to Assess Risk of Bias in Systematic Reviews Was Developed.” Journal of Clinical Epidemiology 69: 225–34. https://doi.org/10.1016/j.jclinepi.2015.06.005.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019. “Effectiveness of Food Handler Training and Education Interventions: A Systematic Review and Meta-Analysis.” Journal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality of Evidence and Reporting</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html",
    "href": "meta_analysis.html",
    "title": "11  Meta-Analysis Practice",
    "section": "",
    "text": "11.1 Import the Dataset\nAs a first step, we will import the dataset and examine it.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#import-the-dataset",
    "href": "meta_analysis.html#import-the-dataset",
    "title": "11  Meta-Analysis Practice",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#conduct-the-meta-analysis",
    "href": "meta_analysis.html#conduct-the-meta-analysis",
    "title": "11  Meta-Analysis Practice",
    "section": "11.2 Conduct the Meta-analysis",
    "text": "11.2 Conduct the Meta-analysis\nNow we will conduct a meta-analysis on a selected outcome. For this example, we can examine the depression outcome.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#produce-a-forest-plot",
    "href": "meta_analysis.html#produce-a-forest-plot",
    "title": "11  Meta-Analysis Practice",
    "section": "11.3 Produce a Forest Plot",
    "text": "11.3 Produce a Forest Plot\nThe following coding template can be used for a forest plot figure. Add some customization options to make it look unique and appealing. See the Chapter 8 for more details.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#conduct-sensitivity-analysis",
    "href": "meta_analysis.html#conduct-sensitivity-analysis",
    "title": "11  Meta-Analysis Practice",
    "section": "11.4 Conduct Sensitivity Analysis",
    "text": "11.4 Conduct Sensitivity Analysis\nReproduce the meta-analysis using a different method of calculating \\(\\tau^2\\) and compare the results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#conduct-a-subgroup-analysis",
    "href": "meta_analysis.html#conduct-a-subgroup-analysis",
    "title": "11  Meta-Analysis Practice",
    "section": "11.5 Conduct a Subgroup Analysis",
    "text": "11.5 Conduct a Subgroup Analysis\nNow try filtering the analysis to two or three different health/wellbeing outcomes and conduct a subgroup analysis on the outcome type. How does this change the results?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#conduct-a-meta-regression",
    "href": "meta_analysis.html#conduct-a-meta-regression",
    "title": "11  Meta-Analysis Practice",
    "section": "11.6 Conduct a Meta-Regression",
    "text": "11.6 Conduct a Meta-Regression\nNow try conducting a meta-regression on two specific outcomes, depression and stress, to see how the intervention type variable (therapy or non-therapy) affected the results across studies. Try again using the comparison type variable instead.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "meta_analysis.html#conduct-a-publication-bias-assessment",
    "href": "meta_analysis.html#conduct-a-publication-bias-assessment",
    "title": "11  Meta-Analysis Practice",
    "section": "11.7 Conduct a Publication Bias Assessment",
    "text": "11.7 Conduct a Publication Bias Assessment\nNow, using the same meta-analysis of the depression and stress outcomes combined, try producing a funnel plot. Additionally, conduct a publication bias test using the Pustejovsky option (given the SMD measure being used). Is there evidence of possible publication bias?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nNow conduct a trim-and-fill analysis to see the impact of possible publication bias.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nSoga, Masashi, Kevin J. Gaston, and Yuichi Yamaura. 2017. “Gardening Is Beneficial for Health: A Meta-Analysis.” Preventive Medicine Reports 5 (March): 92–99. https://doi.org/10.1016/j.pmedr.2016.11.007.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Meta-Analysis Practice</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "12  References",
    "section": "",
    "text": "Aromataris, Edoardo, Ritin Fernandez, Christina M. Godfrey, Cheryl\nHolly, Hanan Khalil, and Patraporn Tungpunkom. 2015. “Summarizing\nSystematic Reviews: Methodological Development, Conduct and Reporting of\nan Umbrella Review Approach.” International Journal of\nEvidence-Based Healthcare 13 (3): 132–40. https://doi.org/10.1097/XEB.0000000000000055.\n\n\nBarnett-Page, Elaine, and James Thomas. 2009. “Methods for the\nSynthesis of Qualitative Research: A Critical Review.” BMC\nMedical Research Methodology 9 (1): 59. http://www.biomedcentral.com/1471-2288/9/59.\n\n\nBartlett, Larissa, Angela Martin, Amanda L. Neil, Kate Memish, Petr\nOtahal, Michelle Kilpatrick, and Kristy Sanderson. 2019. “A\nSystematic Review and Meta-Analysis of Workplace Mindfulness Training\nRandomized Controlled Trials.” Journal of Occupational Health\nPsychology 24: 108–26. https://doi.org/10.1037/ocp0000146.\n\n\nBradt, Joke, Cheryl Dileo, Katherine Myers-Coffman, and Jacelyn Biondo.\n2021. “Music Interventions for Improving Psychological and\nPhysical Outcomes in People with Cancer.” Cochrane Database\nof Systematic Reviews, no. 10. https://doi.org/10.1002/14651858.CD006911.pub4.\n\n\nEhrlich, Rodney, Paula Akugizibwe, Nandi Siegfried, and David Rees.\n2021. “The Association Between Silica Exposure, Silicosis and\nTuberculosis: A Systematic Review and Meta-Analysis.” BMC\nPublic Health 21 (1): 953. https://doi.org/10.1186/s12889-021-10711-1.\n\n\nHamel, C., M. Hersi, S. E. Kelly, A. C. Tricco, Sharon Straus, G. Wells,\nB. Pham, and B. Hutton. 2021. “Guidance for Using Artificial\nIntelligence for Title and Abstract Screening While Conducting Knowledge\nSyntheses.” BMC Medical Research Methodology 21 (1):\n285. https://doi.org/10.1186/s12874-021-01451-2.\n\n\nHamel, C., S. E. Kelly, K. Thavorn, D. B. Rice, G. A. Wells, and B.\nHutton. 2020. “An Evaluation of DistillerSR’s Machine\nLearning-Based Prioritization Tool for Title/Abstract Screening – Impact\non Reviewer-Relevant Outcomes.” BMC Medical Research\nMethodology 20 (1): 256. https://doi.org/10.1186/s12874-020-01129-1.\n\n\nHargreaves, Sally, Kieran Rustage, Laura B Nellums, Alys McAlpine,\nNicola Pocock, Delan Devakumar, Robert W Aldridge, et al. 2019.\n“Occupational Health Outcomes Among International Migrant Workers:\nA Systematic Review and Meta-Analysis.” The Lancet Global\nHealth 7 (7): e872–82. https://doi.org/10.1016/S2214-109X(19)30204-9.\n\n\nHartmann‐Boyce, Jamie, Samantha C. Chepkin, Weiyu Ye, Chris Bullen, and\nTim Lancaster. 2018. “Nicotine Replacement Therapy Versus Control\nfor Smoking Cessation.” Cochrane Database of Systematic\nReviews, no. 5. https://doi.org/10.1002/14651858.CD000146.pub5.\n\n\nHiggins, J P T, S G Thompson, J J Deeks, and D G Altman. 2003.\n“Measuring Inconsistency in Meta-Analyses.” BMJ\n(Clinical Research Ed.) 327 (7414): 557–60. https://doi.org/10.1136/bmj.327.7414.557.\n\n\nHiggins, JPT, J Thomas, J Chandler, M Cumpston, T Li, MJ Page, and VA\nWelch, eds. 2022. Cochrane Handbook for\nSystematic Reviews of Interventions.\nCochrane. www.training.cochrane.org/handbook.\n\n\nJarach, Carlotta M., Alessandra Lugo, Marco Scala, Piet A. van den\nBrandt, Christopher R. Cederroth, Anna Odone, Werner Garavello, Winfried\nSchlee, Berthold Langguth, and Silvano Gallus. 2022. “Global\nPrevalence and Incidence of\nTinnitus: A Systematic Review and Meta-analysis.” JAMA Neurology 79\n(9): 888–900. https://doi.org/10.1001/jamaneurol.2022.2189.\n\n\nLangan, Dean, Julian P. T. Higgins, Dan Jackson, Jack Bowden, Areti\nAngeliki Veroniki, Evangelos Kontopantelis, Wolfgang Viechtbauer, and\nMark Simmonds. 2019. “A Comparison of Heterogeneity Variance\nEstimators in Simulated Random-Effects Meta-Analyses.”\nResearch Synthesis Methods 10 (1): 83–98. https://doi.org/10.1002/jrsm.1316.\n\n\nLawn, Sharon, Louise Roberts, Eileen Willis, Leah Couzner, Leila\nMohammadi, and Elizabeth Goble. 2020. “The Effects of Emergency\nMedical Service Work on the Psychological, Physical, and Social\nWell-Being of Ambulance Personnel: A Systematic Review of Qualitative\nResearch.” BMC Psychiatry 20 (1): 348. https://doi.org/10.1186/s12888-020-02752-4.\n\n\nLewin, Simon, Andrew Booth, Claire Glenton, Heather Munthe-Kaas, Arash\nRashidian, Megan Wainwright, Meghan A. Bohren, et al. 2018.\n“Applying GRADE-CERQual to Qualitative Evidence\nSynthesis Findings: Introduction to the Series.”\nImplementation Science 13 (S1): 2. https://doi.org/10.1186/s13012-017-0688-3.\n\n\nLockwood, Craig, Zachary Munn, and Kylie Porritt. 2015.\n“Qualitative Research Synthesis: Methodological Guidance for\nSystematic Reviewers Utilizing Meta-Aggregation.” JBI\nEvidence Implementation 13 (3): 179. https://doi.org/10.1097/XEB.0000000000000062.\n\n\nLorenz, Robert C., Katja Matthias, Dawid Pieper, Uta Wegewitz, Johannes\nMorche, Marc Nocon, Olesja Rissling, Jacqueline Schirm, and Anja Jacobs.\n2019. “A Psychometric Study Found AMSTAR 2 to Be a\nValid and Moderately Reliable Appraisal Tool.” Journal of\nClinical Epidemiology 114 (October): 133–40. https://doi.org/10.1016/j.jclinepi.2019.05.028.\n\n\nMoher, David, Larissa Shamseer, Mike Clarke, Davina Ghersi, Alessandro\nLiberati, Mark Petticrew, Paul Shekelle, and Lesley A Stewart. 2015.\n“Preferred Reporting Items for Systematic Review and Meta-Analysis\nProtocols (PRISMA-P) 2015 Statement.” Systematic\nReviews 4 (1): 1. https://doi.org/10.1186/2046-4053-4-1.\n\n\nMunn, Zachary, Sandeep Moola, Karolina Lisy, Dagmara Riitano, and\nCatalin Tufanaru. 2015. “Methodological Guidance for Systematic\nReviews of Observational Epidemiological Studies Reporting Prevalence\nand Cumulative Incidence Data.” International Journal of\nEvidence-Based Healthcare 13 (3): 147–53. https://doi.org/10.1097/XEB.0000000000000054.\n\n\nMunn, Zachary, Cindy Stern, Edoardo Aromataris, Craig Lockwood, and Zoe\nJordan. 2018. “What Kind of Systematic Review Should\nI Conduct? A Proposed Typology and Guidance\nfor Systematic Reviewers in the Medical and Health Sciences.”\nBMC Medical Research Methodology 18 (1): 5. https://doi.org/10.1186/s12874-017-0468-4.\n\n\nMunthe-Kaas, Heather M., Andrew Booth, Isolde Sommer, Sara Cooper, Ruth\nGarside, Karin Hannes, Jane Noyes, and The CAMELOT Development Group.\n2024. “Developing CAMELOT for Assessing\nMethodological Limitations of Qualitative Research for Inclusion in\nQualitative Evidence Syntheses.” Cochrane Evidence Synthesis\nand Methods 2 (6): e12058. https://doi.org/10.1002/cesm.12058.\n\n\nPage, Matthew J., Jonathan A. C. Sterne, Isabelle Boutron, Asbjørn\nHróbjartsson, Jamie J. Kirkham, Tianjing Li, Andreas Lundh, et al. 2023.\n“ROB-ME: A Tool for Assessing Risk of Bias Due to\nMissing Evidence in Systematic Reviews with Meta-Analysis.”\nBMJ 383 (November): e076754. https://doi.org/10.1136/bmj-2023-076754.\n\n\nPaterson, Charlotte, Caleb Leduc, Margaret Maxwell, Birgit Aust, Heather\nStrachan, Ainslie O’Connor, Fotini Tsantila, et al. 2024.\n“Barriers and Facilitators to Implementing Workplace Interventions\nto Promote Mental Health: Qualitative Evidence Synthesis.”\nSystematic Reviews 13 (1, 1): 1–24. https://doi.org/10.1186/s13643-024-02569-2.\n\n\nPerry, R., A. Whitmarsh, V. Leach, and P. Davies. 2021. “A\nComparison of Two Assessment Tools Used in Overviews of Systematic\nReviews: ROBIS Versus AMSTAR-2.”\nSystematic Reviews 10 (1): 273. https://doi.org/10.1186/s13643-021-01819-x.\n\n\nRawlings, G. H., R. K. Williams, D. J. Clarke, C. English, C.\nFitzsimons, I. Holloway, R. Lawton, G. Mead, A. Patel, and A. Forster.\n2019. “Exploring Adults’ Experiences of Sedentary Behaviour and\nParticipation in Non-Workplace Interventions Designed to Reduce\nSedentary Behaviour: A Thematic Synthesis of Qualitative\nStudies.” BMC Public Health 19 (1): 1099. https://doi.org/10.1186/s12889-019-7365-1.\n\n\nRojas-Rueda, David, Mark J. Nieuwenhuijsen, Mireia Gascon, Daniela\nPerez-Leon, and Pierpaolo Mudu. 2019. “Green Spaces and Mortality:\nA Systematic Review and Meta-Analysis of Cohort Studies.” The\nLancet Planetary Health 3 (11): e469–77. https://doi.org/10.1016/S2542-5196(19)30215-3.\n\n\nSchünemann, Holger J., Ignacio Neumann, Monica Hultcrantz, Romina\nBrignardello-Petersen, Linan Zeng, M. Hassan Murad, Ariel Izcovich, et\nal. 2022. “GRADE Guidance 35: Update on Rating\nImprecision for Assessing Contextualized Certainty of Evidence and\nMaking Decisions.” Journal of Clinical Epidemiology 150\n(October): 225–42. https://doi.org/10.1016/j.jclinepi.2022.07.015.\n\n\nSchwarzer, Guido, Hiam Chemaitelly, Laith J. Abu-Raddad, and Gerta\nRücker. 2019. “Seriously Misleading Results Using Inverse of\nFreeman-Tukey Double Arcsine Transformation in\nMeta-Analysis of Single Proportions.” Research Synthesis\nMethods 10 (3): 476–83. https://doi.org/10.1002/jrsm.1348.\n\n\nShamseer, Larissa, David Moher, Mike Clarke, Davina Ghersi, Alessandro\nLiberati, Mark Petticrew, Paul Shekelle, et al. 2015. “Preferred\nReporting Items for Systematic Review and Meta-Analysis Protocols\n(Prisma-p) 2015: Elaboration and Explanation.”\nBMJ (Online) 349 (January): 1–25. https://doi.org/10.1136/bmj.g7647.\n\n\nShea, Beverley J., Barnaby C. Reeves, George Wells, Micere Thuku,\nCandyce Hamel, Julian Moran, David Moher, et al. 2017.\n“AMSTAR 2: A Critical Appraisal Tool for Systematic\nReviews That Include Randomised or Non-Randomised Studies of Healthcare\nInterventions, or Both.” BMJ 358 (September): j4008. https://doi.org/10.1136/bmj.j4008.\n\n\nSim, Julius, and Chris C Wright. 2005. “The Kappa\nStatistic in Reliability Studies: Use, Interpretation, and Sample Size\nRequirements.” Physical Therapy 85 (3): 257–68. https://doi.org/10.1093/ptj/85.3.257.\n\n\nSoga, Masashi, Kevin J. Gaston, and Yuichi Yamaura. 2017.\n“Gardening Is Beneficial for Health: A\nMeta-Analysis.” Preventive Medicine Reports 5 (March):\n92–99. https://doi.org/10.1016/j.pmedr.2016.11.007.\n\n\nSterne, J A C, A J Sutton, J P Ioannidis, N Terrin, D R Jones, J Lau, J\nCarpenter, et al. 2011. “Recommendations for Examining and\nInterpreting Funnel Plot Asymmetry in Meta-Analyses of Randomised\nControlled Trials.” BMJ (Clinical Research Ed.) 343:\nd4002. http://www.scopus.com/inward/record.url?eid=2-s2.0-79961238388&partnerID=40&md5=116d9b214a2fee3add89b254255f8cde.\n\n\nSterne, Jonathan A. C., Jelena Savović, Matthew J. Page, Roy G. Elbers,\nNatalie S. Blencowe, Isabelle Boutron, Christopher J. Cates, et al.\n2019. “RoB 2: A Revised Tool for\nAssessing Risk of Bias in Randomised Trials.” The BMJ\n366. https://doi.org/10.1136/bmj.l4898.\n\n\nThomas, J, and A Harden. 2008. “Methods for the Thematic Synthesis\nof Qualitative Research in Systematic Reviews.” BMC Medical\nResearch Methodology 8 (July): 45. https://doi.org/10.1186/1471-2288-8-45.\n\n\nTricco, A C, J Tetzlaff, and D Moher. 2011. “The Art and Science\nof Knowledge Synthesis.” Journal of Clinical\nEpidemiology 64 (1): 11–20. http://www.scopus.com/inward/record.url?eid=2-s2.0-78149388685&partnerID=40&md5=cff0dc293630269631887e1b25835cd7.\n\n\nTsou, Amy Y., Jonathan R. Treadwell, Eileen Erinoff, and Karen\nSchoelles. 2020. “Machine Learning for Screening Prioritization in\nSystematic Reviews: Comparative Performance of Abstrackr\nand EPPI-Reviewer.” Systematic Reviews 9\n(1): 73. https://doi.org/10.1186/s13643-020-01324-7.\n\n\nVeroniki, Areti Angeliki, Dan Jackson, Wolfgang Viechtbauer, Ralf\nBender, Jack Bowden, Guido Knapp, Oliver Kuss, Julian P. T. Higgins,\nDean Langan, and Georgia Salanti. 2016. “Methods to Estimate the\nBetween-Study Variance and Its Uncertainty in Meta-Analysis.”\nResearch Synthesis Methods 7 (1): 55–79. https://doi.org/10.1002/jrsm.1164.\n\n\nWhiting, Penny, Jelena Savović, Julian P. T. Higgins, Deborah M.\nCaldwell, Barnaby C. Reeves, Beverley Shea, Philippa Davies, Jos\nKleijnen, and Rachel Churchill. 2016. “ROBIS:\nA New Tool to Assess Risk of Bias in Systematic Reviews Was\nDeveloped.” Journal of Clinical Epidemiology 69: 225–34.\nhttps://doi.org/10.1016/j.jclinepi.2015.06.005.\n\n\nWilker, Elissa H., Marwa Osman, and Marc G. Weisskopf. 2023.\n“Ambient Air Pollution and Clinical Dementia: Systematic Review\nand Meta-Analysis.” BMJ 381 (April): e071620. https://doi.org/10.1136/bmj-2022-071620.\n\n\nYoung, Ian, Judy Greig, Barbara J. Wilhelm, and Lisa A. Waddell. 2019.\n“Effectiveness of Food Handler Training and Education\nInterventions: A Systematic Review and Meta-Analysis.”\nJournal of Food Protection 82 (10): 1714–28. https://doi.org/10.4315/0362-028X.JFP-19-108.\n\n\nYoung, Ian, and Abhinand Thaivalappil. 2018. “A Systematic Review\nand Meta-Regression of the Knowledge, Practices, and Training of\nRestaurant and Food Service Personnel Toward Food Allergies and\nCeliac Disease.” Edited by Louise Emilsson. PLOS\nONE 13 (9): e0203496. https://doi.org/10.1371/journal.pone.0203496.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>References</span>"
    ]
  }
]