---
webr:
  packages: ['irr', 'tibble']
---

# Selecting and Summarizing Studies {#sec-selecting}

A structured, reliable, and transparent approach is needed to select articles for inclusion in a systematic review. The first step is selecting from the references identified in the search is to conduct a formal **relevance screening** of the titles and abstracts. A second level of screening can be conducted if needed following this on the full-text articles, or an **article characterization** stage that confirms relevance and extracts study details. Results of this process should be document in a **review flow chart** and descriptively summarized. 

![](images/meme_rs.jpg)

::: callout-important
## Multiple reviewers

The relevance screening, article characterization, data extraction, and risk-of-bias assessment steps should be conducted by **two independent reviewers** to reduce bias and the chance of human errors.
:::

## Relevance Screening

A relevance screening form should be developed, pilot tested, and included in the review protocol. It is easiest and most straightforward for this to contain only **one key question** if possible to streamline the process. The form should explicitly identify the eligibility criteria and any key definitions. An example from Young et al. [-@youngEffectivenessFoodHandler2019] is shown below:

![Relevance Screening Form Example](images/rs_form_example.png)

### Pilot Testing

The relevance screening form should be pilot tested before use to ensure the eligibility and inclusion decisions are valid and reliable. The recommended process is as follows:

-   **Purposively select** a small number of abstracts for pre-testing (e.g, 30-50) to ensure the sample includes those known to be relevant, not relevant, and potentially relevant
-   Each reviewer assesses each abstract using the form and records their results
-   Assess agreement in the pilot test using the **kappa statistic**
-   A kappa of \>0.8 is often recommended for "almost perfect agreement" [e.g, @simKappaStatisticReliability2005]

::: callout-note
### Kappa Agreement Example

We will simulate reviewer inclusion/exclusion (i.e., yes/no) ratings for a sample of 30 abstracts. Assume that Reviewer 1 included 50% of the abstracts, including two that Reviewer 2 did not include. Assume Reviewer 2 included 16 abstracts, including three of the abstracts that Reviewer 1 did not include.

```{webr-r}
#| read-only: true
# Create simulated rating data
rs_test <- tibble(abstract_id = seq(1:30),
                  reviewer1 = rep(0:1, each = 15),
                  reviewer2 = c(rep(0, times = 12), rep(1, times = 16), rep(0, times = 2)))

print(rs_test, n = Inf)
```

Now we can evaluate the kappa agreement between the two reviewer's ratings.

```{webr-r}
#| read-only: true
library(irr)

kappa2(rs_test[,2:3])
```

The agreement is moderate, and below an ideal level of 0.8 or higher.

What happens if we edit the simulated data so that this time each reviewer only differs by one abstract each (i.e., there are only two conflicts)?

::: panel-tabset
### Code Editor

```{webr-r}
rs_test2 <- tibble(abstract_id = seq(1:30),
                   reviewer1 = rep(0:1, each = 15),
                   reviewer2 = c(rep(0, times = 12), rep(1, times = 16), rep(0, times = 2)))

print(rs_test2, n = Inf)

kappa2(rs_test2[,2:3])
```

### Answer

```{webr-r}
#| read-only: true
# Create the updated simulated rating data
rs_test2 <- tibble(abstract_id = seq(1:30),
                   reviewer1 = c(rep(0, times = 15), rep(1, times = 15)),
                   reviewer2 = c(rep(0, times = 14), rep(1, times = 15), rep(0, times = 1)))

print(rs_test2, n = Inf)

kappa2(rs_test2[,2:3])
```
:::

You can further modify the simulated dataset using the code above to explore how different patterns of conflicts affect the Kappa measure of agreement.

:::

### Systematic Review Management Software

Systematic management programs, such as **Covidence**, automatically calculate a kappa agreement statistic, along with other inter-rater reliability measures. The relevance screening form in Covidence defaults to having three options: yes, no, or maybe. Answers of *maybe* are considered the same as *yes* for screening and inter-rated reliability calculation purposes. For Covidence, a CSV file can be downloaded to show the screening reliability statistics for each pair of reviewers. An example of this output is shown in the figure below.

![](images/covidence_kappa.png)

### Machine Learning for Relevance Screening

Many systematic management programs have implemented or are developing **machine learning tools** to semi-automate the relevance screening process. The tools work by prioritizing the abstracts that are most likely to be included (*screening prioritization*), and suggesting a percentage that can be "safely" excluded (*screening truncation*), which can both enhance review efficiency. The screening truncation process usually uses a stopping rule or threshold, such as 95% estimated recall, after which point the review team can decide not to review the remaining references or to use a modified process (e.g., only one reviewer).

These tools requires careful training and pilot testing on a subset of abstracts (e.g., 2% of total abstracts) to ensure the algorithm can make more accurate predictions. Research has shown that such machine learning tools can perform with high accuracy and reliability, and can result in substantial time savings for review teams [@hamelEvaluationDistillerSRMachine2020; @tsouMachineLearningScreening2020]. [Recent guidance is available](https://doi.org/10.1186/s12874-021-01451-2) for reviewers who are interested to adopt these tools in their review [@hamelGuidanceUsingArtificial2021].

::: callout-tip
### Relevance Screening Exercise

Using the example relevance screening form above, from Young et al. [-@youngEffectivenessFoodHandler2019], assess the relevance of the [following five abstracts captured in the search from that review](assets/Example_RS_Form.docx) and record your answers for discussion.
:::

## Article Procurement

Following relevance screening, full-text articles (e.g., PDF files) should be obtained for all references considered relevant. These can be saved to a folder and uploaded/linked to the applicable reference IDs in the systematic review management program used.

::: callout-important
## Finding Articles

You should be able to identify and access most articles using a combination of simple Google Searching (for open-access papers) and the [TMU scholarly paper search feature](https://library.torontomu.ca/).

Occasionally, you may need to search for TMU access to the specific journal of interest, then search for the paper in specific databases if the global search does not find the paper(s) you need.

In cases where you cannot find the paper via these methods, you can ask the librarian for assistance, or make a special request for the paper through the [Interlibrary Loan Service](https://library.torontomu.ca/services/ill/).
:::

## Article Characterization

It is often useful to include a separate screening confirmation (sometimes referred to as second-level screening) step and an article characterization step (i.e., data charting). The relevance confirmation step can be used to exclude irrelevant papers, which can be combined with article characterization to subsequently extract key characteristics of the relevant articles. This step is sometimes combined with data extraction, where study outcomes are also extracted. The confirmation of relevance should include a checkbox to indicate the primary reason for exclusion for articles that are not relevant.

The data captured in this form should be summarized in a characteristics of included studies table in the review results. Additionally, some of the variables captured in this form may also be used in the analysis (e.g., subgroup analysis or meta-regression). Some frequently important variables to capture include:

-   Document type, year of publication, language
-   Study design
-   Study dates and location(s)
-   Recruitment and sampling methods
-   Characteristics of participants (e.g., age, gender)
-   Characteristics of the intervention or exposure (e.g., dose, length, how they were measured)
-   Characteristics of the control or comparison group (if applicable)
-   Types of outcomes measured and how they were measured

It is often useful to develop outlines of the anticipated figures and tables that you plan to include in the results section of your systematic review. This will help you to decide on which important characteristics to extract.

::: callout-warning
## Multiple reports of the same study

It is sometimes the case that the same study is reported in multiple publications. In the case of authors splitting results of a study across multiple journal papers, it might be easiest to treat these as separate studies at this stage and note that they are from the same larger study. In cases where preliminary data are published in a thesis or abstract, usually you will want to keep only the published journal article as the main source for that study.
:::

### Pilot Testing

It is also important to pilot test the article characterization form. Usually this can be done on a small selection of articles (e.g., 5-10). Instead of checking kappa agreement, you should compare reviewer answers, look for discrepancies in interpretation of questions, discuss, and modify the form to enhance its clarify and ensure consistent interpretation.

::: callout-tip
## Article Characterization Exercise

Examine the [example article characterization form](assets/Example_Article_Characterization_Form.docx) from Young et al. [-@youngEffectivenessFoodHandler2019]. Use this form to extract data on a few different characteristics from the three full-text articles captured and screened as potentially relevant in that review. Then record your answers for discussion.

The three articles are available at the following shared folder: <https://drive.google.com/drive/folders/1mvBj074odovMpWIQ-UQ9BJprNzRiB9cv?usp=sharing>.
:::

## Review Flow Chart

Usually the first part of data analysis and reporting results includes **summarizing details of the number of studies captured in the review and included/excluded at each step**. These details should be shown through a flow chart diagram or figure. The [Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) reporting guideline](http://prisma-statement.org/) has a [template for flow charts](http://www.prisma-statement.org/PRISMAStatement/FlowDiagram). A copy of the flow chart template is shown below:

![](images/flow_chart.png)

The flow chart should capture exact numbers of references identified from each source (e.g., bibliographic databases, grey literature, etc.) It should clearly indicate the number of references excluded and included at each step of the review, along with reasons for excluding references or articles. There is also an [online Shiny app for creating flow charts using this template](https://estech.shinyapps.io/prisma_flowdiagram/). An example flow chart from Young et al. [-@youngEffectivenessFoodHandler2019] is show below.

![](images/flow_chart_example.png)

## Descriptive Analysis of Article Characterization Results

The next step in analysis is usually to **descriptively summarize the article characteristics** (e.g., PICO/PECO elements, and other relevant article and study details). It can be helpful at this stage to start categorizing the PICO/PECO elements into groups that can facilitate summary displays (e.g., tables) and further analysis. For example, specific types of interventions and outcomes might be grouped together. A table can then be presented that has *each relevant study as a row* and specific *columns for each of the relevant PICO/PECO elements or groups*. An example of such a table from Bartlett et al. [-@bartlettSystematicReviewMetaanalysis2019] is shown below (note this shows only the first several studies included in the table):

![](images/table1_example.png)

In addition to showing a table with a the summary characteristics and PICO/PECO details for *each study*, in can also be useful, especially in cases where the number of relevant studies included in the review is very large, to create a *tabulation table* to show overall numbers and percentages of studies that investigated specific PICO/PECO elements and reported specific details. An example of such a table from Young et al. [-@youngEffectivenessFoodHandler2019] is shown below (note that only the first several rows of the table are shown here). In the example below, we will examine how to reproduce such a table in *R*.

![](images/table1_example2.png)

::: callout-note
### Summary Article Characterization Table Example

As an illustrative example of calculating a summary tabulation table, we will load some *selected* article characterization data from the relevant articles in Young et al. [-@youngEffectivenessFoodHandler2019].

```{r}
#| echo: false
#| warning: false
# Display dataset as an interactive table
library(DT)
library(rio)
library(here)

Young_2019_chart <- import(here("assets", "Young_2019_charting.xlsx"))

Young_2019_chart |> datatable(
  rownames = FALSE,
  options = list(
    columnDefs = list(list(className = 'dt-center', 
                           targets = 0:4)))) |> 
  formatStyle(columns = colnames(Young_2019_chart), fontSize = '70%')
```


Now we can produce some summary statistics and tabulations of the variables and organize them into a table to present in our results.

```{r}
#| warning: false
library(dplyr)
library(gtsummary)

# First convert Year variable to numeric
Young_2019_chart <- Young_2019_chart |> 
  mutate(Year = as.numeric(Year))

# Display preferences
theme_gtsummary_language("en", big.mark = "")

# Create and display summary table
Young_2019_summary <- Young_2019_chart |> 
  select(-Refid, -"Author (year)") |>  # Remove Refid and author (year) columns from summary
  tbl_summary(type = Year ~ "continuous",
              statistic = Year ~ "{median} ({min}-{max})",
              sort = list(all_categorical() ~ "frequency"),
              digits = list(all_categorical() ~ c(0, 1)))
Young_2019_summary
```

We can further adjust the settings and presentation of the table as needed, see the [Epi R Handbook](https://epirhandbook.com/en/descriptive-tables.html#tbl_gt) section on the **gtsummary package** for more information. We can also copy and paste our table from the HTML viewer in RStudio into a Word document for easy editing and inclusion in our report.
:::

::: callout-tip
## Flow Chart and Article Summary Exercise

Using the systematic reviews you identified in previous weeks related to your thesis topic, review the article to examine how they displayed their **flow chart** and **article summary data**. Consider the following questions:

-   How does the flow chart compare to the PRIMSA template shown above?
-   What format is used for the article summary information?
-   Are any important article characteristics missing?
:::


## Homework

Students will be assigned a risk-of-bias *domain* from @sec-bias to review and present in class next week. The bias domains to be assigned include:

-   Bias arising from the randomization process
-   Bias due to deviations from intended interventions
-   Bias due to missing outcome data
-   Bias in measurement of the outcome
-   Bias in selection of the reported result
-   Risk of bias due to confounding
-   Risk of bias arising from measurement of the exposure
-   Risk of bias in selection of participants into the study
-   Risk of bias due to post-exposure interventions
-   Assessing methodological limitations of qualitative evidence

You can read in detail about the first five domains in the [Cochrane RCT risk-of-bias tool guidance document](https://drive.google.com/file/d/19R9savfPdCHC8XLz2iiMvL_71lPJERWK/view?usp=sharing). Information on the other four domains can be found in the [ROBINS-E tool for cohort studies](https://drive.google.com/file/d/1bcOVMna3CJJXpsIm5vobFJMP0-13CW8Q/view?usp=sharing). For the qualitative methodological assessment, details can be found starting in [section 4.4 of this article describing the CAMELOT tool](https://doi.org/10.1002/cesm.12058).




