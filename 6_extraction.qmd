---
webr:
  packages: ['dplyr', 'esc']
---

# Data Extraction, Outcome Measures, and Introduction to Meta-Analysis {#sec-extraction}

This section will cover the important considerations involved in extracting **outcome data** from the relevant studies. At this stage, it is important to consider the different possible outcome **measures of effect** that could be reported in the studies and which are of primary interest and relevance to your review question. Then we will introduce **meta-analysis** and describe differences between fixed-effect and random-effects models.

![](images/meme_ma.jpg)

## Types of Outcomes and Measures of Effect

A key first step in designing the outcome data extraction form and planning for analysis is to determine the relevant types of **outcome data measures** for the review. The most common types are listed below:

-   Dichotomous (AKA binary)
-   Continuous
-   Ordinal
-   Counts and rates
-   Time-to-event (survival data)

Each of these are discussed further below. For each study, we will want to either directly extract a **measure of effect** (i.e., effect measure, effect size) from the study, or extract the **raw data** and use that data to calculate a measure of effect. Typically, for reviews of RCTs, the raw data is sufficient for extraction (if available), as the randomization process should mitigate the impact of confounding bias. When extracting data from observational studies and non-randomization trials, typically the *adjusted* measure of effect should be extracted as it reduces to the impact of confounding variables. Additionally, it is preferable to extract measures of effect directly from RCTs that have adjusted for clustering or baseline measurements.

**Measures of effect** can be expressed as ratios (e.g., odds ratio, risk ratio, hazard ratio) or differences (e.g., mean difference, risk difference). For reviews of prevalence and incidence questions, the **measure of effect** is simply the prevalence or incidence point estimate(s) of interest.

::: callout-note
### Ratio Measures of Effect

Ratio measures of effect, such as odds ratios, are analyzed on the natural log scale, so must be log-transformed for all studies prior to analysis.
:::

## Unit of Analysis

When extracting outcomes from studies you will likely encounter **unit-of-analysis issues**. These are situations where outcomes are nested within larger units, or there are repeated measurements on the same participants, and this extra level of variation must be accounted for when conducting analysis. The following are common situations of this:

-   The study has **clusters** (e.g., schools, communities) with outcomes measured and reported on participants within the cluster
-   There are **repeated measurements** on participants (e.g., outcomes are reported at multiple follow-up periods)
-   Each participant receives **multiple treatments or interventions**
-   There are **multiple variations of the same outcome** reported for participants in the same study
-   There are **multiple intervention groups** compared in the same study

Statistical approaches to address some of these issues will be covered in a later section.

## Data Extraction Form

The **data extraction form** should be designed so that multiple outcomes can be extracted for each study. The suggested approach is to pre-determine **outcome categories** for analysis based on your review question. These should represent specific groups of similar outcomes that will combined together in the same analysis. You can also include additional questions on the form that might have *different responses for different outcome categories* (e.g., how the outcome was measured). You may also have different categories for intervention or exposure groups of interest.

An example of a data extraction form from Young et al. [-@youngEffectivenessFoodHandler2019] is [available for download here](assets/Example_DE_Form.docx).

## Dichotomous Measures

**Dichotomous outcomes** are those with only two possibilities (e.g., disease positive or negative). Two common measures of effect for these outcomes are **odds ratios** (OR) and **risk ratios** (RR). An RR is a ratio of the risk of the event in each group, while the OR is ratio of the odds of event (compared to a non-event) in each group. The table below shows how these two measures are calculated:

|              | Event | Non-Event | Total   |
|--------------|-------|-----------|---------|
| Intervention | A     | B         | ${N_1}$ |
| Control      | C     | D         | ${N_2}$ |

: 2 X 2 table for an intervention study

$$
\text{RR} = \frac{\text{A}/{N_1}}{\text{C}/{N_2}}
$$

$$
\text{OR} = \frac{\text{A}/{B}}{\text{C}/{D}}
$$

The RR and the OR are only similar when the event is rare.

Another possible measure for dichotomous outcomes is the **risk difference** (RD). This measure reflects the difference between the two observed groups risks. The practical significance of the RD depends greatly on the baseline risk of the event in the population. 

$$
\text{RD} = \left(\frac{\text{A}}{N_1}\right)-\left(\frac{\text{C}}{N_2}\right)
$$

To conduct a meta-analysis of dichotomous data, we typically need one of the following sets of values for the intervention (or association) effect:

-   Numerator and denominator in each group
-   Proportion + EITHER numerator or denominator in each group, or
-   Measure of effect (e.g., OR, RR) + a measure of variability

## Continuous Measures

**Continuous outcomes** can theoretically take any value within a specified range. The two most common measures of effect for continuous data are the **mean difference** (MD) and the **standardization mean difference** (SMD). 

The **MD** can be used when the outcome is reported on a meaningful scale (e.g., weight in kg, blood pressure in mmHg), and the *same scale is used in all studies*. This measure is easy and intuitive to interpret if possible to calculate. 

The **SMD** can be used when the studies in a SR measure the same outcome, but in different ways (e.g., different scales). The SMD is calculated by dividing a study's mean difference by its standard deviation (SD) to create an index. There are different SMD metrics, the most common of which is called **Hedges' g**, which uses a pooled SD in the denominator based on outcome data in both groups being compared, and assuming that the SDs of the two groups are similar.  

For continuous data outcomes, we typically require the mean, sample size, and SD in each group to calculate either the MD or SMD. Alternatively, a pre-calculated measure of effect (e.g., MD) and its measure of variability (e.g., standard error) can also be used directly in a meta-analysis.

::: callout-warning
### Change from baseline data

Some studies will report baseline and post-intervention values (often with different follow-up time points). It can be useful to extract both values if they are available and compare any differences. However, often the standard deviation (SD) of the change score is missing and needs to be [imputed or estimated to include the score in a meta-analysis](https://training.cochrane.org/handbook/current/chapter-06#section-6-5-2-8). This process involves many assumptions that should be evaluated through sensitivity analysis.
:::

## Other Outcomes

**Ordinal outcomes** occur when there are multiple, ordered categories of an outcome (e.g., low, moderate, and high knowledge levels). As the number of ordinal categories increases, they start to resemble continuous outcomes, and could potentially be analyzed using those methods. If there is an obvious cut-point, the categories could be combined and used in a dichotomous analysis. Otherwise, if the same categories are used across all studies, this outcome can be analyzed using a proportional odds model.

**Count outcomes** occur when the outcome can happen multiple times for each participant. These are usually expressed as **rates** (e.g., number of events per person-year), with the comparison of two rates being expressed as a rate ratio. The most common situation involving rate data is that studies directly report rate ratios and a measure of variability (e.g., standard error) from regression models, which can be extracted and combined in a meta-analysis. 

**Time-to-event outcomes** refer to the measurement of the time elapsed until some event (e.g., death) is experienced (e.g., survival data). These data are usually analyzed in studies using survival analysis and expressed as a **hazard ratio** (HR). Hazard measures instantaneous risk and can change continuously over time. As above, studies usually report the HR and a measure of variability which can be extracted directly and included in a meta-analysis.

::: callout-tip
## Data Extraction Exercise

Consider again the Young et al. [-@youngEffectivenessFoodHandler2019] systematic review about the effectiveness of food handler training interventions. [Using three example articles](https://drive.google.com/open?id=1a6ozc5hmlMo7Ff1qG3O7oLjJIHVzMq9D&usp=drive_fs), identify the outcomes of interest in each study and which data should be extracted. 

- How many relevant outcomes are reported in each study?
- Are any important outcomes or values (e.g., measures of variability) missing? 
:::

## Data Required for Meta-Analysis

If you are interested or planning to conduct a meta-analysis, data must be sufficiently reported in the article to allow calculation of a summary measure of effect and to allow inclusion in a meta-analysis. Often, many studies will be missing information on one of the common inputs required. Thankfully, in many cases we can use various formulas to estimate the missing information. Some common scenarios are highlighted below.

### Convert Confidence Interval to a Standard Error (SE)

If only a 95% confidence interval (CI) is reported for an absolute measure of effect (e.g., standardized mean difference, risk difference), the SE can be calculated from the following formula:

$$
\text{SE} = \frac{(\text{Upper limit}-\text{Lower limit})}{3.92}
$$

For ratio measures (e.g., odds ratio, risk ratio), the upper and lower CI limits, and intervention effect estimate, should be on the natural log scale.

The same formula can also be used to convert a CI of a mean value within each group (e.g., intervention, control) to its SE. However, in this case, if the sample size in each group is small (e.g., \<60), then the CIs should have been calculated using a t distribution and the divisor (3.92) [should be replaced by a different number from a t distribution](https://training.cochrane.org/handbook/current/chapter-06#section-6-5-2-2).

### Convert SE for Each Group to a SD for Each Goup

If only a SE, and not a SD, is reported within each group being compared, the SD can be obtained from this formula:

$$
\text{SD} = \text{SE}\sqrt{N}
$$

::: callout-note
### Example SE to SD conversion

We can illustrate this conversion with some simulated data in *R*.

```{webr-r}
library(dplyr)

# Simulate some study data for meta-analysis
data <- data.frame(study_ID = as.factor(rep(1:4, times = 1)),
                   treat_mean = c(1.5, 2.1, 2.2, 2.7),
                   treat_SE = c(0.03, 0.06, 0.07, 0.08),
                   treat_n = c(100, 50, 70, 60),
                   control_mean = c(1.3, 1.8, 2.0, 2.2),
                   control_SE = c(0.03, 0.07, 0.07, 0.09),
                   control_n = c(100, 50, 70, 50)
                   )
data
```

Now calculate a SD variable for each group using the formula above.

```{webr-r}
data <- data |> mutate(
  treat_SD = treat_SE*sqrt(treat_n),
  control_SD = control_SE*sqrt(control_n)
)
data |> select(-treat_SE, -control_SE)
```
:::

### Convert from SE, CI, t value, or P value to a SD for Mean Differences

When using the raw MD as the outcome, missing SD values can be estimated from a SE, CI, t value, or P value. These conversions assume that the SD values are the same in both groups being compared.

To calculate the t value from a P value (from a t-test), we can use the `qt` function, inputting the P value first (divided by 2), and the degrees of freedom next (equal to the sample size in each group minus 2). For example, for a study with a P value of 0.03 and a sample size of 20 in each group, we could use this formula:

```{webr-r}
qt(p=0.03/2, df=40-2, lower.tail=FALSE)
```

Now try calculating the t value from a study with a P value of 0.001 and a sample size of 150 participants in each group.

::: panel-tabset
### Code Editor

```{webr-r}
qt(p=___/2, df=___-2, lower.tail=FALSE)
```

### Answer

```{webr-r}
#| read-only: true
qt(p=0.001/2, df=300-2, lower.tail=FALSE)
```
:::

To convert from a t value to a SE, we can use the following formula:

$$
\text{SE} = |\frac{\text{MD}}{\text{t value}}|
$$

The SE can then be converted to a within-group SD using the following formula, where $N_1$ and $N_2$ represent the sample size in each group:

$$ 
\text{SD} = \frac{\text{SE}}{\sqrt{\frac{1}{N_1}+{\frac{1}{N_2}}}}
$$

The SD is the average of the SDs of each group, and should be entered for both groups.

### Conversions to a Standardized Mean Difference

There are various formulas available to convert from SEs, ANOVA F values, t-test values, regression beta coefficients to SMD measures such as Hedges' *g*, a log odds ratio, or a correlation coefficient *r*.

These use formulas from the [practical meta-analysis effect size calculator](http://mason.gmu.edu/~dwilsonb/downloads/esformulas.pdf) and are implemented in the [**esc**](https://cran.r-project.org/web/packages/esc/) package in *R*. Some examples are given in the [Doing Meta-Analysis in R](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/es-calc.html) online textbook.

Two examples are shown below for a one-way ANOVA F test and for a $chi^2$ test.

::: callout-note
### Example F Test and $chi^2$ Test Converison

Suppose we have a study that only reports the F value from a one-way ANOVA test, instead of means and SDs within each group or a MD value. If the F value is 6.23, and the sample size is 190 in the intervention group and 80 in the control group, we can calculate the SMD (as Hedges' *g*) as follows:

```{webr-r}
library(esc)

esc_f(f = 6.23, grp1n = 190, grp2n = 80, es.type = "g")
```

Similarly, if we had study that only reported a $chi^2$ test value for a dichotomous association, we could convert that to a log odds ratio and its SE. Note that this conversion assumes a degrees of freedom of 1. For example, suppose the $chi^2$ value was 2.5 and the sample size was 120:

```{webr-r}
esc <- esc_chisq(chisq = 2.5, totaln = 120, es.type = "logit")
esc
```

We can then load the data from the saved object into the applicable row in the dataset. For example, suppose the previous conversion was conducted for Study 3 as per the mock dataset below:

```{webr-r}
data2 <- data.frame(study_ID = as.factor(rep(1:3, times = 1)),
                    log_or = c(1.1, 0.8, NA),
                    log_se = c(0.5, 0.4, NA),
                    n = c(100, 150, 120)
                    )
data2
```

We can then use the following code to insert the saved conversion values into the applicable columns for Study 3:

```{webr-r}
data2$log_or["Study ID" = 3] <- esc$es
data2$log_se["Study ID" = 3] <- esc$se
data2
```
:::

## Methods of Analyzing Measures of Effect

The recommended approach to analyze quantitative outcome data (i.e., measures of effect), if possible and reasonable to do so, is **meta-analysis**. Meta-analysis is defined as statistical method for combining the results of two or more studies.

There are some situations where it may not be possible to use meta-analysis, for example:

-   There is **only one or no studies** containing the outcome of interest
-   The **outcome data are not sufficiently reported** and cannot be estimated from available statistics
-   The study designs, methods, or outcome measures are **too diverse and heterogeneous** that it doesn't make biological or reasonable sense to combine them
-   There are **major concerns about bias** in the studies

In these cases, one can consider conducting a **descriptive and narrative summary** of the evidence. This includes presenting *summary tables* of the results of each study and some discussion of the results of each study. Tables can be grouped by relevant criteria from the question (e.g., intervention groups, outcome domains), or by other factors such as strength of evidence or risk of bias.

Another option that can be used when summary measures of effect are available from studies, but measures of variability needed to conduct meta-analysis are *not available*, is to summarize the effect estimates directly using **box (and/or violin) plots**. This type of summary provides information about the size and range of effects across studies, but does not account for the differences in sample size across studies. Combining **P values** is another method that can be used when only P values are available, and aims to determine is there is evidence of an effect in at least one study. For more information on these and other alternative analysis methods, [see the relevant chapter in the Cochrane Collaboration Handbook](https://training.cochrane.org/handbook/current/chapter-12).

::: callout-note
### Example of when Meta-Analysis was Not Feasible

In a systematic review about the prevalence of knowledge, practices, and training outcomes among restaurant and food service personnel toward food allergies and Celiac disease [@youngSystematicReviewMetaregression2018], the authors did not conduct meta-analysis on the outcome due to substantive differences in the populations assessed, their characteristics, and how outcomes were measured. Instead, they summarized the **distribution** of prevalence outcomes across studies within each outcome domain using box plots. The figure below shows the prevalence of food allergy and Celiac disease **behaviour outcomes** reported across studies:

![](images/non_MA_example.png)
:::

## Introduction to Meta-Analysis

Meta-analysis is a **statistical technique** that involves combining the measure of effect for each study along with its weight to calculate a weighted average of the measure of effect. Each study receives a **weight** based on its standard error (SE), so studies with greater precision (i.e., less error) receive more weight. There are two main approaches to meta-analysis, which determine how weights are calculated: fixed-effect and random-effects.

A **fixed-effect** meta-analysis assumes there is one *true effect size* among all of the included studies, and that any observed differences in the effects are due only to sampling error. In contrast, **random-effects** meta-analysis models assume that the study effects represent a random sample of possible effect sizes (that usually follows a normal distribution). The null hypothesis under a fixed-effect model is that *there is zero effect in every study*, while for a random-effects model the null hypothesis is that the *average effect is zero*. Both models will give identical results when there is no statistical heterogeneity among the studies.

Assuming an overall true effect of $\theta$, under a **fixed-effect** model, the observed effect of study *j*, $\theta_j$, differs from the overall true effect only because of its sampling error ($\epsilon_j$):

$$
\hat{\theta_j} = \theta + \epsilon_j
$$ 
Studies are then **weighted** by the *inverse* of the variance (which is equal to the SE squared). For study *j*, the weight would be calculated as follows:

$$
{W_j} = \frac{1}{SE_j^2}
$$

The weighted average, or *pooled effect*, can then be calculated by multiplying each study's effect size by its weight, adding those together, and then dividing by the sum of all study weights. This is referred to as the **generic inverse-variance** weighting approach:

$$
\hat{\theta} = \frac{\sum_{j=1}^J \hat{\theta_j}{W_j}} {\sum_{j=1}^J {W_j}}
$$

In contrast, **random-effects** models incorporate an additional source of error, denoted $\zeta_j$, to indicate the true effect size of study *j* is part of a distribution of true effect sizes with a mean value of $\mu$:

$$
\hat{\theta_j} = \mu + \zeta_j + \epsilon_j
$$

In a **random-effects** model, study weights incorporate a measure of the *variance* of the distribution of true effect sizes, denoted $\tau^2$:

$$
{W_j} = \frac{1}{{SE_j^2}+{\tau^2}}
$$

The adjusted weight is then used to calculate the weighted average measure of effect. There are several methods to estimate $\tau^2$. The most famous is called the **DerSimonian-Laird (DL)** method. This method has straightforward calculations, and is often the default method in various software packages. However, this method can be biased and results in confidence intervals that are too narrow when the number of studies is small and heterogeneity is high.

Based on simulation studies, the **restricted maximum likelihood estimator (REML)** approach seems to be good choice for *continuous* outcome data [@langanComparisonHeterogeneityVariance2019; @veronikiMethodsEstimateBetweenstudy2016]. For **dichotomous** (binary) outcome data, the **Paule-Mandel estimator** or the **Empirical Bayes** method seem to be good choices. You can always conduct a sensitivity analysis to compare with other approaches or the standard DL method.

Additionally, it is generally recommended to apply **Knapp-Hartung adjustments** if the option is available, as this method widens the confidence interval of the weighted measure of effect to account for uncertainty in the estimation of $\tau^2$.

## Fixed vs. Random-Effects Models

Fixed-effect models are **usually not appropriate** for reviews in occupational and public health, as they assume that all studies are essentially identical, which is usually only the case under highly controlled and replicable conditions. The results from such an analysis are only applicable to the participants in each study, and cannot be generalized for other populations.

In contrast, **random-effects** models are more appropriate when there are differences across studies beyond what we would expect from sampling error alone (e.g., differences in interventions, populations, how outcomes are measured). However, this approach gives more weight to smaller studies, which can exacerbate potential biases that may be present in smaller studies. Additionally, if the number of included studies is very small, the estimate of between-study variance $\tau^2$ will have poor precision and is less reliable. Unfortunately, there is no clear guidance about which approach should be used in this situation. **Bayesian methods** are probably the best option, but require more advanced expertise.

::: callout-tip
### Meta-Analysis Exercise

Using the systematic review article you identified earlier in the course, review the article to examine whether meta-analysis was conducted. If it wasn't, you can answer the following questions based on this systematic review and meta-analysis of the effectiveness of workplace mindfulness training interventions [@bartlettSystematicReviewMetaanalysis2019]. Consider the following questions for the article analyzed:

-   What was the main outcome measure extracted and analyzed?
-   Did the authors report whether any data conversions were necessary? If so, which ones, how many, and how were they conducted?
-   Did the authors provide any references or rationale for any conversions conducted?
-   Did they conduct a fixed-effect or random-effects model, and what rationale (if any) was given for the choice?
-   If a random-effects model was used, which method was used to estimate $\tau^2$?
-   Were any sensitivity analyses conducted related to the data conversions or assumptions made?
:::



